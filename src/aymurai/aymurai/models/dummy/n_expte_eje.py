# codeblock generated by devtools
# source: /workspace/notebooks/dev/patterns/04-expediente/00-numero_expediente.ipynb

import re
from copy import deepcopy

from more_itertools import unique_justseen

from aymurai.meta.types import DataItem, DataBlock
from aymurai.meta.pipeline_interfaces import TrainModule


class DummyExtractorExpediente(TrainModule):
    def save(self, path: str):
        return

    def load(self, path: str):
        return

    def fit(self, train: DataBlock, val: DataBlock):
        return

    def predict(self, data: DataBlock) -> DataBlock:
        data = [self.predict_single(item) for item in data]

        return data

    def predict_single(self, item: DataItem) -> DataItem:
        item = deepcopy(item)

        # format prediction
        if "predictions" not in item:
            item["predictions"] = {}
        if "records" not in item["predictions"]:
            item["predictions"]["records"] = {}
        if "entities" not in item["predictions"]:
            item["predictions"]["entities"] = []
        if "doc-cats" not in item["predictions"]:
            item["predictions"]["doc-cats"] = {}
        item["predictions"]["doc-cats"]["n_expte_eje"] = None
        item["predictions"]["records"]["n_expte_eje"] = []

        ents = []
        if "entities" in item["data"]:
            ents += item["data"]["entities"]

        ents = filter(lambda x: x["label"] == "N_EXPTE_EJE", ents)
        ents = sorted(ents, key=lambda e: e["attrs"]["aymurai_score"])
        ents = list(ents)

        # if there is no entities just pass
        if not ents:
            return item

        parser = re.compile(r"(?P<exp>\d+)/(?P<year>\d+)(?P<code>-\d)?")
        exptes = map(lambda x: parser.search(x["text"]), ents)
        exptes = list(exptes)

        if not exptes:
            return item

        for ent, expte in zip(ents, exptes):
            char_offset = expte.span()
            subtext = expte[0]
            text = ent["text"]
            subpre = text[: char_offset[0]]
            subpost = text[char_offset[1] :]

            ent["context_pre"] += subpre
            ent["context_post"] = subpost + ent["context_post"]
            ent["end_char"] = ent["start_char"] + char_offset[1]
            ent["start_char"] += char_offset[0]

            # FIXME: should use spacy tokenizer instead split with spaces!
            tokenspre = list(filter(bool, subpre.split(" ")))
            tokenspost = list(filter(bool, subpost.split(" ")))
            ent["start"] += len(tokenspre)
            ent["end"] -= len(tokenspost)
            ent["text"] = subtext

        # get first prediction
        # span = dates[0]
        # year = span["attrs"]["aymurai_date"].strftime("%Y")

        item["predictions"]["entities"].append(ents)
        item["predictions"]["records"]["n_expte_eje"] += [ent["text"] for ent in ents]
        item["predictions"]["doc-cats"]["n_expte_eje"] = ents[0]["text"]

        return item
