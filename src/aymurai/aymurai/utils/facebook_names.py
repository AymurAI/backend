# codeblock generated by devtools
# source: /workspace/notebooks/dev/patterns/names/03-facebook-lookup-prod.ipynb


import os
import re
import hashlib
import tarfile
import subprocess
from glob import glob
from pathlib import Path

import srsly
import pandas as pd

from aymurai.logging import get_logger

logger = get_logger(__name__)

FACEBOOK_NAMES_URI = os.getenv(
    "FACEBOOK_NAMES_URI",
    "https://drive.google.com/file/d/1wRQfw5EYpzulvRfHCGIUWB2am5JUYVGk/view",
)
FACEBOOK_NAMES_PATH = os.getenv("FACEBOOK_NAMES_URI", "/resources/data/facebook-names/")


def download_fb_names_db() -> str:
    """
    download facebook names database from massive 2021 hack

    Returns:
        str: path
    """
    logger.info(f"getting facebook names database from {FACEBOOK_NAMES_URI}")
    fname = "full.tar.bz2"
    fpath = f"{FACEBOOK_NAMES_PATH}/{fname}"
    os.makedirs(FACEBOOK_NAMES_PATH, exist_ok=True)
    cmd = f"gdown --fuzzy --continue --output {fpath} {FACEBOOK_NAMES_URI}"
    subprocess.check_output(cmd.split())

    return fpath


def extract_fb_names(country_codes: list = ["AR"], use_cache: bool = True):
    """
    Extract the facebook names from a list of country codes

    Args:
        country_codes (list, optional): extract names from <code> countries.
            Defaults to ["AR"].
        use_cache (bool, optional): use cache.
            Defaults to True.
    """
    tar_path = download_fb_names_db()
    basepath = os.path.dirname(tar_path)

    # check if files are extracted (use_cache = True)
    extracted = glob(f"{basepath}/curate/*.csv")
    extracted_codes = [os.path.basename(path).split(".")[0] for path in extracted]
    if not use_cache:  # force to extract all
        extracted_codes = []

    tar_members = [
        f"curate/{code}.csv" for code in country_codes if code not in extracted_codes
    ]
    with tarfile.open(tar_path, "r:*") as tar:
        if tar_members:
            logger.info(f"extracting {tar_members}")
            tar.extractall(path=basepath, members=tar_members)

    return [f"{basepath}/curate/{code}.csv" for code in country_codes]


def norm(name: str) -> list[str]:
    """
    Normalize and split a name to be used in a valid way .
    FIXME: nomalization was designed with argentinian names in mind.
    Not a general solution.

    Args:
        name (str): string containing multiples first/last names

    Returns:
        list[str]: list of names
    """
    if not isinstance(name, str):
        return name
    # spliting in space except on compose names
    name = re.sub(r"(?i)((?<!\W(De|Da|Di|D\'|D\"))\s)", "|", name)
    # fixing some prefixes
    name = re.sub(r"(?i)(?<!\w)(San|Del|las?)(\|)", r"\g<1> ", name)

    # splitting names
    name = name.split("|")
    return name


def load_database(country_codes=["AR"], use_cache: bool = True) -> pd.DataFrame:
    """
    Load a database of facebook names by country code.
    FIXME: nomalization was designed with argentinian names in mind.
     Not a general solution.

    Args:
        country_codes (list, optional): country codes to use. Defaults to ["AR"].
        use_cache (bool, optional): use cache. Defaults to True.

    Returns:
        pd.DataFrame: names, gender and origin
    """
    logger.info(f"loading facebook names database for {country_codes}")
    paths = extract_fb_names(country_codes=country_codes, use_cache=use_cache)
    db = pd.concat(
        [
            pd.read_csv(
                path,
                names=["first_name", "last_name", "gender", "loc"],
            )
            for path in paths
        ],
        ignore_index=True,
    )

    fname = db["first_name"].apply(norm)
    lname = db["last_name"].apply(norm)

    db["name"] = fname + lname

    db = db.explode("name")
    db.dropna(subset=["name"], inplace=True)

    return db


def load_counts(
    country_codes: list = ["AR"],
    min_freq: int = 100,
    min_name_length: int = 4,
    use_cache: bool = True,
) -> pd.DataFrame:
    """
    Load facebook name counts for a list of country codes.

    Args:
        country_codes (list, optional): list of country codes to use. Defaults to ['AR'].
        min_freq (int, optional): Minimum number of name repetitions. defaults to 100.
        min_name_length (int, optional): minimum name length
        use_cache (bool, optional): use cache. Defaults to True.

    Returns:
        pd.DataFrame: [description]
    """
    cache_key = hashlib.md5(
        srsly.json_dumps(sorted(country_codes)).encode()
    ).hexdigest()
    cache_path = f"{FACEBOOK_NAMES_PATH}/counts/{cache_key}.csv"

    db = load_database(country_codes=country_codes, use_cache=use_cache)

    if use_cache and Path(cache_path).exists():
        logger.info(f"loading name counts from cache: {cache_path}")
        counts = pd.read_csv(cache_path)

    else:
        logger.info("building names database")
        counts = (
            db.groupby("name")
            .agg({"loc": "count"})
            .sort_values(by=["loc"], ascending=False)
        )
        counts = counts.reset_index()
        counts.rename(columns={"loc": "counts"}, inplace=True)
        counts["len"] = counts["name"].apply(len)

    if use_cache:
        os.makedirs(os.path.dirname(cache_path), exist_ok=True)
        counts.to_csv(cache_path, index=False)

    counts.query(f"counts > {min_freq} and len >= {min_name_length}", inplace=True)

    return counts
