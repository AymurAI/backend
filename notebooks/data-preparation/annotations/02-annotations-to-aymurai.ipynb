{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext aymurai.devtools.magic\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%export aymurai.datasets.ar_juz_pcyf_10.labelstudio.utils\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "from glob import glob\n",
    "from numpy import cumsum\n",
    "from copy import deepcopy\n",
    "from itertools import groupby\n",
    "from more_itertools import unzip, collapse\n",
    "\n",
    "from aymurai.meta.types import DataItem\n",
    "from aymurai.spacy.utils import format_entity\n",
    "from aymurai.utils.json_data import load_json\n",
    "\n",
    "nlp = spacy.blank(\"es\")\n",
    "\n",
    "\n",
    "def join_label_category(spans: list[dict]) -> dict:\n",
    "    \"\"\"join entity & entity-category (labelstudio) on one object\"\"\"\n",
    "    span = {}\n",
    "    for s in spans:\n",
    "        span.update(s)\n",
    "    return span\n",
    "\n",
    "\n",
    "def reformat_entity(doc: spacy.tokens.Doc, span: dict) -> dict:\n",
    "    \"\"\"\n",
    "    reformat labelstudio entity to aymurai\n",
    "\n",
    "    Args:\n",
    "        doc (spacy.tokens.Doc): pointer to document spacy.Doc\n",
    "        span (dict): span to reformat\n",
    "\n",
    "    Returns:\n",
    "        dict: aymurai formatted entity\n",
    "    \"\"\"\n",
    "    entity = doc.char_span(\n",
    "        span[\"start\"], span[\"end\"], label=span[\"labels\"][0], alignment_mode=\"expand\"\n",
    "    )\n",
    "    entity = format_entity(entity)\n",
    "    entity[\"attrs\"][\"aymurai_label\"] = entity[\"label\"]\n",
    "    if \"choices\" in span:\n",
    "        entity[\"attrs\"][\"aymurai_label_subclass\"] = span[\"choices\"]\n",
    "    return entity\n",
    "\n",
    "\n",
    "def parse_annots(data: dict) -> dict:\n",
    "    \"\"\"\n",
    "    parse annotations from labelstudio-data\n",
    "\n",
    "    Args:\n",
    "        data (dict): labelstudio-json export\n",
    "\n",
    "    Returns:\n",
    "        dict: categories & entities (aymurai format)\n",
    "    \"\"\"\n",
    "    doc = nlp(data[\"data\"][\"text\"])\n",
    "\n",
    "    annotations = data[\"annotations\"][0][\"result\"]\n",
    "    annotations = list(annotations)\n",
    "\n",
    "    # filter span entities\n",
    "    spans = filter(lambda x: x[\"type\"] in [\"labels\", \"choices\"], annotations)\n",
    "    spans = map(lambda x: x[\"value\"], spans)\n",
    "    spans = sorted(spans, key=lambda x: x[\"start\"])\n",
    "\n",
    "    _, group = unzip(groupby(spans, key=lambda x: (x[\"start\"], x[\"end\"], x[\"text\"])))\n",
    "    group = map(join_label_category, group)\n",
    "\n",
    "    spans = list(group)\n",
    "    spans = map(lambda span: reformat_entity(doc, span), spans)\n",
    "    spans = list(spans)\n",
    "\n",
    "    # categories\n",
    "    categories = filter(lambda x: x.get(\"type\") == \"textarea\", annotations)\n",
    "    categories = {cat[\"from_name\"]: cat[\"value\"][\"text\"][0] for cat in categories}\n",
    "\n",
    "    return {\n",
    "        \"categories\": categories,\n",
    "        \"entities\": spans,\n",
    "    }\n",
    "\n",
    "\n",
    "def load_conll_annots(basepath: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    load annotations (CoNLL format)\n",
    "\n",
    "    Args:\n",
    "        basepath (dict): path where to look the annotations (conll file)\n",
    "\n",
    "    Returns:\n",
    "        list[str]: list containing the annotations of different docs\n",
    "    \"\"\"\n",
    "    # read annotations\n",
    "    conll_path = glob(f\"{basepath}/*.conll\")[0]\n",
    "    with open(conll_path) as file:\n",
    "        annotations = file.read()\n",
    "\n",
    "    # remove header\n",
    "    annotations = annotations.replace(\"-DOCSTART- -X- O\\n\", \"\")\n",
    "\n",
    "    # remove unuseful tags\n",
    "    annotations = annotations.replace(\" -X- _\", \"\")\n",
    "\n",
    "    # split annotations corresponding to different documents\n",
    "    annotations = annotations.split(\"\\n\\n\")\n",
    "\n",
    "    # pop empty element\n",
    "    if \"\" in annotations:\n",
    "        _ = annotations.pop(annotations.index(\"\"))\n",
    "\n",
    "    return annotations\n",
    "\n",
    "\n",
    "def parse_conll_annots(item: DataItem, annotation: str) -> DataItem:\n",
    "    \"\"\"\n",
    "    parse CoNLL annotations and document to split the text by paragraphs\n",
    "\n",
    "    Args:\n",
    "        item (DataItem): aymurai dataitem\n",
    "        annotation (str): CoNLL annotation corresponding to item\n",
    "\n",
    "    Returns:\n",
    "        DataItem: aymurai dataitem with CoNLL annotation\n",
    "    \"\"\"\n",
    "    item = deepcopy(item)\n",
    "\n",
    "    # document text\n",
    "    doc = item[\"data\"][\"doc.text\"]\n",
    "\n",
    "    # NOTE this should be donde as part of the preprocessing\n",
    "    # replace '\\t' and '\\xa0' for white space\n",
    "    doc = re.sub(r\"(?:\\t|\\xa0)+\", \" \", doc)\n",
    "\n",
    "    # remove multiple spaces except new lines\n",
    "    doc = re.sub(r\"[^\\S\\r\\n]+\", \" \", doc)\n",
    "\n",
    "    # replace multiple new lines with just one break\n",
    "    doc = re.sub(r\"\\n+\", \"\\n\", doc)\n",
    "\n",
    "    # split document by line\n",
    "    splitted_doc = doc.splitlines()\n",
    "\n",
    "    # number of tokens per line\n",
    "    n_tokens = [len(line.split()) for line in splitted_doc]\n",
    "\n",
    "    # indexes where a new line character must be inserted to separate paragraph\n",
    "    idx = [idx + i for i, idx in enumerate(cumsum(n_tokens))]\n",
    "\n",
    "    # split annotations by line\n",
    "    splitted_annotation = annotation.splitlines()\n",
    "\n",
    "    # insert new line character where needed\n",
    "    for i in idx:\n",
    "        splitted_annotation.insert(i, \"\\n\")\n",
    "\n",
    "    # join the new annotations\n",
    "    joined_annotation = \"\\n\".join(splitted_annotation)\n",
    "    joined_annotation = re.sub(\"\\n{3,}\", \"\\n\\n\", joined_annotation)\n",
    "\n",
    "    # add CoNLL annotation to dataitem\n",
    "    item[\"annotations\"][\"conll\"] = joined_annotation\n",
    "\n",
    "    return item\n",
    "\n",
    "\n",
    "def annotation_to_dataitem(annotation: dict) -> DataItem:\n",
    "    \"\"\"\n",
    "    format a whole labelstudio document into the aymurai format\n",
    "\n",
    "    Args:\n",
    "        annotation (dict): labelstudio document\n",
    "\n",
    "    Returns:\n",
    "        DataItem: aymurai dataitem\n",
    "    \"\"\"\n",
    "    item = {}\n",
    "    item[\"path\"] = annotation[\"data\"][\"meta_info\"][\"path\"]\n",
    "    item[\"data\"] = {\"doc.text\": annotation[\"data\"][\"text\"]}\n",
    "    annots = parse_annots(annotation)\n",
    "    item[\"metadata\"] = annots[\"categories\"]\n",
    "    item[\"annotations\"] = {\"entities\": annots[\"entities\"]}\n",
    "    return item\n",
    "\n",
    "\n",
    "def load_annotations(basepath: str) -> list[DataItem]:\n",
    "    \"\"\"\n",
    "    load all annotations from `basepath`. this directory must contain\n",
    "    the annotations both in json and conll formats.\n",
    "    internally, use glob to look for all the annotation files files inside `basepath`.\n",
    "\n",
    "    Args:\n",
    "        basepath (str): path where to look for the annotations (json and conll files)\n",
    "\n",
    "    Returns:\n",
    "        list[DataItem]: list of dataitems (aymurai format)\n",
    "    \"\"\"\n",
    "    paths = glob(f\"{basepath}/*.json\")\n",
    "    items = map(load_json, paths)\n",
    "    items = collapse(items, base_type=dict)\n",
    "    items = map(annotation_to_dataitem, items)\n",
    "    coll_annots = load_conll_annots(basepath)\n",
    "    items = map(parse_conll_annots, items, coll_annots)\n",
    "    return list(items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%export aymurai.datasets.ar_juz_pcyf_10.annotations\n",
    "\n",
    "from collections import UserList\n",
    "\n",
    "from aymurai.datasets.ar_juz_pcyf_10.labelstudio.utils import load_annotations\n",
    "\n",
    "\n",
    "class ArgentinaJuzgadoPCyF10LabelStudioAnnotations(UserList):\n",
    "    def __init__(self, basepath: str):\n",
    "        self.data = load_annotations(basepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aymurai.datasets.ar_juz_pcyf_10.annotations import ArgentinaJuzgadoPCyF10LabelStudioAnnotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ArgentinaJuzgadoPCyF10LabelStudioAnnotations('/test/api/mock-response/input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%export aymurai.transforms.misc.annot2pred\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from aymurai.meta.types import DataItem\n",
    "from aymurai.meta.pipeline_interfaces import Transform\n",
    "\n",
    "\n",
    "class DummyAnnotToPred(Transform):\n",
    "    \"\"\"dummy transform to convert annotations into predictions\"\"\"\n",
    "    def __call__(self, item: DataItem) -> DataItem:\n",
    "        item = deepcopy(item)\n",
    "        if 'annotations' not in item:\n",
    "            return item\n",
    "        item[\"predictions\"] = {\"entities\": item[\"annotations\"][\"entities\"]}\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aymurai.pipeline import AymurAIPipeline\n",
    "from aymurai.transforms.entities import FilterEntity\n",
    "from aymurai.transforms.misc.annot2pred import DummyAnnotToPred\n",
    "from aymurai.text.extraction import FulltextExtract\n",
    "from aymurai.text.normalize import TextNormalize\n",
    "\n",
    "config = {\n",
    "    \"preprocess\": [\n",
    "        (\n",
    "            FulltextExtract,\n",
    "            {\n",
    "                \"errors\": \"ignore\",\n",
    "                \"use_cache\": False,\n",
    "            },\n",
    "        ),\n",
    "        (TextNormalize, {}),\n",
    "        (DummyAnnotToPred, {}),\n",
    "        ],\n",
    "    \"models\": [],\n",
    "    \"postprocess\": [\n",
    "        (\n",
    "            FilterEntity,\n",
    "            {\n",
    "                # we skip DECISION because we nown that overlap with other entities\n",
    "                \"entities\": [\"DECISION\"],\n",
    "            },\n",
    "        )\n",
    "    ],\n",
    "    \"multiprocessing\": {},\n",
    "    \"use_cache\": True,\n",
    "    # 'log_level': 'debug'\n",
    "}\n",
    "\n",
    "pipeline = AymurAIPipeline(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.save('/resources/pipelines/examples/dummy-annot2pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = AymurAIPipeline.load('/resources/pipelines/examples/dummy-annot2pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = pipeline.preprocess(dataset)\n",
    "processed = pipeline.predict(processed)\n",
    "processed = pipeline.postprocess(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aymurai.utils.display import DocRender\n",
    "render = DocRender()\n",
    "\n",
    "render(processed[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
