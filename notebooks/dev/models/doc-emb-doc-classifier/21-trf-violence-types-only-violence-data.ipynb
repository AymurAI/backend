{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow_hub tensorflow-gpu tensorflow_text tensorflow-addons scikit-multilearn iterative-stratification simpletransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%load_ext aymurai.devtools.magic\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from aymurai.spacy.display import DocRender\n",
    "from aymurai.pipeline import AymurAIPipeline\n",
    "from aymurai.datasets.ar_juz_pcyf_10 import ArgentinaJuzgadoPCyF10Dataset\n",
    "\n",
    "locale.setlocale(locale.LC_ALL, 'es_AR.UTF-8')\n",
    "render = DocRender()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "private = ArgentinaJuzgadoPCyF10Dataset('private', use_cache=True)\n",
    "public = ArgentinaJuzgadoPCyF10Dataset('latest', use_cache=True)\n",
    "\n",
    "train, test = train_test_split(public, test_size=0.2, random_state=22)\n",
    "train, val = train_test_split(train, test_size=0.2, random_state=22)\n",
    "\n",
    "print('private', len(private))\n",
    "print('public', len(public))\n",
    "print('---')\n",
    "print('train:', len(train))\n",
    "print('test:', len(test))\n",
    "print('val:', len(val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from aymurai.meta.types import DataItem\n",
    "from aymurai.meta.pipeline_interfaces import Transform\n",
    "\n",
    "# CATEGORIES = ['v_fisica', 'v_econ', 'v_psic', 'v_sex', 'v_soc', 'v_amb', 'v_simb', 'v_polit']\n",
    "# CATEGORIES = ['violencia_de_genero', 'v_fisica', 'v_econ', 'v_psic', 'v_sex', 'v_soc', 'v_amb', 'v_simb', 'v_polit']\n",
    "CATEGORIES = ['violencia_de_genero', 'v_fisica', 'v_econ', 'v_psic', 'v_sex', 'v_soc', 'v_amb', 'v_simb']\n",
    "# CATEGORIES = ['v_fisica']\n",
    "\n",
    "class ViolenceDocCategoryParser(Transform):\n",
    "    categories = CATEGORIES\n",
    "    \n",
    "    def __call__(self, item: DataItem) -> DataItem:\n",
    "        item = deepcopy(item)\n",
    "        annotations = pd.DataFrame(item['annotations'])\n",
    "        annotations = annotations[self.categories].any().to_list()\n",
    "\n",
    "        item['data']['doc-cats'] = {\n",
    "            f'{cat}': int(value) for cat, value in zip(self.categories, annotations)\n",
    "        }\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aymurai.spacy.models.core import SpacyModel\n",
    "from aymurai.text.normalize import TextNormalize\n",
    "from aymurai.spacy.ruler import SpacyRulerPipeline\n",
    "from aymurai.text.extraction import FulltextExtract\n",
    "\n",
    "config = {\n",
    "    \"preprocess\": [\n",
    "        (\n",
    "            FulltextExtract,\n",
    "            {\n",
    "                \"extension\": \"pdf\",\n",
    "                \"method\": \"tesseract\",\n",
    "                \"language\": \"spa\",\n",
    "                \"errors\": \"ignore\",\n",
    "                \"use_cache\": True,\n",
    "            },\n",
    "        ),\n",
    "        (TextNormalize, {}),\n",
    "        (ViolenceDocCategoryParser, {}),\n",
    "    ],\n",
    "    \"models\": [\n",
    "    ],\n",
    "    \"postprocess\": [],\n",
    "    \"multiprocessing\": {},\n",
    "    \"use_cache\": False,\n",
    "    # 'log_level': 'debug'\n",
    "}\n",
    "\n",
    "pipeline = AymurAIPipeline(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "from sklearn.utils.validation import _num_samples\n",
    "from sklearn.utils import indexable, _safe_indexing\n",
    "from sklearn.model_selection._split import _validate_shuffle_split\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "\n",
    "\n",
    "def multilabel_train_test_split(*arrays,\n",
    "                                test_size=None,\n",
    "                                train_size=None,\n",
    "                                random_state=None,\n",
    "                                shuffle=True,\n",
    "                                stratify=None):\n",
    "    \"\"\"\n",
    "    Train test split for multilabel classification. Uses the algorithm from: \n",
    "    'Sechidis K., Tsoumakas G., Vlahavas I. (2011) On the Stratification of Multi-Label Data'.\n",
    "    \"\"\"\n",
    "    if stratify is None:\n",
    "        return train_test_split(*arrays, test_size=test_size,train_size=train_size,\n",
    "                                random_state=random_state, stratify=None, shuffle=shuffle)\n",
    "    \n",
    "    assert shuffle, \"Stratified train/test split is not implemented for shuffle=False\"\n",
    "    \n",
    "    n_arrays = len(arrays)\n",
    "    arrays = indexable(*arrays)\n",
    "    n_samples = _num_samples(arrays[0])\n",
    "    n_train, n_test = _validate_shuffle_split(\n",
    "        n_samples, test_size, train_size, default_test_size=0.25\n",
    "    )\n",
    "    cv = MultilabelStratifiedShuffleSplit(test_size=n_test, train_size=n_train, random_state=123)\n",
    "    train, test = next(cv.split(X=arrays[0], y=stratify))\n",
    "\n",
    "    return list(\n",
    "        chain.from_iterable(\n",
    "            (_safe_indexing(a, train), _safe_indexing(a, test)) for a in arrays\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset from private data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "dataset = private\n",
    "preprocessed = pipeline.preprocess(dataset)\n",
    "\n",
    "x = map(lambda x: x['data']['doc.text'], preprocessed)\n",
    "x = np.array(list(x))\n",
    "y = map(lambda x: list(x['data']['doc-cats'].values()), preprocessed)\n",
    "y = np.array(list(y))\n",
    "\n",
    "train, test = multilabel_train_test_split(dataset, test_size=0.2, random_state=22, stratify=y)\n",
    "train, val = multilabel_train_test_split(train, test_size=0.2, random_state=22)\n",
    "\n",
    "print('train:', len(train))\n",
    "print('test:', len(test))\n",
    "print('val:', len(val))\n",
    "\n",
    "preprocessed_train = pipeline.preprocess(train)\n",
    "preprocessed_val = pipeline.preprocess(val)\n",
    "preprocessed_test = pipeline.preprocess(test)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "x_train = map(lambda x: x['data']['doc.text'], preprocessed_train)\n",
    "x_train = np.array(list(x_train))\n",
    "y_train = map(lambda x: list(x['data']['doc-cats'].values()), preprocessed_train)\n",
    "y_train = np.array(list(y_train))\n",
    "\n",
    "x_val = map(lambda x: x['data']['doc.text'], preprocessed_val)\n",
    "x_val = np.array(list(x_val))\n",
    "y_val = map(lambda x: list(x['data']['doc-cats'].values()), preprocessed_val)\n",
    "y_val = np.array(list(y_val))\n",
    "\n",
    "x_test = map(lambda x: x['data']['doc.text'], preprocessed_test)\n",
    "x_test = np.array(list(x_test))\n",
    "y_test = map(lambda x: list(x['data']['doc-cats'].values()), preprocessed_test)\n",
    "y_test = np.array(list(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "from simpletransformers.classification import ClassificationArgs, ClassificationModel\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.DataFrame.from_dict({'text': x_train, 'labels': y_train.tolist()})\n",
    "eval_data = pd.DataFrame.from_dict({'text': x_val, 'labels': y_val.tolist()})\n",
    "# Preparing train data\n",
    "# train_data = [\n",
    "#     [\"Aragorn\", [1, 0, 0]],\n",
    "#     [\"Frodo\", [0, 1, 1]],\n",
    "#     [\"Gimli\", [1, 0, 1]],\n",
    "# ]\n",
    "# train_df = pd.DataFrame(train_data)\n",
    "# train_df.columns = [\"text\", \"labels\"]\n",
    "\n",
    "# # Preparing eval data\n",
    "# eval_data = [\n",
    "#     [\"Legolas\", [1, 0, 0]],\n",
    "#     [\"Merry\", [0, 0, 1]],\n",
    "#     [\"Eomer\", [1, 0, 0]],\n",
    "# ]\n",
    "# eval_df = pd.DataFrame(eval_data)\n",
    "# eval_df.columns = [\"text\", \"labels\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpletransformers.classification import (\n",
    "    MultiLabelClassificationArgs,\n",
    "    MultiLabelClassificationModel,\n",
    ")\n",
    "\n",
    "# Optional model configuration\n",
    "model_args = MultiLabelClassificationArgs(num_train_epochs=1, sliding_window=True)\n",
    "model_args.use_early_stopping = True\n",
    "model_args.early_stopping_delta = 0.01\n",
    "# model_args.early_stopping_metric = \"eval_loss\"\n",
    "model_args.early_stopping_metric = \"LRAP\"\n",
    "model_args.early_stopping_metric_minimize = False\n",
    "model_args.early_stopping_patience = 5\n",
    "model_args.evaluate_during_training_steps = 1000\n",
    "model_args.regression = False\n",
    "# model_args.overwrite_output_dir = True\n",
    "model_args.output_dir = 'beto-01'\n",
    "\n",
    "\n",
    "# class weights\n",
    "class_weights = np.array([w for k, w in enumerate(len(y_train)/y_train[:,1:].sum(axis=0))])\n",
    "class_weights /= class_weights.min()\n",
    "class_weights = [1] + list(class_weights)\n",
    "\n",
    "# Create a MultiLabelClassificationModel\n",
    "model = MultiLabelClassificationModel(\n",
    "    # \"roberta\",\n",
    "    # \"outputs/checkpoint-15489-epoch-3\",\n",
    "    'bert',\n",
    "    # 'dccuchile/bert-base-spanish-wwm-uncased',\n",
    "    'beto-01/checkpoint-3231-epoch-1',\n",
    "    num_labels=8,\n",
    "    args=model_args,\n",
    "    pos_weight=class_weights,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.train_model(train_data, eval_df=eval_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
    "\n",
    "print('TRAIN')\n",
    "\n",
    "reference = y_train\n",
    "# hypothesis = model.predict(x_train)\n",
    "predictions, raw_outputs = model.predict(train_data['text'])\n",
    "hypothesis = np.array([np.max(p, axis=0) for p in raw_outputs])\n",
    "\n",
    "fig, subplot = plt.subplots(3, 4, figsize=(15, 8))\n",
    "\n",
    "confusion = multilabel_confusion_matrix(reference, hypothesis > 0.5)\n",
    "for ax, matrix, cat in zip(subplot.flatten(), confusion, CATEGORIES):\n",
    "    sns.heatmap(matrix, annot=True, fmt='d', ax=ax)\n",
    "    ax.set_xlabel(\"hypothesis\")\n",
    "    ax.set_ylabel(\"reference\")\n",
    "    ax.set_xticklabels([\"false\", \"true\"])\n",
    "    ax.set_yticklabels([\"false\", \"true\"])\n",
    "    ax.set_title(cat)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "report = classification_report(reference, hypothesis > 0.5, output_dict=True, target_names=CATEGORIES)\n",
    "pd.DataFrame(report).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
    "\n",
    "print('eval')\n",
    "\n",
    "reference = y_val\n",
    "# hypothesis = model.predict(x_train)\n",
    "predictions, raw_outputs = model.predict(eval_data['text'])\n",
    "hypothesis = np.array([np.max(p, axis=0) for p in raw_outputs])\n",
    "\n",
    "fig, subplot = plt.subplots(3, 4, figsize=(15, 8))\n",
    "\n",
    "confusion = multilabel_confusion_matrix(reference, hypothesis > 0.5)\n",
    "for ax, matrix, cat in zip(subplot.flatten(), confusion, CATEGORIES):\n",
    "    sns.heatmap(matrix, annot=True, fmt='d', ax=ax)\n",
    "    ax.set_xlabel(\"hypothesis\")\n",
    "    ax.set_ylabel(\"reference\")\n",
    "    ax.set_xticklabels([\"false\", \"true\"])\n",
    "    ax.set_yticklabels([\"false\", \"true\"])\n",
    "    ax.set_title(cat)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "report = classification_report(reference, hypothesis > 0.5, output_dict=True, target_names=CATEGORIES)\n",
    "pd.DataFrame(report).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
    "\n",
    "print('test')\n",
    "\n",
    "reference = y_test\n",
    "# hypothesis = model.predict(x_train)\n",
    "predictions, raw_outputs = model.predict(x_test)\n",
    "hypothesis = np.array([np.max(p, axis=0) for p in raw_outputs])\n",
    "\n",
    "fig, subplot = plt.subplots(3, 4, figsize=(15, 8))\n",
    "\n",
    "confusion = multilabel_confusion_matrix(reference, hypothesis > 0.5)\n",
    "for ax, matrix, cat in zip(subplot.flatten(), confusion, CATEGORIES):\n",
    "    sns.heatmap(matrix, annot=True, fmt='d', ax=ax)\n",
    "    ax.set_xlabel(\"hypothesis\")\n",
    "    ax.set_ylabel(\"reference\")\n",
    "    ax.set_xticklabels([\"false\", \"true\"])\n",
    "    ax.set_yticklabels([\"false\", \"true\"])\n",
    "    ax.set_title(cat)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "report = classification_report(reference, hypothesis > 0.5, output_dict=True, target_names=CATEGORIES)\n",
    "pd.DataFrame(report).T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
