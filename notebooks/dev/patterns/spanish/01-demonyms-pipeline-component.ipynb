{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%load_ext aymurai.devtools.magic\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "import random\n",
    "\n",
    "from aymurai.pipeline import AymurAIPipeline\n",
    "from aymurai.datasets.ar_juz_pcyf_10 import ArgentinaJuzgadoPCyF10Dataset\n",
    "from aymurai.spacy.display import DocRender\n",
    "\n",
    "render = DocRender()\n",
    "\n",
    "locale.setlocale(locale.LC_ALL, 'es_AR.UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def demonym_annotated(item) -> bool:\n",
    "    annotations = item['annotations']\n",
    "    genders = [x['nacionalidad_acusado/a'] for x in annotations]\n",
    "    genders += [x['nacionalidad_denunciante'] for x in annotations]\n",
    "    genders = filter(bool, genders)\n",
    "    genders = list(genders)\n",
    "    return bool(genders)\n",
    "\n",
    "\n",
    "private = ArgentinaJuzgadoPCyF10Dataset('private')\n",
    "private = filter(demonym_annotated, private)\n",
    "private = list(private)\n",
    "sample, _ = train_test_split(private, train_size=20, random_state=22)\n",
    "\n",
    "private = ArgentinaJuzgadoPCyF10Dataset('private', use_cache=True)\n",
    "train, test = train_test_split(private, test_size=0.2, random_state=22)\n",
    "train, val = train_test_split(train, test_size=0.2, random_state=22)\n",
    "print('train:', len(train))\n",
    "print('test:', len(test))\n",
    "print('val:', len(val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define spacy component\n",
    "\n",
    "We define an entity ruler using the demonyms created on `00-demonyms-database.ipynb`\n",
    "\n",
    "> `dev note:`\n",
    ">\n",
    "> using the `%%export` magic from `aymurai.devtools.magic`, we can build the aymurai submodule component directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%export aymurai.spacy.components.es.norp_ruler\n",
    "\n",
    "from glob import glob\n",
    "import os  # export: hide\n",
    "import shutil  # export: hide\n",
    "\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from spacy.pipeline import EntityRuler\n",
    "\n",
    "from aymurai.devtools import resolve_package_path\n",
    "\n",
    "DEMONYMS_BASEPATH = resolve_package_path('aymurai.data.spanish.demonyms')\n",
    "\n",
    "# export: start hide\n",
    "os.makedirs(DEMONYMS_BASEPATH, exist_ok=True)\n",
    "shutil.copy('es-demonyms-global.csv', DEMONYMS_BASEPATH)\n",
    "# export: end hide\n",
    "\n",
    "database = pd.concat(\n",
    "    [pd.read_csv(path) for path in glob(f'{DEMONYMS_BASEPATH}/*.csv')],\n",
    "    ignore_index=True,\n",
    ")\n",
    "DEMONYMS = database[\"demonym\"].to_list()\n",
    "\n",
    "\n",
    "@spacy.language.Language.factory(\"aymurai_norp_ruler\")\n",
    "def es_norp_ruler(nlp, name):\n",
    "    ruler = EntityRuler(\n",
    "        nlp,\n",
    "        patterns=[{\"label\": \"NORP\", \"pattern\": demonym} for demonym in DEMONYMS],\n",
    "    )\n",
    "    return ruler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aymurai.spacy.ruler import SpacyRulerPipeline\n",
    "from aymurai.text.extraction import FulltextExtract\n",
    "from aymurai.text.normalize import JunkCleaner, TextNormalize\n",
    "\n",
    "# import aymurai.spacy.components\n",
    "\n",
    "config = {\n",
    "    \"preprocess\": [\n",
    "        (\n",
    "            FulltextExtract,\n",
    "            {\n",
    "                \"extension\": \"pdf\",\n",
    "                \"method\": \"tesseract\",\n",
    "                \"language\": \"spa\",\n",
    "                \"errors\": \"ignore\",\n",
    "                \"use_cache\": True,\n",
    "            },\n",
    "        ),\n",
    "        (TextNormalize, {}),\n",
    "        (\n",
    "            JunkCleaner,\n",
    "            {\n",
    "                \"patterns\": [\n",
    "                    \"Juzgado PCyF N* 10 - TacuarÃ­ 138, 7* Piso - juzcyf10ejusbaires.gob.ar - 4014-6821/20 - Gipcyf10\",\n",
    "                ]\n",
    "            },\n",
    "        ),\n",
    "        (\n",
    "            SpacyRulerPipeline,\n",
    "            {\n",
    "                \"base\": \"es\",\n",
    "                \"steps\": [\n",
    "                    (\"aymurai_norp_ruler\", {}),\n",
    "                ],\n",
    "            },\n",
    "        ),\n",
    "    ],\n",
    "    \"models\": [],\n",
    "    \"postprocess\": [],\n",
    "    \"multiprocessing\": {},\n",
    "    \"use_cache\": False,\n",
    "    # 'log_level': 'debug'\n",
    "}\n",
    "\n",
    "pipeline = AymurAIPipeline(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = pipeline.preprocess(train)\n",
    "result = pipeline.predict(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import spacy\n",
    "from more_itertools import flatten\n",
    "\n",
    "norp = filter(lambda x: any(map(lambda y: y['label'] == 'NORP', x['data']['entities'])), result)\n",
    "norp = filter(lambda x: len(x['annotations']) > 1, norp)\n",
    "norp = list(norp)\n",
    "\n",
    "registry = norp[9]\n",
    "# registry = result[15]\n",
    "metadata = {k: v for k, v in registry['metadata'].items() if type(v) not in [dict, list]}\n",
    "print(registry['path'])\n",
    "print(json.dumps(metadata, indent=4))\n",
    "print('annotated demonyms')\n",
    "\n",
    "gender1 = map(lambda x: x['nacionalidad_acusado/a'], registry['annotations'])\n",
    "gender2 = map(lambda x: x['nacionalidad_denunciante'], registry['annotations'])\n",
    "print(list(flatten([gender1, gender2])))\n",
    "\n",
    "print('\\n-------\\n')\n",
    "render(registry, 'span', spans_key='section')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.DataFrame(registry['annotations'])\n",
    "campos_de_interes = ['genero_acusado/a', 'persona_acusada_no_determinada', 'nacionalidad_acusado/a', 'edad_acusado/a al momento del hecho', 'nivel_instruccion_acusado/a' ,'genero_denunciante', 'nacionalidad_denunciante', 'edad_denunciante_al_momento_del_hecho', 'nivel_instruccion_denunciante', 'domicilio_denunciante']\n",
    "df[campos_de_interes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from more_itertools import collapse\n",
    "import pickle\n",
    "\n",
    "paths = pd.read_csv('casos-lio.csv', header=None).values\n",
    "paths = collapse(paths)\n",
    "paths = list(paths)\n",
    "print(paths)\n",
    "\n",
    "muestra = filter(lambda x: x['path'] in paths, preprocess)\n",
    "muestra = list(muestra)\n",
    "muestra\n",
    "with open('muestra.pickle', 'wb') as file:\n",
    "    pickle.dump(muestra, file)\n",
    "\n",
    "preprocess = pipeline.preprocess(test)\n",
    "result = pipeline.predict(preprocess)\n",
    "\n",
    "with open('preprocessed-test.pickle', 'wb') as file:\n",
    "    pickle.dump(preprocess, file)\n",
    "\n",
    "preprocess = pipeline.preprocess(val)\n",
    "result = pipeline.predict(preprocess)\n",
    "\n",
    "with open('preprocessed-val.pickle', 'wb') as file:\n",
    "    pickle.dump(preprocess, file)\n",
    "\n",
    "preprocess = pipeline.preprocess(train)\n",
    "result = pipeline.predict(preprocess)\n",
    "\n",
    "with open('preprocessed-train.pickle', 'wb') as file:\n",
    "    pickle.dump(preprocess, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
