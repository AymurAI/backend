{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%load_ext aymurai.devtools.magic\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from aymurai.spacy.display import DocRender\n",
    "from aymurai.pipeline import AymurAIPipeline\n",
    "from aymurai.datasets.ar_juz_pcyf_10 import ArgentinaJuzgadoPCyF10Dataset\n",
    "\n",
    "colors = {\n",
    "    'SECTION:DECISION': 'red',\n",
    "    'KEYWORDS': 'blue'\n",
    "\n",
    "}\n",
    "render = DocRender(config={'colors': colors})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "private = ArgentinaJuzgadoPCyF10Dataset('private', use_cache=True)\n",
    "train, test = train_test_split(private, test_size=0.2, random_state=22)\n",
    "train, val = train_test_split(train, test_size=0.2, random_state=22)\n",
    "print('train:', len(train))\n",
    "print('test:', len(test))\n",
    "print('val:', len(val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pipeline definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%export aymurai.models.dummy.n_expte_eje\n",
    "import re\n",
    "from copy import deepcopy\n",
    "\n",
    "from more_itertools import unique_justseen\n",
    "\n",
    "from aymurai.meta.types import DataItem, DataBlock\n",
    "from aymurai.meta.pipeline_interfaces import TrainModule\n",
    "\n",
    "\n",
    "class DummyExtractorExpediente(TrainModule):\n",
    "    def save(self, path: str):\n",
    "        return\n",
    "\n",
    "    def load(self, path: str):\n",
    "        return\n",
    "\n",
    "    def fit(self, train: DataBlock, val: DataBlock):\n",
    "        return\n",
    "\n",
    "    def predict(self, data: DataBlock) -> DataBlock:\n",
    "        data = [self.predict_single(item) for item in data]\n",
    "\n",
    "        return data\n",
    "\n",
    "    def predict_single(self, item: DataItem) -> DataItem:\n",
    "        item = deepcopy(item)\n",
    "\n",
    "        # format prediction\n",
    "        if \"predictions\" not in item:\n",
    "            item[\"predictions\"] = {}\n",
    "        if \"records\" not in item[\"predictions\"]:\n",
    "            item[\"predictions\"][\"records\"] = {}\n",
    "        if \"entities\" not in item[\"predictions\"]:\n",
    "            item[\"predictions\"][\"entities\"] = []\n",
    "        if \"doc-cats\" not in item[\"predictions\"]:\n",
    "            item[\"predictions\"][\"doc-cats\"] = {}\n",
    "        item[\"predictions\"][\"doc-cats\"][\"n_expte_eje\"] = None\n",
    "        item[\"predictions\"][\"records\"][\"n_expte_eje\"] = []\n",
    "\n",
    "        ents = []\n",
    "        if \"entities\" in item[\"data\"]:\n",
    "            ents += item[\"data\"][\"entities\"]\n",
    "\n",
    "        ents = filter(lambda x: x[\"label\"] == \"N_EXPTE_EJE\", ents)\n",
    "        ents = sorted(ents, key=lambda e: e[\"attrs\"][\"aymurai_score\"])\n",
    "        ents = list(ents)\n",
    "\n",
    "        # if there is no entities just pass\n",
    "        if not ents:\n",
    "            return item\n",
    "\n",
    "        parser = re.compile(r\"(?P<exp>\\d+)/(?P<year>\\d+)(?P<code>-\\d)?\")\n",
    "        exptes = map(lambda x: parser.search(x[\"text\"]), ents)\n",
    "        exptes = list(exptes)\n",
    "\n",
    "        if not exptes:\n",
    "            return item\n",
    "\n",
    "        for ent, expte in zip(ents, exptes):\n",
    "            char_offset = expte.span()\n",
    "            subtext = expte[0]\n",
    "            text = ent[\"text\"]\n",
    "            subpre = text[: char_offset[0]]\n",
    "            subpost = text[char_offset[1] :]\n",
    "\n",
    "            ent[\"context_pre\"] += subpre\n",
    "            ent[\"context_post\"] = subpost + ent[\"context_post\"]\n",
    "            ent[\"end_char\"] = ent[\"start_char\"] + char_offset[1]\n",
    "            ent[\"start_char\"] += char_offset[0]\n",
    "\n",
    "            # FIXME: should use spacy tokenizer instead split with spaces!\n",
    "            tokenspre = list(filter(bool, subpre.split(\" \")))\n",
    "            tokenspost = list(filter(bool, subpost.split(\" \")))\n",
    "            ent[\"start\"] += len(tokenspre)\n",
    "            ent[\"end\"] -= len(tokenspost)\n",
    "            ent[\"text\"] = subtext\n",
    "\n",
    "        # get first prediction\n",
    "        # span = dates[0]\n",
    "        # year = span[\"attrs\"][\"aymurai_date\"].strftime(\"%Y\")\n",
    "\n",
    "        item[\"predictions\"][\"entities\"].append(ents)\n",
    "        item[\"predictions\"][\"records\"][\"n_expte_eje\"] += [ent[\"text\"] for ent in ents]\n",
    "        item[\"predictions\"][\"doc-cats\"][\"n_expte_eje\"] = ents[0][\"text\"]\n",
    "\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aymurai.spacy.components\n",
    "from aymurai.text.normalize import TextNormalize\n",
    "from aymurai.spacy.ruler import SpacyRulerPipeline\n",
    "from aymurai.text.extraction import FulltextExtract\n",
    "from aymurai.models.dummy.n_expte_eje import DummyExtractorExpediente\n",
    "from aymurai.spacy.rulers.section_parser import AymuraiRulerSectionParser\n",
    "\n",
    "config = {\n",
    "    \"preprocess\": [\n",
    "        (\n",
    "            FulltextExtract,\n",
    "            {\n",
    "                \"extension\": \"pdf\",\n",
    "                \"method\": \"tesseract\",\n",
    "                \"language\": \"spa\",\n",
    "                \"errors\": \"ignore\",\n",
    "                \"use_cache\": True,\n",
    "            },\n",
    "        ),\n",
    "        (TextNormalize, {}),\n",
    "        (\n",
    "            SpacyRulerPipeline,\n",
    "            {\n",
    "                \"base\": \"es\",\n",
    "                \"steps\": [\n",
    "                    (\n",
    "                        \"enhanced_regex_ruler\",\n",
    "                        {\n",
    "                            \"patterns\": {\n",
    "                                \"N_EXPTE_EJE\": [\n",
    "                                    r\"(?i)causa\\s*(n.)?\\s*\\d+/%Y(-\\d)?\",\n",
    "                                    r\"(?i)causa\\s*(n.)?\\s*\\d+/%y(-\\d)?\",\n",
    "                                    r\"(?i)caso\\s*(n.)?\\s*\\d+/%Y(-\\d)?\",\n",
    "                                    r\"(?i)caso\\s*(n.)?\\s*\\d+/%y(-\\d)?\",\n",
    "                                    r'EXP:\\s*\\d+/%Y(-\\d)?',\n",
    "                                    r'EXP:\\s*\\d+/%y(-\\d)?',\n",
    "                                    r'IPP?\\s*\\d+/%Y(-\\d)?',\n",
    "                                    r'IPP?\\s*\\d+/%y(-\\d)?'\n",
    "                                ],\n",
    "                            },\n",
    "                        },\n",
    "                    ),\n",
    "                ],\n",
    "            },\n",
    "        ),\n",
    "    ],\n",
    "    \"models\": [\n",
    "        (DummyExtractorExpediente, {})\n",
    "    ],\n",
    "    \"postprocess\": [],\n",
    "    \"multiprocessing\": {},\n",
    "    \"use_cache\": False,\n",
    "    # 'log_level': 'debug'\n",
    "}\n",
    "\n",
    "pipeline = AymurAIPipeline(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = pipeline.preprocess(train)\n",
    "result = pipeline.predict(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "index = random.choice(range(len(result)))\n",
    "index = 854\n",
    "registry = result[index]\n",
    "print('index', index)\n",
    "print(registry['path'])\n",
    "metadata = {k: v for k, v in registry['metadata'].items() if type(v) not in [dict, list]}\n",
    "print(json.dumps(metadata, indent=4))\n",
    "\n",
    "\n",
    "print('annotations')\n",
    "print('expte_eje:', [x['n_expte_eje'] for x in registry['annotations']])\n",
    "print('prediction')\n",
    "print(registry['predictions']['doc-cats'])\n",
    "\n",
    "\n",
    "print('\\n-------\\n')\n",
    "render(registry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = pipeline.preprocess(train)\n",
    "result = pipeline.predict(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from more_itertools import collapse\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from aymurai.meta.types import DataItem\n",
    "from jiwer import cer\n",
    "\n",
    "\n",
    "def annot_dataframe(item: DataItem) -> pd.DataFrame:\n",
    "    path = item['path']\n",
    "    annots = item['annotations']\n",
    "    df = pd.DataFrame(annots)\n",
    "    df.insert(0, 'path', path)\n",
    "\n",
    "    df = df[['path', 'n_expte_eje']]\n",
    "    return df\n",
    "\n",
    "def preds_dataframe(item: DataItem) -> pd.DataFrame:\n",
    "    path = item['path']\n",
    "    preds = item['predictions']['doc-cats']\n",
    "    df = pd.DataFrame([preds])\n",
    "    df.insert(0, 'path', path)\n",
    "    return df\n",
    "\n",
    "references = pd.concat(map(annot_dataframe, result), ignore_index=True)\n",
    "references = references.rename(columns={'n_expte_eje': 'reference'})\n",
    "\n",
    "hypotheses = pd.concat(map(preds_dataframe, result), ignore_index=True)\n",
    "hypotheses = hypotheses.rename(columns={'n_expte_eje': 'hypothesis'})\n",
    "hypotheses.fillna('', inplace=True)\n",
    "\n",
    "raw = pd.merge(references, hypotheses, on='path')\n",
    "print('raw cer:', cer(raw['reference'].tolist(), raw['hypothesis'].tolist()))\n",
    "raw['cer'] = raw.apply(lambda row: cer(row['reference'], row['hypothesis']), axis=1)\n",
    "raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "fixed = raw.copy()\n",
    "fixed['reference'] = fixed['reference'].apply(lambda s: s.replace('_', '/'))\n",
    "\n",
    "def short_to_long_years(text: str):\n",
    "    parser = re.compile(r\"(?P<exp>\\d+)/(?P<year>\\d+)(?P<code>-\\d)?\")\n",
    "    matches = parser.findall(text)\n",
    "    if not matches:\n",
    "        return text\n",
    "    matches = matches[0]\n",
    "    expt, year, code = matches\n",
    "    if len(year) == 2:\n",
    "        year = datetime.strptime(year, '%y').year\n",
    "    \n",
    "    code = '' if code == '-0' else code\n",
    "    return f'{expt}/{year}{code}'\n",
    "\n",
    "fixed['hypothesis'] = fixed['hypothesis'].apply(short_to_long_years)\n",
    "print('fixed cer:', cer(fixed['reference'].tolist(), fixed['hypothesis'].tolist()))\n",
    "fixed['cer'] = fixed.apply(lambda row: cer(row['reference'], row['hypothesis']), axis=1)\n",
    "fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = pipeline.preprocess(test)\n",
    "result = pipeline.predict(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from more_itertools import collapse\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from aymurai.meta.types import DataItem\n",
    "from jiwer import cer\n",
    "\n",
    "\n",
    "def annot_dataframe(item: DataItem) -> pd.DataFrame:\n",
    "    path = item['path']\n",
    "    annots = item['annotations']\n",
    "    df = pd.DataFrame(annots)\n",
    "    df.insert(0, 'path', path)\n",
    "\n",
    "    df = df[['path', 'n_expte_eje']]\n",
    "    return df\n",
    "\n",
    "def preds_dataframe(item: DataItem) -> pd.DataFrame:\n",
    "    path = item['path']\n",
    "    preds = item['predictions']['doc-cats']\n",
    "    df = pd.DataFrame([preds])\n",
    "    df.insert(0, 'path', path)\n",
    "    return df\n",
    "\n",
    "references = pd.concat(map(annot_dataframe, result), ignore_index=True)\n",
    "references = references.rename(columns={'n_expte_eje': 'reference'})\n",
    "\n",
    "hypotheses = pd.concat(map(preds_dataframe, result), ignore_index=True)\n",
    "hypotheses = hypotheses.rename(columns={'n_expte_eje': 'hypothesis'})\n",
    "hypotheses.fillna('', inplace=True)\n",
    "\n",
    "raw = pd.merge(references, hypotheses, on='path')\n",
    "print('raw cer:', cer(raw['reference'].tolist(), raw['hypothesis'].tolist()))\n",
    "raw['cer'] = raw.apply(lambda row: cer(row['reference'], row['hypothesis']), axis=1)\n",
    "raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "fixed = raw.copy()\n",
    "fixed['reference'] = fixed['reference'].apply(lambda s: s.replace('_', '/'))\n",
    "\n",
    "def short_to_long_years(text: str):\n",
    "    parser = re.compile(r\"(?P<exp>\\d+)/(?P<year>\\d+)(?P<code>-\\d)?\")\n",
    "    matches = parser.findall(text)\n",
    "    if not matches:\n",
    "        return text\n",
    "    matches = matches[0]\n",
    "    expt, year, code = matches\n",
    "    if len(year) == 2:\n",
    "        year = datetime.strptime(year, '%y').year\n",
    "    \n",
    "    code = '' if code == '-0' else code\n",
    "    return f'{expt}/{year}{code}'\n",
    "\n",
    "fixed['hypothesis'] = fixed['hypothesis'].apply(short_to_long_years)\n",
    "print('fixed cer:', cer(fixed['reference'].tolist(), fixed['hypothesis'].tolist()))\n",
    "fixed['cer'] = fixed.apply(lambda row: cer(row['reference'], row['hypothesis']), axis=1)\n",
    "fixed['exact-match'] = fixed['reference'] == fixed['hypothesis']\n",
    "print('exact-match', fixed['exact-match'].mean())\n",
    "fixed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
