{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "databases = glob('/resources/data/facebook-names/curate/*.csv')\n",
    "databases = {os.path.basename(file).split('.')[0]: file for file in databases}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def join_names(row):\n",
    "    fname = row['first_name'] or ''\n",
    "    lname = row['last_name'] or ''\n",
    "\n",
    "    return f'{fname} {lname}'\n",
    "\n",
    "ar = pd.read_csv(databases['AR'], names=['first_name', 'last_name', 'gender', 'loc'])\n",
    "\n",
    "\n",
    "\n",
    "ar['full_name'] = ar[['first_name', 'last_name']].apply(join_names, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def norm(name: str):\n",
    "    if not isinstance(name, str):\n",
    "        return name\n",
    "    # spliting in space except on compose names\n",
    "    name = re.sub(r'(?i)((?<!\\W(De|Da|Di))\\s)', '|', name)\n",
    "    # fixing some prefixes\n",
    "    name = re.sub(r'(?i)(?<!\\w)(San|Del|las?)(\\|)', '\\g<1> ', name)\n",
    "    \n",
    "    # splitting names\n",
    "    name = name.split('|')\n",
    "    return name\n",
    "\n",
    "\n",
    "ar_ = ar.copy()\n",
    "ar_['first_name'] = ar_['first_name'].apply(norm)\n",
    "ar_['last_name'] = ar_['last_name'].apply(norm)\n",
    "\n",
    "ar_['name'] = ar_['first_name'] + ar_['last_name']\n",
    "# ar['first_name'] = ar['first_name'].apply(norm)\n",
    "# ar['last_name'] = ar['last_name'].apply(norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_ = ar_.explode('name')\n",
    "ar_.drop(['first_name', 'last_name'], axis=1, inplace=True)\n",
    "ar_.dropna(subset=['name'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = ar_.groupby('name').agg({'full_name': 'count'}).sort_values(by=['full_name'], ascending=False)\n",
    "# counts = ar_.groupby('name').transform('count')\n",
    "# counts /= counts.sum()\n",
    "counts = counts.reset_index()\n",
    "counts.rename(columns={'full_name': 'counts'}, inplace=True)\n",
    "counts['len'] = counts['name'].apply(len)\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try in pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from aymurai.spacy.display import DocRender\n",
    "from aymurai.pipeline import AymurAIPipeline\n",
    "from aymurai.datasets.ar_juz_pcyf_10 import ArgentinaJuzgadoPCyF10Dataset\n",
    "\n",
    "locale.setlocale(locale.LC_ALL, 'es_AR.UTF-8')\n",
    "render = DocRender()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "private = ArgentinaJuzgadoPCyF10Dataset('private', use_cache=True)\n",
    "train, test = train_test_split(private, test_size=0.2, random_state=22)\n",
    "train, val = train_test_split(train, test_size=0.2, random_state=22)\n",
    "print('train:', len(train))\n",
    "print('test:', len(test))\n",
    "print('val:', len(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext aymurai.devtools.magic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ruler definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%export aymurai.spacy.components.names\n",
    "\n",
    "import unicodedata\n",
    "from itertools import chain\n",
    "\n",
    "from unidecode import unidecode\n",
    "from spacy.language import Language\n",
    "from spacy.pipeline import EntityRuler\n",
    "\n",
    "\n",
    "@Language.factory(name=\"name_lookup_ruler\")\n",
    "def name_lookup(\n",
    "    nlp,\n",
    "    name,\n",
    "    country_codes=[\"AR\"],\n",
    "    overwrite_ents: bool = True,\n",
    "    min_freq: int = 100,\n",
    "    min_name_length: int = 4,\n",
    "    unicode_norm: str = \"NFKC\",\n",
    "):\n",
    "    db_names = counts.query(f\"counts > {min_freq} and len >= {min_name_length}\")[\"name\"]\n",
    "    with open('extra_names.txt', 'r') as file:\n",
    "        extra_names = file.readlines()\n",
    "        extra_names = map(str.strip, extra_names)\n",
    "        extra_names = map(str.title, extra_names)\n",
    "    \n",
    "    names = chain(db_names, extra_names)\n",
    "\n",
    "    norm_names = map(lambda x: unicodedata.normalize(unicode_norm, x), names)\n",
    "    norm_names = tuple(norm_names)\n",
    "    ascii_names = map(lambda x: unidecode(x), norm_names)\n",
    "\n",
    "    names = tuple(chain(norm_names, ascii_names))\n",
    "    names = chain(names, map(str.upper, names))\n",
    "\n",
    "\n",
    "    ruler = EntityRuler(nlp, name=name, overwrite_ents=overwrite_ents)\n",
    "    ruler.add_patterns(\n",
    "        [\n",
    "            {\n",
    "                \"label\": \"PER\",\n",
    "                \"id\": \"PER\",\n",
    "                \"pattern\": [{\"ORTH\": name}],\n",
    "            }\n",
    "            for name in names\n",
    "        ]\n",
    "    )\n",
    "    return ruler\n",
    "\n",
    "@Language.factory('join_consecutive_names')\n",
    "def join_consecutive_name_entities(nlp, name):\n",
    "    ruler = EntityRuler(nlp, name=name, overwrite_ents=True)\n",
    "    ruler.add_patterns(\n",
    "        [\n",
    "            # explicit consecutive names\n",
    "            {\n",
    "                \"label\": \"PER\",\n",
    "                \"id\": \"PER\",\n",
    "                \"pattern\": [{\"ENT_TYPE\": \"PER\", \"OP\": \"{2,}\"}],\n",
    "            },\n",
    "            # comma/dot separated\n",
    "            {\n",
    "                \"label\": \"PER\",\n",
    "                \"id\": \"PER\",\n",
    "                \"pattern\": [\n",
    "                    {\"ENT_TYPE\": \"PER\", \"OP\": \"+\"},\n",
    "                    {\"IS_PUNCT\": True},\n",
    "                    {\"ENT_TYPE\": \"PER\", \"OP\": \"+\"},\n",
    "                ],\n",
    "            },\n",
    "            # names with abbrvs within\n",
    "            {\n",
    "                \"label\": \"PER\",\n",
    "                \"id\": \"PER\",\n",
    "                \"pattern\": [\n",
    "                    {\"ENT_TYPE\": \"PER\", \"OP\": \"+\"},\n",
    "                    {\"TEXT\": {\"REGEX\": r\"[A-Z][\\.\\s\\,]?\"}},\n",
    "                    {\"ENT_TYPE\": \"PER\", \"OP\": \"+\"},\n",
    "                ],\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "    return ruler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.language import Language\n",
    "from spacy.pipeline import EntityRuler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = counts.query('counts > 100 and len >= 3')['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names.values[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aymurai.spacy.ner import SpacyNER\n",
    "from aymurai.text.normalize import TextNormalize\n",
    "from aymurai.spacy.ruler import SpacyRulerPipeline\n",
    "from aymurai.text.extraction import FulltextExtract\n",
    "from aymurai.pipeline.pipeline import AymurAIPipeline\n",
    "\n",
    "config = {\n",
    "    \"preprocess\": [\n",
    "        (\n",
    "            FulltextExtract,\n",
    "            {\n",
    "                \"extension\": \"pdf\",\n",
    "                \"method\": \"tesseract\",\n",
    "                \"language\": \"spa\",\n",
    "                \"errors\": \"ignore\",\n",
    "                \"use_cache\": True,\n",
    "            },\n",
    "        ),\n",
    "        (TextNormalize, {}),\n",
    "        # (\n",
    "        #     SpacyRulerPipeline,\n",
    "        #     {\n",
    "        #         \"base\": \"es\",\n",
    "        #         \"steps\": [\n",
    "        #             (\"name_lookup_ruler\", {'country_codes': ['AR']})\n",
    "        #         ],\n",
    "        #     },\n",
    "        # ),\n",
    "    ],\n",
    "    \"models\": [],\n",
    "    \"postprocess\": [],\n",
    "    \"multiprocessing\": {},\n",
    "    \"use_cache\": False,\n",
    "    # 'log_level': 'debug'\n",
    "}\n",
    "\n",
    "pipeline = AymurAIPipeline(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_train = pipeline.preprocess(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tryout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.blank('es')\n",
    "# nlp.add_pipe('name_lookup_ruler')\n",
    "# nlp.add_pipe('merge_entities')\n",
    "# ruler = name_lookup(nlp, 'asd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruler = name_lookup(nlp, 'names_lookup')\n",
    "join_entities = join_consecutive_name_entities(nlp, 'asd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "import srsly\n",
    "\n",
    "item = preprocessed_train[906]\n",
    "text = item['data']['doc.text']\n",
    "print(srsly.yaml_dumps(item))\n",
    "doc = nlp(text)\n",
    "doc = ruler(doc)\n",
    "doc = join_entities(doc)\n",
    "\n",
    "displacy.render(doc, 'ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
