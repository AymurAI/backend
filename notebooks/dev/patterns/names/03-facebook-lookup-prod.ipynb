{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext aymurai.devtools.magic\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from aymurai.spacy.display import DocRender\n",
    "from aymurai.pipeline import AymurAIPipeline\n",
    "from aymurai.datasets.ar_juz_pcyf_10 import ArgentinaJuzgadoPCyF10Dataset\n",
    "\n",
    "locale.setlocale(locale.LC_ALL, 'es_AR.UTF-8')\n",
    "render = DocRender()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "private = ArgentinaJuzgadoPCyF10Dataset('private', use_cache=True)\n",
    "train, test = train_test_split(private, test_size=0.2, random_state=22)\n",
    "train, val = train_test_split(train, test_size=0.2, random_state=22)\n",
    "print('train:', len(train))\n",
    "print('test:', len(test))\n",
    "print('val:', len(val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build names database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gdown --fuzzy --continue --output /resources/data/facebook-names/full.tar.bz2 https://drive.google.com/file/d/1wRQfw5EYpzulvRfHCGIUWB2am5JUYVGk/view\n",
    "# !cd /resources/data/facebook-names && tar xvf full.tar.bz2 && cd - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%export aymurai.utils.facebook_names\n",
    "\n",
    "import os\n",
    "import re\n",
    "import hashlib\n",
    "import tarfile\n",
    "import subprocess\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "\n",
    "import srsly\n",
    "import pandas as pd\n",
    "\n",
    "from aymurai.logging import get_logger\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "FACEBOOK_NAMES_URI = os.getenv(\n",
    "    \"FACEBOOK_NAMES_URI\",\n",
    "    \"https://drive.google.com/file/d/1wRQfw5EYpzulvRfHCGIUWB2am5JUYVGk/view\",\n",
    ")\n",
    "FACEBOOK_NAMES_PATH = os.getenv(\"FACEBOOK_NAMES_URI\", \"/resources/data/facebook-names/\")\n",
    "\n",
    "\n",
    "def download_fb_names_db() -> str:\n",
    "    \"\"\"\n",
    "    download facebook names database from massive 2021 hack\n",
    "\n",
    "    Returns:\n",
    "        str: path\n",
    "    \"\"\"\n",
    "    logger.info(f'getting facebook names database from {FACEBOOK_NAMES_URI}')\n",
    "    fname = \"full.tar.bz2\"\n",
    "    fpath = f\"{FACEBOOK_NAMES_PATH}/{fname}\"\n",
    "    cmd = f\"gdown --fuzzy --continue --output {fpath} {FACEBOOK_NAMES_URI}\"\n",
    "    subprocess.check_output(cmd.split())\n",
    "\n",
    "    return fpath\n",
    "\n",
    "\n",
    "def extract_fb_names(country_codes: list = [\"AR\"], use_cache: bool = True):\n",
    "    \"\"\"\n",
    "    Extract the facebook names from a list of country codes\n",
    "\n",
    "    Args:\n",
    "        country_codes (list, optional): extract names from <code> countries.\n",
    "            Defaults to [\"AR\"].\n",
    "        use_cache (bool, optional): use cache.\n",
    "            Defaults to True.\n",
    "    \"\"\"\n",
    "    tar_path = download_fb_names_db()\n",
    "    basepath = os.path.dirname(tar_path)\n",
    "\n",
    "    # check if files are extracted (use_cache = True)\n",
    "    extracted = glob(f\"{basepath}/curate/*.csv\")\n",
    "    extracted_codes = [os.path.basename(path).split(\".\")[0] for path in extracted]\n",
    "    if not use_cache:  # force to extract all\n",
    "        extracted_codes = []\n",
    "\n",
    "    tar_members = [\n",
    "        f\"curate/{code}.csv\" for code in country_codes if code not in extracted_codes\n",
    "    ]\n",
    "    with tarfile.open(tar_path, \"r:*\") as tar:\n",
    "        if tar_members:\n",
    "            logger.info(f'extracting {tar_members}')\n",
    "            tar.extractall(path=basepath, members=tar_members)\n",
    "\n",
    "    return [f\"{basepath}/curate/{code}.csv\" for code in country_codes]\n",
    "\n",
    "\n",
    "def norm(name: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Normalize and split a name to be used in a valid way .\n",
    "    FIXME: nomalization was designed with argentinian names in mind.\n",
    "    Not a general solution.\n",
    "\n",
    "    Args:\n",
    "        name (str): string containing multiples first/last names\n",
    "\n",
    "    Returns:\n",
    "        list[str]: list of names\n",
    "    \"\"\"\n",
    "    if not isinstance(name, str):\n",
    "        return name\n",
    "    # spliting in space except on compose names\n",
    "    name = re.sub(r\"(?i)((?<!\\W(De|Da|Di|D\\'|D\\\"))\\s)\", \"|\", name)\n",
    "    # fixing some prefixes\n",
    "    name = re.sub(r\"(?i)(?<!\\w)(San|Del|las?)(\\|)\", \"\\g<1> \", name)\n",
    "\n",
    "    # splitting names\n",
    "    name = name.split(\"|\")\n",
    "    return name\n",
    "\n",
    "\n",
    "def load_database(country_codes=[\"AR\"], use_cache: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a database of facebook names by country code.\n",
    "    FIXME: nomalization was designed with argentinian names in mind.\n",
    "     Not a general solution.\n",
    "\n",
    "    Args:\n",
    "        country_codes (list, optional): country codes to use. Defaults to [\"AR\"].\n",
    "        use_cache (bool, optional): use cache. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: names, gender and origin\n",
    "    \"\"\"\n",
    "    logger.info(f'loading facebook names database for {country_codes}')\n",
    "    paths = extract_fb_names(country_codes=country_codes, use_cache=use_cache)\n",
    "    db = pd.concat(\n",
    "        [\n",
    "            pd.read_csv(\n",
    "                path,\n",
    "                names=[\"first_name\", \"last_name\", \"gender\", \"loc\"],\n",
    "            )\n",
    "            for path in paths\n",
    "        ],\n",
    "        ignore_index=True,\n",
    "    )\n",
    "\n",
    "    fname = db[\"first_name\"].apply(norm)\n",
    "    lname = db[\"last_name\"].apply(norm)\n",
    "\n",
    "    db[\"name\"] = fname + lname\n",
    "\n",
    "    db = db.explode(\"name\")\n",
    "    db.dropna(subset=[\"name\"], inplace=True)\n",
    "\n",
    "    return db\n",
    "\n",
    "\n",
    "def load_counts(\n",
    "    country_codes: list = [\"AR\"],\n",
    "    min_freq: int = 100,\n",
    "    min_name_length: int = 4,\n",
    "    use_cache: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load facebook name counts for a list of country codes.\n",
    "\n",
    "    Args:\n",
    "        country_codes (list, optional): list of country codes to use. Defaults to ['AR'].\n",
    "        min_freq (int, optional): Minimum number of name repetitions. defaults to 100.\n",
    "        min_name_length (int, optional): minimum name length\n",
    "        use_cache (bool, optional): use cache. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: [description]\n",
    "    \"\"\"\n",
    "    cache_key = hashlib.md5(\n",
    "        srsly.json_dumps(sorted(country_codes)).encode()\n",
    "    ).hexdigest()\n",
    "    cache_path = f\"{FACEBOOK_NAMES_PATH}/counts/{cache_key}.csv\"\n",
    "\n",
    "    db = load_database(country_codes=country_codes, use_cache=use_cache)\n",
    "\n",
    "    if use_cache and Path(cache_path).exists():\n",
    "        logger.info(f'loading name counts from cache: {cache_path}')\n",
    "        counts = pd.read_csv(cache_path)\n",
    "\n",
    "    else:\n",
    "        logger.info(f'building names database')\n",
    "        counts = (\n",
    "            db.groupby(\"name\")\n",
    "            .agg({\"loc\": \"count\"})\n",
    "            .sort_values(by=[\"loc\"], ascending=False)\n",
    "        )\n",
    "        counts = counts.reset_index()\n",
    "        counts.rename(columns={\"loc\": \"counts\"}, inplace=True)\n",
    "        counts[\"len\"] = counts[\"name\"].apply(len)\n",
    "\n",
    "    if use_cache:\n",
    "        os.makedirs(os.path.dirname(cache_path), exist_ok=True)\n",
    "        counts.to_csv(cache_path, index=False)\n",
    "\n",
    "    counts.query(f\"counts > {min_freq} and len >= {min_name_length}\", inplace=True)\n",
    "\n",
    "    return counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define pipeline components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%export aymurai.spacy.components.names\n",
    "\n",
    "import unicodedata\n",
    "from itertools import chain\n",
    "\n",
    "from unidecode import unidecode\n",
    "from spacy.language import Language\n",
    "from spacy.pipeline import EntityRuler\n",
    "\n",
    "from aymurai.logging import get_logger\n",
    "from aymurai.utils.facebook_names import load_counts\n",
    "from aymurai.devtools import resolve_package_path\n",
    "from more_itertools import unique_everseen\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "EXTRANAMES_BASEPATH = resolve_package_path(\"aymurai.data.spanish.names\")\n",
    "EXTRANAMES_FILENAME = f\"{EXTRANAMES_BASEPATH}/extra_names.txt\"\n",
    "# export: start hide\n",
    "import os\n",
    "os.makedirs(EXTRANAMES_BASEPATH, exist_ok=True)\n",
    "extra_names_db = pd.read_csv(\"extra_names.txt\", header=None, sep=\"|\")\n",
    "extra_names_db = unique_everseen(extra_names_db[0].values)\n",
    "extra_names_db = list(extra_names_db)\n",
    "extra_names_db = pd.Series(extra_names_db)\n",
    "extra_names_db.to_csv(EXTRANAMES_FILENAME, index=False, header=None)\n",
    "# export: end hide\n",
    "\n",
    "@Language.factory(name=\"name_lookup_ruler\")\n",
    "def name_lookup(\n",
    "    nlp,\n",
    "    name,\n",
    "    country_codes=[\"AR\"],\n",
    "    overwrite_ents: bool = True,\n",
    "    min_freq: int = 100,\n",
    "    min_name_length: int = 4,\n",
    "    unicode_norm: str = \"NFKC\",\n",
    "):\n",
    "    db_names = load_counts(\n",
    "        country_codes=country_codes,\n",
    "        min_freq=min_freq,\n",
    "        min_name_length=min_name_length,\n",
    "    )['name']\n",
    "    with open(EXTRANAMES_FILENAME, \"r\") as file:\n",
    "        extra_names = file.readlines()\n",
    "        extra_names = map(str.strip, extra_names)\n",
    "        extra_names = map(str.title, extra_names)\n",
    "\n",
    "    names = chain(db_names, extra_names)\n",
    "\n",
    "    norm_names = map(lambda x: unicodedata.normalize(unicode_norm, x), names)\n",
    "    norm_names = tuple(norm_names)\n",
    "    ascii_names = map(lambda x: unidecode(x), norm_names)\n",
    "\n",
    "    names = tuple(chain(norm_names, ascii_names))\n",
    "    names = chain(names, map(str.upper, names))\n",
    "\n",
    "    ruler = EntityRuler(nlp, name=name, overwrite_ents=overwrite_ents)\n",
    "    ruler.add_patterns(\n",
    "        [\n",
    "            {\n",
    "                \"label\": \"PER\",\n",
    "                \"id\": \"PER\",\n",
    "                \"pattern\": [{\"ORTH\": name}],\n",
    "            }\n",
    "            for name in names\n",
    "        ]\n",
    "    )\n",
    "    return ruler\n",
    "\n",
    "\n",
    "@Language.factory(name='join_consecutive_names')\n",
    "def join_consecutive_name_entities(nlp, name):\n",
    "    ruler = EntityRuler(nlp, name=name, overwrite_ents=True)\n",
    "    ruler.add_patterns(\n",
    "        [\n",
    "            # explicit consecutive names\n",
    "            {\n",
    "                \"label\": \"PER\",\n",
    "                \"id\": \"PER\",\n",
    "                \"pattern\": [{\"ENT_TYPE\": \"PER\", \"OP\": \"{2,}\"}],\n",
    "            },\n",
    "            # comma/dot separated\n",
    "            {\n",
    "                \"label\": \"PER\",\n",
    "                \"id\": \"PER\",\n",
    "                \"pattern\": [\n",
    "                    {\"ENT_TYPE\": \"PER\", \"OP\": \"+\"},\n",
    "                    {\"IS_PUNCT\": True},\n",
    "                    {\"ENT_TYPE\": \"PER\", \"OP\": \"+\"},\n",
    "                ],\n",
    "            },\n",
    "            # names with abbrvs within\n",
    "            {\n",
    "                \"label\": \"PER\",\n",
    "                \"id\": \"PER\",\n",
    "                \"pattern\": [\n",
    "                    {\"ENT_TYPE\": \"PER\", \"OP\": \"+\"},\n",
    "                    {\"TEXT\": {\"REGEX\": r\"[A-Z][\\.\\s\\,]?\"}},\n",
    "                    {\"ENT_TYPE\": \"PER\", \"OP\": \"+\"},\n",
    "                ],\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "    return ruler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aymurai.spacy.ner import SpacyNER\n",
    "from aymurai.text.normalize import TextNormalize\n",
    "from aymurai.spacy.ruler import SpacyRulerPipeline\n",
    "from aymurai.text.extraction import FulltextExtract\n",
    "from aymurai.pipeline.pipeline import AymurAIPipeline\n",
    "\n",
    "config = {\n",
    "    \"preprocess\": [\n",
    "        (\n",
    "            FulltextExtract,\n",
    "            {\n",
    "                \"extension\": \"pdf\",\n",
    "                \"method\": \"tesseract\",\n",
    "                \"language\": \"spa\",\n",
    "                \"errors\": \"ignore\",\n",
    "                \"use_cache\": True,\n",
    "            },\n",
    "        ),\n",
    "        (TextNormalize, {}),\n",
    "        (\n",
    "            SpacyRulerPipeline,\n",
    "            {\n",
    "                \"base\": \"es\",\n",
    "                \"steps\": [\n",
    "                    (\"name_lookup_ruler\", {'country_codes': ['AR']}),\n",
    "                    (\"join_consecutive_names\", {}),\n",
    "                ],\n",
    "            },\n",
    "        ),\n",
    "    ],\n",
    "    \"models\": [],\n",
    "    \"postprocess\": [],\n",
    "    \"multiprocessing\": {},\n",
    "    \"use_cache\": False,\n",
    "    # 'log_level': 'debug'\n",
    "}\n",
    "\n",
    "pipeline = AymurAIPipeline(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_train = pipeline.preprocess(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tryout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"es\")\n",
    "nlp.add_pipe(\"name_lookup_ruler\")\n",
    "nlp.add_pipe(\"join_consecutive_names\")\n",
    "# ruler = name_lookup(nlp, 'asd')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruler = name_lookup(nlp, 'names_lookup')\n",
    "join_entities = join_consecutive_name_entities(nlp, 'asd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import srsly\n",
    "from spacy import displacy\n",
    "\n",
    "item = preprocessed_train[53]\n",
    "text = item['data']['doc.text']\n",
    "print(srsly.yaml_dumps(item))\n",
    "doc = nlp(text)\n",
    "doc = ruler(doc)\n",
    "doc = join_entities(doc)\n",
    "\n",
    "displacy.render(doc, 'ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
