{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%load_ext aymurai.devtools.magic\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from aymurai.spacy.display import DocRender\n",
    "from aymurai.pipeline import AymurAIPipeline\n",
    "from aymurai.datasets.ar_juz_pcyf_10 import ArgentinaJuzgadoPCyF10Dataset\n",
    "\n",
    "locale.setlocale(locale.LC_ALL, \"es_AR.UTF-8\")\n",
    "\n",
    "render = DocRender()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_oral(item):\n",
    "    annots = item['annotations']\n",
    "    is_oral = map(lambda x: x['oral_escrita'] == 'oral', annots)\n",
    "    return any(is_oral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "private = ArgentinaJuzgadoPCyF10Dataset('private', use_cache=True)\n",
    "public = ArgentinaJuzgadoPCyF10Dataset('latest', use_cache=True)\n",
    "\n",
    "train, test = train_test_split(private, test_size=0.2, random_state=22)\n",
    "train, val = train_test_split(train, test_size=0.2, random_state=22)\n",
    "\n",
    "# train = list(filter(is_oral, train))\n",
    "# val = list(filter(is_oral, val))\n",
    "# test = list(filter(is_oral, test))\n",
    "\n",
    "print('private', len(private))\n",
    "print('public', len(public))\n",
    "print('---')\n",
    "print('train:', len(train))\n",
    "print('test:', len(test))\n",
    "print('val:', len(val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pipeline definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dummy extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%export aymurai.models.dummy.hora_inicio_cierre\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from more_itertools import zip_offset\n",
    "\n",
    "from aymurai.meta.types import DataItem, DataBlock\n",
    "from aymurai.spacy.components.fuzzy import FuzzyMatcher\n",
    "from aymurai.meta.pipeline_interfaces import TrainModule\n",
    "\n",
    "\n",
    "class DummyExtractorHoraInicioCierre(TrainModule):\n",
    "    def __init__(self, lang: str = \"es\"):\n",
    "        self.nlp = spacy.blank(\"es\")\n",
    "        self.matcher = FuzzyMatcher(self.nlp.vocab)\n",
    "        self.matcher.add(\"START\", patterns=[self.nlp.make_doc(t) for t in [\"inicio\"]])\n",
    "        self.matcher.add(\n",
    "            \"END\",\n",
    "            patterns=[\n",
    "                self.nlp.make_doc(t) for t in [\"cierre\", \"finalizacion\", \"finalizaciÃ³n\"]\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    def save(self, path: str):\n",
    "        return\n",
    "\n",
    "    def load(self, path: str):\n",
    "        return\n",
    "\n",
    "    def fit(self, train: DataBlock, val: DataBlock):\n",
    "        return\n",
    "\n",
    "    def predict(self, data: DataBlock) -> DataBlock:\n",
    "        data = [self.predict_single(item) for item in data]\n",
    "\n",
    "        return data\n",
    "\n",
    "    def predict_single(self, item: DataItem) -> DataItem:\n",
    "        item = deepcopy(item)\n",
    "\n",
    "        # format prediction\n",
    "        if \"predictions\" not in item:\n",
    "            item[\"predictions\"] = {}\n",
    "        if \"records\" not in item[\"predictions\"]:\n",
    "            item[\"predictions\"][\"records\"] = {}\n",
    "        if \"entities\" not in item[\"predictions\"]:\n",
    "            item[\"predictions\"][\"entities\"] = []\n",
    "        if \"doc-cats\" not in item[\"predictions\"]:\n",
    "            item[\"predictions\"][\"doc-cats\"] = {}\n",
    "\n",
    "        item[\"predictions\"][\"records\"][\"hora_de_inicio\"] = []\n",
    "        item[\"predictions\"][\"records\"][\"hora_de_cierre\"] = []\n",
    "        item[\"predictions\"][\"records\"][\"duracion\"] = []\n",
    "        item[\"predictions\"][\"doc-cats\"][\"oral_escrita\"] = \"escrita\"\n",
    "\n",
    "        ents = []\n",
    "        if \"entities\" in item[\"data\"]:\n",
    "            ents += item[\"data\"][\"entities\"]\n",
    "\n",
    "        # if there is no entities just pass\n",
    "        if not ents:\n",
    "            return item\n",
    "\n",
    "        times = filter(lambda x: x[\"label\"] == \"TIME\", ents)\n",
    "        times = sorted(times, key=lambda e: e[\"start\"])\n",
    "\n",
    "        if not times:\n",
    "            return item\n",
    "\n",
    "        starts = ends = 0\n",
    "        for span in times:\n",
    "            pre = span[\"context_pre\"]\n",
    "            dt = span[\"attrs\"][\"aymurai_date\"]\n",
    "\n",
    "            matches = self.matcher(self.nlp.make_doc(pre))\n",
    "            matches = sorted(matches, key=lambda x: x[3])\n",
    "            if not matches:\n",
    "                continue\n",
    "\n",
    "            candidate = matches[0]\n",
    "            match candidate[0]:\n",
    "                case \"START\":\n",
    "                    span[\"label\"] = \"HORA_INICIO\"\n",
    "                    item[\"predictions\"][\"entities\"].append(span)\n",
    "                    item[\"predictions\"][\"records\"][\"hora_de_inicio\"].append(dt)\n",
    "                    item[\"predictions\"][\"doc-cats\"][\"oral_escrita\"] = \"oral\"\n",
    "                    starts += 1\n",
    "                case \"END\":\n",
    "                    span[\"label\"] = \"HORA_CIERRE\"\n",
    "                    item[\"predictions\"][\"entities\"].append(span)\n",
    "                    item[\"predictions\"][\"records\"][\"hora_de_cierre\"].append(dt)\n",
    "                    item[\"predictions\"][\"doc-cats\"][\"oral_escrita\"] = \"oral\"\n",
    "                    ends += 1\n",
    "\n",
    "        pairs = max(starts, ends)\n",
    "        item[\"predictions\"][\"records\"][\"hora_de_inicio\"] = [\n",
    "            time\n",
    "            for i, time in zip_offset(\n",
    "                range(pairs),\n",
    "                item[\"predictions\"][\"records\"][\"hora_de_inicio\"],\n",
    "                offsets=(0, 0),\n",
    "                longest=True,\n",
    "                fillvalue=pd.NaT,\n",
    "            )\n",
    "        ]\n",
    "        item[\"predictions\"][\"records\"][\"hora_de_cierre\"] = [\n",
    "            time\n",
    "            for i, time in zip_offset(\n",
    "                range(pairs),\n",
    "                item[\"predictions\"][\"records\"][\"hora_de_cierre\"],\n",
    "                offsets=(0, 0),\n",
    "                longest=True,\n",
    "                fillvalue=pd.NaT,\n",
    "            )\n",
    "        ]\n",
    "        for start, end in zip(\n",
    "            item[\"predictions\"][\"records\"][\"hora_de_inicio\"],\n",
    "            item[\"predictions\"][\"records\"][\"hora_de_cierre\"],\n",
    "        ):\n",
    "            item[\"predictions\"][\"records\"][\"duracion\"] += [end - start]\n",
    "\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aymurai.spacy.components.loader\n",
    "from aymurai.text.normalize import TextNormalize\n",
    "from aymurai.spacy.ruler import SpacyRulerPipeline\n",
    "from aymurai.text.extraction import FulltextExtract\n",
    "from aymurai.models.dummy.hora_inicio_cierre import DummyExtractorHoraInicioCierre\n",
    "\n",
    "config = {\n",
    "    \"preprocess\": [\n",
    "        (\n",
    "            FulltextExtract,\n",
    "            {\n",
    "                \"extension\": \"pdf\",\n",
    "                \"method\": \"tesseract\",\n",
    "                \"language\": \"spa\",\n",
    "                \"errors\": \"ignore\",\n",
    "                \"use_cache\": True,\n",
    "            },\n",
    "        ),\n",
    "        (TextNormalize, {}),\n",
    "        (\n",
    "            SpacyRulerPipeline,\n",
    "            {\n",
    "                \"base\": \"es\",\n",
    "                \"steps\": [\n",
    "                    (\n",
    "                        \"enhanced_regex_ruler\",\n",
    "                        {\n",
    "                            \"patterns\": {\n",
    "                                \"TIME\": [\n",
    "                                    \"%H(\\.|:)%M\",\n",
    "                                    \"%-H(.|:)%M (?i)horas\",\n",
    "                                    \"%-H.%M h(rs|r|s)\\.?\",\n",
    "                                ],\n",
    "                            },\n",
    "                        },\n",
    "                    ),\n",
    "                ],\n",
    "            },\n",
    "        ),\n",
    "    ],\n",
    "    \"models\": [\n",
    "        (DummyExtractorHoraInicioCierre, {})\n",
    "    ],\n",
    "    \"postprocess\": [],\n",
    "    \"multiprocessing\": {},\n",
    "    \"use_cache\": False,\n",
    "    # 'log_level': 'debug'\n",
    "}\n",
    "\n",
    "pipeline = AymurAIPipeline(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pipeline.preprocess(train)\n",
    "preds = pipeline.predict(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import srsly\n",
    "\n",
    "registry = preds[21]\n",
    "metadata = {k: v for k, v in registry.items() if type(v) not in [dict, list]}\n",
    "# print(\n",
    "#     srsly.yaml_dumps(\n",
    "#         metadata, indent_mapping=2, indent_sequence=4, indent_offset=2, sort_keys=False\n",
    "#     )\n",
    "# )\n",
    "print(\n",
    "    # srsly.yaml_dumps(\n",
    "        registry[\"predictions\"][\"records\"],\n",
    "    #     indent_mapping=2,\n",
    "    #     indent_sequence=4,\n",
    "    #     indent_offset=2,\n",
    "    #     sort_keys=False,\n",
    "    # )\n",
    ")\n",
    "print()\n",
    "\n",
    "print(\"\\n-------\\n\")\n",
    "render(registry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed = pipeline.preprocess(test)\n",
    "preds = pipeline.predict(preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from more_itertools import collapse\n",
    "\n",
    "from aymurai.meta.types import DataItem\n",
    "\n",
    "\n",
    "def format_duration(value):\n",
    "    if not isinstance(value, str):\n",
    "        return value\n",
    "    # add seconds count\n",
    "    return value + ':00'\n",
    "\n",
    "def annot_dataframe(item: DataItem) -> pd.DataFrame:\n",
    "    path = item[\"path\"]\n",
    "    annots = item[\"annotations\"]\n",
    "    df = pd.DataFrame(annots)\n",
    "    df.insert(0, \"path\", path)\n",
    "\n",
    "    # df = df[['path', 'date']]\n",
    "    df = df[[\"path\", \"oral_escrita\", \"hora_de_inicio\", \"hora_de_cierre\", \"duracion\"]]\n",
    "    return df\n",
    "\n",
    "references = pd.concat(map(annot_dataframe, preds), ignore_index=True)\n",
    "references['hora_de_inicio'] = pd.to_datetime(references['hora_de_inicio']).apply(lambda x: x.replace(year=1900, month=1, day=1))\n",
    "references['hora_de_cierre'] = pd.to_datetime(references['hora_de_cierre']).apply(lambda x: x.replace(year=1900, month=1, day=1))\n",
    "references['duracion'] = references['duracion'].apply(format_duration)\n",
    "references['duracion'] = pd.to_timedelta(references['duracion'])\n",
    "references['oral_escrita'] = references['oral_escrita'].astype('category')\n",
    "\n",
    "# references.drop_duplicates(subset=['path'], keep='first', inplace=True)\n",
    "references.set_index('path', inplace=True)\n",
    "references"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preds_dataframe(item: DataItem) -> pd.DataFrame:\n",
    "    path = item[\"path\"]\n",
    "    preds_ = item[\"predictions\"][\"records\"]\n",
    "    df = pd.DataFrame(preds_)\n",
    "    df.insert(0, \"path\", path)\n",
    "    if not len(df):\n",
    "        df = pd.DataFrame({'path': path}, index=pd.Index([0]))\n",
    "    \n",
    "    oral_escrita = item['predictions']['doc-cats']['oral_escrita']\n",
    "    df['oral_escrita'] = oral_escrita\n",
    "    return df\n",
    "\n",
    "\n",
    "hypotheses = pd.concat(map(preds_dataframe, preds), ignore_index=True)\n",
    "hypotheses['oral_escrita'] = hypotheses['oral_escrita'].astype('category')\n",
    "\n",
    "hypotheses.drop_duplicates(subset=['path'], keep='first', inplace=True)\n",
    "hypotheses.set_index('path', inplace=True)\n",
    "hypotheses\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "refs = references.reset_index().drop_duplicates(subset=['path'], keep='first')[['path', 'oral_escrita']]\n",
    "hyps = hypotheses.reset_index().drop_duplicates(subset=['path'], keep='first')[['path', 'oral_escrita']]\n",
    "hyps.rename(columns={'oral_escrita': 'hypothesis'}, inplace=True)\n",
    "refs.rename(columns={'oral_escrita': 'reference'}, inplace=True)\n",
    "\n",
    "df = pd.merge(refs, hyps, on='path')\n",
    "df['acc'] = df['reference'] == df['hypothesis']\n",
    "\n",
    "report = classification_report(df['reference'], df['hypothesis'])\n",
    "print(report)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(row):\n",
    "    ref = set(row['reference'])\n",
    "    hyp = set(row['hypothesis'])\n",
    "    if not (ref) and not (hyp):\n",
    "        return 1\n",
    "    elif not (ref) or not (hyp):\n",
    "        return 0\n",
    "    \n",
    "    return len(ref & hyp) / len(ref | hyp)\n",
    "\n",
    "def first_match(row):\n",
    "    ref = row['reference'][0]\n",
    "    hyp = row['hypothesis'][0]\n",
    "    return ref == hyp\n",
    "\n",
    "\n",
    "def metrics(ref, hyp, column):\n",
    "    ref = ref[[\"path\", column]].copy()\n",
    "    ref = ref.groupby('path').agg({column: list}).reset_index()\n",
    "    ref.rename(columns={column: \"reference\"}, inplace=True)\n",
    "\n",
    "    hyp = hyp[[\"path\", column]].copy()\n",
    "    hyp = hyp.groupby('path').agg({column: list}).reset_index()\n",
    "    hyp.rename(columns={column: \"hypothesis\"}, inplace=True)\n",
    "\n",
    "    df = pd.merge(ref[[\"path\", \"reference\"]], hyp[[\"path\", \"hypothesis\"]])\n",
    "    df['jaccard'] = df.apply(lambda row: jaccard(row), axis=1)\n",
    "    df['1st_match'] = df.apply(lambda row: first_match(row), axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refs = references.reset_index()\n",
    "refs = refs.query('oral_escrita == \"oral\"')\n",
    "hyps = hypotheses.reset_index()\n",
    "df = metrics(refs, hyps, 'hora_de_inicio')\n",
    "print('jaccard:', df['jaccard'].mean())\n",
    "print('1st_match acc:', df['1st_match'].mean())\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[0, 'path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = filter(lambda x: x['path'] == '/resources/restricted/ar-juz-pcyf-10/RESOLUCIONES DEL JUZGADO-pdf/2018/2) FEBRERO/1045_9_20170_8_02_18_1.1.1_L_451_alimentos_en_infraccion.pdf', test)\n",
    "example = list(example)\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refs = references.reset_index()\n",
    "refs = refs.query('oral_escrita == \"oral\"')\n",
    "hyps = hypotheses.reset_index()\n",
    "df = metrics(refs, hyps, 'hora_de_cierre')\n",
    "print('jaccard:', df['jaccard'].mean())\n",
    "print('1st_match acc:', df['1st_match'].mean())\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refs = references.reset_index()\n",
    "refs = refs.query('oral_escrita == \"oral\"')\n",
    "hyps = hypotheses.reset_index()\n",
    "df = metrics(refs, hyps, 'duracion')\n",
    "print('jaccard:', df['jaccard'].mean())\n",
    "print('1st_match acc:', df['1st_match'].mean())\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refs = references.reset_index()\n",
    "refs = refs.query('oral_escrita == \"oral\"')\n",
    "hyps = hypotheses.reset_index()\n",
    "df = metrics(refs, hyps, 'hora_de_cierre')\n",
    "print('jaccard:', df['jaccard'].mean())\n",
    "print('1st_match acc:', df['1st_match'].mean())\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# path = df.loc[23, 'path']\n",
    "# print(path)\n",
    "path = errors.index[10]\n",
    "\n",
    "filtered = list(filter(lambda x: x['path'] == path, preds))\n",
    "registry = filtered[0]\n",
    "metadata = {k: v for k, v in registry['metadata'].items() if type(v) not in [dict, list]}\n",
    "print(registry['path'])\n",
    "print(json.dumps(metadata, indent=4))\n",
    "\n",
    "print('annotations')\n",
    "print('hora inicio:', [x['hora_de_inicio'] for x in registry['annotations']])\n",
    "print('hora cierre:', [x['hora_de_cierre'] for x in registry['annotations']])\n",
    "print('predictions')\n",
    "# print('hora inicio:', registry['predictions']['records']['hora_de_inicio'][0].strftime('%H:%M'))\n",
    "# print('hora cierre:', registry['predictions']['records']['hora_de_cierre'][0].strftime('%H:%M'))\n",
    "\n",
    "\n",
    "print(\"\\n-------\\n\")\n",
    "render(registry)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
