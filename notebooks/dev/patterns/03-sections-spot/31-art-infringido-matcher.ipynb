{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%load_ext aymurai.devtools.magic\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from aymurai.spacy.display import DocRender\n",
    "from aymurai.pipeline import AymurAIPipeline\n",
    "from aymurai.datasets.ar_juz_pcyf_10 import ArgentinaJuzgadoPCyF10Dataset\n",
    "\n",
    "colors = {\n",
    "    'SECTION:DECISION': 'red',\n",
    "    'SECTION:HEAD': 'red',\n",
    "    'KEYWORDS': 'blue'\n",
    "\n",
    "}\n",
    "render = DocRender(config={'colors': colors})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "private = ArgentinaJuzgadoPCyF10Dataset('private', use_cache=True)\n",
    "docs = ArgentinaJuzgadoPCyF10Dataset('private-docs', use_cache=True)\n",
    "docs = filter(lambda x: 'admisibilidad_prueba' not in x['metadata']['objeto_de_la_resolucion'], docs)\n",
    "docs = list(docs)\n",
    "\n",
    "train, test = train_test_split(private, test_size=0.2, random_state=22)\n",
    "train, val = train_test_split(train, test_size=0.2, random_state=22)\n",
    "print('train:', len(train))\n",
    "print('test:', len(test))\n",
    "print('val:', len(val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pipeline definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %%export aymurai.spacy.components.es_ar.art_infringido\n",
    "# import re\n",
    "# import hashlib\n",
    "# from copy import deepcopy\n",
    "# \n",
    "# import spacy\n",
    "# from spacy.tokens import Span\n",
    "# from more_itertools import unique_everseen\n",
    "# \n",
    "# from aymurai.spacy.utils import format_entity\n",
    "# from aymurai.meta.pipeline_interfaces import Transform\n",
    "# from aymurai.spacy.components.regex import EnhancedRegexRuler, EnhancedRegexMatcher\n",
    "# from aymurai.spacy.components.es_ar.articles.patterns import (\n",
    "#     ABBRS,\n",
    "#     CODES,\n",
    "#     ART_PATTERN_MULTI_MOD,\n",
    "#     ART_PATTERN_MULTI_PREFIX,\n",
    "# )\n",
    "# \n",
    "# \n",
    "# class SpacyRulerArtInfringido(Transform):\n",
    "#     def __init__(self):\n",
    "#         global __nlp\n",
    "#         __nlp = spacy.blank(\"es\")\n",
    "#         self.matcher_layer_0 = EnhancedRegexMatcher(__nlp.vocab)\n",
    "#         self.matcher_layer_1 = EnhancedRegexMatcher(__nlp.vocab)\n",
    "#         self.matcher_layer_0.add(\n",
    "#             \"LAYER_0\", patterns=[f\"{self.LAYER0_PREFIX}.*?{self.LAYER0_SUFFIX}\"]\n",
    "#         )\n",
    "#         self.matcher_layer_1.add(\n",
    "#             \"ART_INFRINGIDO\",\n",
    "#             patterns=[\n",
    "#                 r\"[\\d\\.]{2,}\",\n",
    "#                 ART_PATTERN_MULTI_PREFIX,\n",
    "#                 ART_PATTERN_MULTI_MOD,\n",
    "#             ],\n",
    "#         )\n",
    "#         # self.matcher_layer_1.add(\n",
    "#         #     \"CONDUCTA1\",\n",
    "#         #     patterns=[\n",
    "#         #         f\"{self.PAT_COND1_PREFIX}.*?{self.PAT_COND1_SUFFIX}\",\n",
    "#         #     ],\n",
    "#         # )\n",
    "#         self.matcher_layer_1.add(\"CODIGO_O_LEY\", patterns=CODES)\n",
    "#         self.matcher_layer_1.add(\n",
    "#             \"CODIGO_O_LEY\", patterns=[f\"{abbr}(\\s?CABA)?\" for abbr in ABBRS]\n",
    "#         )\n",
    "#         self.matcher_layer_1.add(\n",
    "#             \"CODIGO_O_LEY\", patterns=[\"(?i)ley(es)?(( y|,)? ([\\d\\.]+))+\"]\n",
    "#         )\n",
    "# \n",
    "#     def clean_span(self, span):\n",
    "#         match span.label_:\n",
    "#             case \"CONDUCTA\":\n",
    "#                 pre = __nlp.make_doc(re.sub(f\"^{self.PAT_COND1_PREFIX}\", \"\", span.text))\n",
    "#                 post = __nlp.make_doc(\n",
    "#                     re.sub(f\"{self.PAT_COND1_SUFFIX}$\", \"\", span.text)\n",
    "#                 )\n",
    "#                 start = len(span) - len(pre)\n",
    "#                 end = len(span) - len(post)\n",
    "#                 span = Span(\n",
    "#                     span.doc,\n",
    "#                     start=span.start + start,\n",
    "#                     end=span.end - end,\n",
    "#                     label=\"CONDUCTA\",\n",
    "#                 )\n",
    "# \n",
    "#         return span\n",
    "# \n",
    "#     def __call__(self, item):\n",
    "#         item = deepcopy(item)\n",
    "#         if not \"entities\" in item[\"data\"]:\n",
    "#             item[\"data\"][\"entities\"] = []\n",
    "# \n",
    "#         fragment = item[\"data\"][\"doc.text\"][:700]\n",
    "#         doc = __nlp(fragment)\n",
    "# \n",
    "#         matches = []\n",
    "#         matches_layer_0 = self.matcher_layer_0(doc)\n",
    "#         for label0, start0, end0, score0 in matches_layer_0:\n",
    "#             span = doc[start0:end0]\n",
    "#             doc0 = span.as_doc()\n",
    "#             \n",
    "#             matches_layer_1 = self.matcher_layer_1(doc0)\n",
    "#             for label1, start1, end1, score1 in matches_layer_1:\n",
    "#                 start1 += start0\n",
    "#                 end1 += start0\n",
    "#                 matches += [(label1, start1, end1, score1)]\n",
    "#                 if label1 == \"ART_INFRINGIDO\":\n",
    "#                     matches += [\n",
    "#                         (\"CONDUCTA\", end1 + 1, start0 + len(doc0) + 1, (0, 0, 0))\n",
    "#                     ]\n",
    "# \n",
    "#         matches = sorted(matches, key=lambda x: (sum(x[3]), x[1]))\n",
    "#         matches = unique_everseen(matches, key=lambda x: x[0])\n",
    "#         matches = list(matches)\n",
    "# \n",
    "#         for label, start, end, score in matches:\n",
    "#             span = Span(doc, start=start, end=end, label=label)\n",
    "#             span = self.clean_span(span)\n",
    "#             item[\"data\"][\"entities\"] += [format_entity(span)]\n",
    "# \n",
    "#         # item[\"data\"][\"doc.text\"] = fulltext\n",
    "# \n",
    "#         # item[\"data\"][\"entities\"] = [\n",
    "#         #     self.clean_art(ent) for ent in item[\"data\"][\"entities\"]\n",
    "#         # ]\n",
    "# \n",
    "#         return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import hashlib\n",
    "from copy import deepcopy\n",
    "from itertools import groupby, filterfalse, zip_longest\n",
    "\n",
    "import regex\n",
    "import spacy\n",
    "import srsly\n",
    "import pandas as pd\n",
    "from spacy.tokens import Span\n",
    "from more_itertools import flatten, collapse, unique_everseen\n",
    "\n",
    "from aymurai.meta.types import DataItem\n",
    "from aymurai.spacy.utils import format_entity\n",
    "from aymurai.spacy.ruler import SpacyRulerPipeline\n",
    "from aymurai.meta.pipeline_interfaces import Transform\n",
    "from aymurai.spacy.components.regex import EnhancedRegexRuler, EnhancedRegexMatcher\n",
    "from aymurai.spacy.components.es_ar.articles.patterns import (\n",
    "    ABBRS,\n",
    "    CODES,\n",
    "    ART_CODE,\n",
    "    ART_PREFIX,\n",
    "    ART_PREFIX_TITLE,\n",
    "    ART_PATTERN_MULTI_MOD,\n",
    "    ART_PATTERN_MULTI_PREFIX,\n",
    "    ART_PATTERN_MULTI_NO_PREFIX,\n",
    "    ART_PATTERN_MULTI_PREFIX_TITLE,\n",
    ")\n",
    "\n",
    "VALIDATION_FIELDS = pd.read_csv(\"validacion_codigos.csv\")\n",
    "VALIDATION_FIELDS.rename(\n",
    "    columns={col: col.lower() for col in VALIDATION_FIELDS.columns},\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "\n",
    "class SpacyRulerArtInfringidoValFields(Transform):\n",
    "    def __init__(self):\n",
    "        FIELDS = [\"conducta\", \"conducta_descripcion\"]\n",
    "        self.VALIDATION_FIELDS = {\n",
    "            k: v for k, v in VALIDATION_FIELDS.items() if k in FIELDS\n",
    "        }\n",
    "\n",
    "        patterns = {\n",
    "            \"ARTICLE\": [\n",
    "                ART_PATTERN_MULTI_PREFIX_TITLE,\n",
    "                ART_PATTERN_MULTI_NO_PREFIX,\n",
    "                ART_PATTERN_MULTI_PREFIX,\n",
    "                ART_PATTERN_MULTI_MOD,\n",
    "            ],\n",
    "        }\n",
    "\n",
    "        for field, validations in self.VALIDATION_FIELDS.items():\n",
    "            validations = filter(lambda x: isinstance(x, str), validations)\n",
    "            validations = map(lambda x: x.replace(\"_\", \"[_\\s]+\"), validations)\n",
    "            # validations = map(lambda x: x.replace('a', '[áa]'), validations)\n",
    "            # validations = map(lambda x: x.replace('e', '[ée]'), validations)\n",
    "            # validations = map(lambda x: x.replace('i', '[íi]'), validations)\n",
    "            # validations = map(lambda x: x.replace('o', '[óo]'), validations)\n",
    "            # validations = map(lambda x: x.replace('u', '[úu]'), validations)\n",
    "            validations = map(lambda x: f\"(?i){x}{{e<={int(0.2*len(x))}}}\", validations)\n",
    "            validations = list(validations)\n",
    "            # validations += [v.replace(\"_\", \"[_\\s]+\") for v in validations]\n",
    "            patterns[field.upper()] = validations\n",
    "\n",
    "        self.ruler = SpacyRulerPipeline(\n",
    "            base=\"es\",\n",
    "            steps=[\n",
    "                (\n",
    "                    \"enhanced_regex_ruler\",\n",
    "                    {\n",
    "                        \"patterns\": patterns,\n",
    "                    },\n",
    "                ),\n",
    "            ],\n",
    "        )\n",
    "\n",
    " \n",
    "\n",
    "    def postprocess(self, entity):\n",
    "        entity = deepcopy(entity)\n",
    "        text = entity[\"text\"]\n",
    "        span = self.ruler.nlp.make_doc(text)\n",
    "\n",
    "        # i = (list(g) for _, g in groupby(text, key='y'.__ne__))\n",
    "        # print([a + b for a, b in zip_longest(i, i, fillvalue=[])])\n",
    "\n",
    "        prefix = regex.compile(f\"(?i)^({ART_PREFIX_TITLE})|({ART_PREFIX})\\.?\\s*\")\n",
    "        art_pat = regex.compile(f\"{ART_CODE}|\\d+\")\n",
    "        match = prefix.match(text)\n",
    "        if match:\n",
    "            span = match[0]\n",
    "            pre = [t for t in self.ruler.nlp.make_doc(span)]\n",
    "            entity[\"start\"] += len(pre)\n",
    "            entity[\"text\"] = prefix.sub(\"\", text).strip()\n",
    "            entity[\"start_char\"] += len(span)\n",
    "\n",
    "            # split string en 'y'\n",
    "            # text = entity['text']\n",
    "            # i = (list(g) for _, g in groupby(text, key='y'.__ne__))\n",
    "            # print([''.join(a + b) for a, b in zip_longest(i, i, fillvalue=[])])\n",
    "\n",
    "        # print(srsly.yaml_dumps(entity))\n",
    "        return [entity]\n",
    "\n",
    "    def __call__(self, item: DataItem) -> DataItem:\n",
    "        item = deepcopy(item)\n",
    "        item = self.ruler(item)\n",
    "\n",
    "        ents = []\n",
    "        if \"entities\" in item[\"data\"]:\n",
    "            ents += item[\"data\"][\"entities\"]\n",
    "\n",
    "        # take articles from ents to work with\n",
    "        arts = filter(lambda x: x[\"label\"] == \"ARTICLE\", ents)\n",
    "        ents = filter(lambda x: x[\"label\"] != \"ARTICLE\", ents)\n",
    "        ents = list(ents)\n",
    "\n",
    "        # post process entities\n",
    "        arts = map(self.postprocess, arts)\n",
    "        arts = collapse(arts, base_type=dict)\n",
    "        arts = list(arts)\n",
    "\n",
    "        # restore entities\n",
    "        ents += arts\n",
    "        ents = sorted(ents, key=lambda x: x[\"start\"])\n",
    "        item[\"data\"][\"entities\"] = list(ents)\n",
    "        # print(srsly.yaml_dumps(arts))\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asd = SpacyRulerArtInfringidoValFields()\n",
    "# a = asd(registry)\n",
    "# render(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%export aymurai.models.dummy.art_infringido\n",
    "\n",
    "from copy import deepcopy\n",
    "from functools import reduce\n",
    "\n",
    "import regex\n",
    "import srsly\n",
    "from more_itertools import zip_offset\n",
    "\n",
    "from aymurai.utils.misc import get_element\n",
    "from aymurai.meta.types import DataItem, DataBlock\n",
    "from aymurai.meta.pipeline_interfaces import TrainModule\n",
    "\n",
    "VALIDATION_FIELDS = ArgentinaJuzgadoPCyF10Dataset(\"validation-fields\", use_cache=False).data\n",
    "VALIDATION_FIELDS = pd.DataFrame(VALIDATION_FIELDS['article_group'])\n",
    "\n",
    "\n",
    "class DummyExtractorArtInfringido(TrainModule):\n",
    "    def __init__(self):\n",
    "        self.articles = VALIDATION_FIELDS['art_infringido']\n",
    "        \n",
    "    def save(self, path: str):\n",
    "        return\n",
    "\n",
    "    def load(self, path: str):\n",
    "        return\n",
    "\n",
    "    def fit(self, train: DataBlock, val: DataBlock):\n",
    "        return\n",
    "\n",
    "    def predict(self, data: DataBlock) -> DataBlock:\n",
    "        data = [self.predict_single(item) for item in data]\n",
    "\n",
    "        return data\n",
    "    \n",
    "    def set_attrs(self, entity):\n",
    "        entity = deepcopy(entity)\n",
    "\n",
    "        # if \n",
    "        return entity\n",
    "\n",
    "    def predict_single(self, item: DataItem) -> DataItem:\n",
    "        item = deepcopy(item)\n",
    "\n",
    "        # format prediction\n",
    "        if \"predictions\" not in item:\n",
    "            item[\"predictions\"] = {}\n",
    "        if \"records\" not in item[\"predictions\"]:\n",
    "            item[\"predictions\"][\"records\"] = {}\n",
    "        if \"entities\" not in item[\"predictions\"]:\n",
    "            item[\"predictions\"][\"entities\"] = []\n",
    "        if \"doc-cats\" not in item[\"predictions\"]:\n",
    "            item[\"predictions\"][\"doc-cats\"] = {}\n",
    "        item[\"predictions\"][\"records\"][\"art_infringido\"] = []\n",
    "        item[\"predictions\"][\"records\"][\"conducta\"] = []\n",
    "        item[\"predictions\"][\"records\"][\"codigo_o_ley\"] = []\n",
    "\n",
    "        ents = []\n",
    "        if \"entities\" in item[\"data\"]:\n",
    "            ents += item[\"data\"][\"entities\"]\n",
    "\n",
    "        # ents = sorted(ents, key=lambda e: e[\"start\"])\n",
    "        # arts = filter(lambda x: x[\"label\"] in [\"ART_INFRINGIDO\"], ents)\n",
    "        # cond = filter(lambda x: x[\"label\"] in [\"CONDUCTA\"], ents)\n",
    "        # codi = filter(lambda x: x[\"label\"] in [\"CODIGO_O_LEY\"], ents)\n",
    "\n",
    "        # if there is no entities just pass\n",
    "        if not ents:\n",
    "            return item\n",
    "\n",
    "        # take articles from ents to work with\n",
    "        # arts = filter(lambda x: x[\"label\"] == \"ARTICLE\", ents)\n",
    "        # ents = filter(lambda x: x[\"label\"] != \"ARTICLE\", ents)\n",
    "        # ents = list(ents)\n",
    "\n",
    "        # arts = map(self.set_attrs, arts)\n",
    "        # arts = list(arts)\n",
    "\n",
    "\n",
    "        # ents += arts\n",
    "        # ents = sorted(ents, key=lambda x: x[\"start\"])\n",
    "        # # print(srsly.yaml_dumps(arts))\n",
    "\n",
    "\n",
    "        # item[\"predictions\"][\"entities\"] = ents\n",
    "        # item[\"predictions\"][\"records\"][\"art_infringido\"] += list(arts)\n",
    "        # item[\"predictions\"][\"records\"][\"conducta\"] += list(cond)\n",
    "        # item[\"predictions\"][\"records\"][\"codigo_o_ley\"] += list(codi)\n",
    "\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aymurai.spacy.components.loader\n",
    "from aymurai.spacy.ruler import SpacyRulerPipeline\n",
    "from aymurai.text.extraction import FulltextExtract\n",
    "from aymurai.text.normalize import JunkCleaner, TextNormalize\n",
    "from aymurai.spacy.rulers.section_parser import AymuraiRulerSectionParser\n",
    "\n",
    "config = {\n",
    "    \"preprocess\": [\n",
    "        (\n",
    "            FulltextExtract,\n",
    "            {\n",
    "                # \"extension\": \"pdf\",\n",
    "                # \"method\": \"tesseract\",\n",
    "                # \"language\": \"spa\",\n",
    "                \"errors\": \"ignore\",\n",
    "                \"use_cache\": True,\n",
    "            },\n",
    "        ),\n",
    "        (TextNormalize, {}),\n",
    "        (\n",
    "            JunkCleaner,\n",
    "            {\n",
    "                \"patterns\": [\n",
    "                    \"Juzgado PCyF N* 10 - Tacuarí 138, 7* Piso - juzcyf10ejusbaires.gob.ar - 4014-6821/20 - Gipcyf10\",\n",
    "                ]\n",
    "            },\n",
    "        ),\n",
    "        (\n",
    "            SpacyRulerArtInfringidoValFields,\n",
    "            {},\n",
    "        ),\n",
    "    ],\n",
    "    \"models\": [\n",
    "        # (DummyExtractorArtInfringido, {})\n",
    "    ],\n",
    "    \"postprocess\": [],\n",
    "    \"multiprocessing\": {},\n",
    "    \"use_cache\": False,\n",
    "    # 'log_level': 'debug'\n",
    "}\n",
    "\n",
    "pipeline = AymurAIPipeline(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = pipeline.preprocess(train[:40])\n",
    "result = pipeline.predict(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "index = 7\n",
    "index = random.choice(range(len(result)))\n",
    "print(index)\n",
    "registry = result[index]\n",
    "\n",
    "\n",
    "metadata = {\n",
    "    k: v for k, v in registry[\"metadata\"].items() if type(v) not in [dict, list]\n",
    "}\n",
    "print(json.dumps(metadata, indent=4))\n",
    "\n",
    "\n",
    "print(\"annotations\")\n",
    "print(\"art infingido:\", [x[\"art_infringido\"] for x in registry[\"annotations\"]])\n",
    "print(\"codigo:\", [x[\"codigo_o_ley\"] for x in registry[\"annotations\"]])\n",
    "print(\"conducta:\", [x[\"conducta\"] for x in registry[\"annotations\"]])\n",
    "print(\"conducta detalle:\", [x[\"conducta_descripcion\"] for x in registry[\"annotations\"]])\n",
    "# print('prediction')\n",
    "# print(registry['predictions'])\n",
    "# print('art_infringido:', registry['predictions']['records']['art_infringido'])\n",
    "# print('conducta:', registry['predictions']['records']['conducta'])\n",
    "\n",
    "\n",
    "print(\"\\n-------\\n\")\n",
    "# render(registry, 'span', spans_key='section', paragraphs=15)\n",
    "render(registry, \"ent\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = pipeline.preprocess(train)\n",
    "result = pipeline.predict(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import zip_longest\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from jiwer import cer\n",
    "import matplotlib.pyplot as plt\n",
    "from more_itertools import collapse\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from aymurai.meta.types import DataItem\n",
    "\n",
    "EMPTY_ENTITY = {\n",
    "    key: None for key in [\"text\", \"start\", \"end\", \"label\", \"start_char\", \"end_char\"]\n",
    "}\n",
    "\n",
    "\n",
    "def annot_dataframe(item: DataItem) -> pd.DataFrame:\n",
    "    path = item[\"path\"]\n",
    "    annots = item[\"annotations\"]\n",
    "    df = pd.DataFrame(annots)\n",
    "    df.insert(0, \"path\", path)\n",
    "\n",
    "    df = df[[\"path\", \"art_infringido\", \"conducta\", \"codigo_o_ley\"]]\n",
    "    return df\n",
    "\n",
    "def get_text(value):\n",
    "    if not isinstance(value, dict):\n",
    "        return value\n",
    "    return value.get('text', '')\n",
    "\n",
    "def preds_dataframe(item: DataItem) -> pd.DataFrame:\n",
    "    path = item[\"path\"]\n",
    "    records = item[\"predictions\"][\"records\"]\n",
    "    max_ = max(map(lambda x: len(x), records.values()))\n",
    "    if max_ > 1:\n",
    "        print(records)\n",
    "    if not max_:\n",
    "        return pd.DataFrame({\"path\": path}, index=pd.Index([0]))\n",
    "\n",
    "    for key, record in records.items():\n",
    "        records[key] = [val for val, _ in zip_longest(record, range(max_))]\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "\n",
    "    df[\"art_infringido\"] = df[\"art_infringido\"].apply(get_text)\n",
    "    df[\"conducta\"] = df[\"conducta\"].apply(get_text)\n",
    "    df[\"codigo_o_ley\"] = df[\"codigo_o_ley\"].apply(get_text)\n",
    "\n",
    "    df.insert(0, \"path\", path)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "references = pd.concat(map(annot_dataframe, result), ignore_index=True)\n",
    "\n",
    "hypotheses = pd.concat(map(preds_dataframe, result), ignore_index=True)\n",
    "hypotheses.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(references, hypotheses, on='path', suffixes=('_ref', '_hyp'))\n",
    "data.dropna(subset=[col for col in data.columns if col.endswith('_ref')], inplace=True)\n",
    "art_scores = data.apply(lambda row: cer(row['art_infringido_ref'], row['art_infringido_hyp']), axis=1)\n",
    "cond_scores = data.apply(lambda row: cer(row['conducta_ref'], row['conducta_hyp']), axis=1)\n",
    "codi_scores = data.apply(lambda row: cer(row['codigo_o_ley_ref'], row['codigo_o_ley_hyp']), axis=1)\n",
    "\n",
    "print('art_infringido cer:', cer(data['art_infringido_ref'].tolist(), data['art_infringido_hyp'].tolist()))\n",
    "print('conducta cer:', cer(data['conducta_ref'].tolist(), data['conducta_hyp'].tolist()))\n",
    "print('codigo_o_ley cer:', cer(data['codigo_o_ley_ref'].tolist(), data['codigo_o_ley_hyp'].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = filter(lambda x: x['path'] == '/resources/restricted/ar-juz-pcyf-10/RESOLUCIONES DEL JUZGADO-pdf/2021/TOMO 38_JUNIO _21/3587_38 CAUSA 77325_21.pdf', test)\n",
    "example = list(example)\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = pipeline.preprocess(test)\n",
    "result = pipeline.predict(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from jiwer import cer\n",
    "import matplotlib.pyplot as plt\n",
    "from more_itertools import collapse\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from aymurai.meta.types import DataItem\n",
    "\n",
    "\n",
    "def annot_dataframe(item: DataItem) -> pd.DataFrame:\n",
    "    path = item['path']\n",
    "    annots = item['annotations']\n",
    "    df = pd.DataFrame(annots)\n",
    "    df.insert(0, 'path', path)\n",
    "\n",
    "    df = df[['path', 'art_infringido', 'conducta', 'codigo_o_ley']]\n",
    "    return df\n",
    "\n",
    "def preds_dataframe(item: DataItem) -> pd.DataFrame:\n",
    "    path = item['path']\n",
    "    preds = item['predictions']['records']\n",
    "    df = pd.DataFrame(preds)\n",
    "    df['art_infringido'] = df['art_infringido'].apply(lambda x: x['text'])\n",
    "    df['conducta'] = df['conducta'].apply(lambda x: x['text'])\n",
    "    df['codigo_o_ley'] = df['codigo_o_ley'].apply(lambda x: x['text'])\n",
    "    df.insert(0, 'path', path)\n",
    "    return df\n",
    "\n",
    "references = pd.concat(map(annot_dataframe, result), ignore_index=True)\n",
    "\n",
    "hypotheses = pd.concat(map(preds_dataframe, result), ignore_index=True)\n",
    "hypotheses.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(references, hypotheses, on='path', suffixes=('_ref', '_hyp'))\n",
    "art_scores = data.apply(lambda row: cer(row['art_infringido_ref'], row['art_infringido_hyp']), axis=1)\n",
    "cond_scores = data.apply(lambda row: cer(row['conducta_ref'], row['conducta_hyp']), axis=1)\n",
    "\n",
    "print('art_infringido cer:', cer(data['art_infringido_ref'].tolist(), data['art_infringido_hyp'].tolist()))\n",
    "print('conducta cer:', cer(data['conducta_ref'].tolist(), data['conducta_hyp'].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
