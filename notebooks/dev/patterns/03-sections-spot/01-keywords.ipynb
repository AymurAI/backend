{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%load_ext aymurai.devtools.magic\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from aymurai.spacy.display import DocRender\n",
    "from aymurai.pipeline import AymurAIPipeline\n",
    "from aymurai.datasets.ar_juz_pcyf_10 import ArgentinaJuzgadoPCyF10Dataset\n",
    "\n",
    "colors = {\n",
    "    'SECTION:DECISION': 'red',\n",
    "    'SECTION:HEAD': 'red',\n",
    "    'KEYWORDS': 'blue'\n",
    "\n",
    "}\n",
    "render = DocRender(config={'colors': colors})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "private = ArgentinaJuzgadoPCyF10Dataset('private', use_cache=True)\n",
    "train, test = train_test_split(private, test_size=0.2, random_state=22)\n",
    "train, val = train_test_split(train, test_size=0.2, random_state=22)\n",
    "print('train:', len(train))\n",
    "print('test:', len(test))\n",
    "print('val:', len(val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pipeline definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%export aymurai.spacy.components.es_ar.keywords\n",
    "from copy import deepcopy\n",
    "from functools import reduce\n",
    "\n",
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "\n",
    "from aymurai.utils.misc import get_element\n",
    "from aymurai.spacy.utils import format_entity\n",
    "from aymurai.meta.pipeline_interfaces import Transform\n",
    "from aymurai.spacy.components.utils import filter_overlapping_matches\n",
    "from aymurai.datasets.ar_juz_pcyf_10 import ArgentinaJuzgadoPCyF10Dataset\n",
    "from aymurai.spacy.components.regex import EnhancedRegexMatcher\n",
    "\n",
    "FIELDS = [\"tipo_de_resolucion\", \"objeto_de_la_resolucion\", \"detalle\", \"decision\"]\n",
    "VALIDATION_FIELDS = ArgentinaJuzgadoPCyF10Dataset(\"validation-fields\").data\n",
    "VALIDATION_FIELDS = {k: v for k, v in VALIDATION_FIELDS.items() if k in FIELDS}\n",
    "\n",
    "\n",
    "class SpacyRulerKeywords(Transform):\n",
    "    def __init__(self):\n",
    "        global __nlp\n",
    "        __nlp = spacy.blank(\"es\")\n",
    "        self.matcher = EnhancedRegexMatcher(__nlp.vocab)\n",
    "\n",
    "        for field, validations in VALIDATION_FIELDS.items():\n",
    "            if field == \"tipo_de_resolucion\":\n",
    "                validations = [f\"resoluci[oó]n_{v}\" for v in validations]\n",
    "\n",
    "            validations += [v.replace(\"_\", \"[_\\s]+\") for v in validations]\n",
    "\n",
    "            self.matcher.add(field, patterns=validations)\n",
    "\n",
    "    def __call__(self, item):\n",
    "        item = deepcopy(item)\n",
    "        if not \"entities\" in item[\"data\"]:\n",
    "            item[\"data\"][\"entities\"] = []\n",
    "\n",
    "        # skip if there are no section parser\n",
    "        if not (sections := get_element(item, [\"data\", \"spans\", \"section\"], [])):\n",
    "            return item\n",
    "\n",
    "        sections = filter(\n",
    "            lambda x: x[\"label\"] in [\"KEYWORDS\"],\n",
    "            sections,\n",
    "        )\n",
    "        sections = sorted(sections, key=lambda e: e[\"start\"])\n",
    "\n",
    "        if not sections:\n",
    "            return item\n",
    "\n",
    "        offset = sections[0][\"start\"]\n",
    "\n",
    "        doc = __nlp(item[\"data\"][\"doc.text\"])\n",
    "        matches = self.matcher(doc)\n",
    "\n",
    "        # sort by score, lenght and position\n",
    "        matches = filter(lambda x: x[1] > offset, matches)\n",
    "        matches = sorted(matches, key=lambda x: (sum(x[3]), -(x[2] - x[1]), x[1]))\n",
    "        matches = filter_overlapping_matches(matches)\n",
    "        matches = list(matches)\n",
    "\n",
    "        for label, start, end, score in matches:\n",
    "            span = Span(doc, start=start, end=end, label=label)\n",
    "            item[\"data\"][\"entities\"] += [format_entity(span)]\n",
    "\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aymurai.spacy.components.loader\n",
    "from aymurai.spacy.ruler import SpacyRulerPipeline\n",
    "from aymurai.text.extraction import FulltextExtract\n",
    "from aymurai.text.normalize import JunkCleaner, TextNormalize\n",
    "from aymurai.spacy.rulers.section_parser import AymuraiRulerSectionParser\n",
    "from aymurai.models.dummy.art_infringido import DummyExtractorArtInfringido\n",
    "from aymurai.spacy.components.es_ar.art_infringido import SpacyRulerArtInfringido\n",
    "\n",
    "config = {\n",
    "    \"preprocess\": [\n",
    "        (\n",
    "            FulltextExtract,\n",
    "            {\n",
    "                \"extension\": \"pdf\",\n",
    "                \"method\": \"tesseract\",\n",
    "                \"language\": \"spa\",\n",
    "                \"errors\": \"ignore\",\n",
    "                \"use_cache\": True,\n",
    "            },\n",
    "        ),\n",
    "        (TextNormalize, {}),\n",
    "        (\n",
    "            JunkCleaner,\n",
    "            {\n",
    "                \"patterns\": [\n",
    "                    \"Juzgado PCyF N* 10 - Tacuarí 138, 7* Piso - juzcyf10ejusbaires.gob.ar - 4014-6821/20 - Gipcyf10\",\n",
    "                ]\n",
    "            },\n",
    "        ),\n",
    "        (\n",
    "            AymuraiRulerSectionParser,\n",
    "            {\n",
    "                \"base\": \"es\",\n",
    "                \"breakpoints\": {\n",
    "                    \"SECTION:DEVELOPMENT\": [\n",
    "                        \"DESARROLLO\",\n",
    "                    ],\n",
    "                    'SECTION:BACKGROUND': [\n",
    "                        \"ANTECEDENTES\",\n",
    "                        \"ANTECEDENTES Y ARGUMENTOS\",\n",
    "                        \"Antecedentes del caso\"\n",
    "                    ],\n",
    "                    \"SECTION:ARGUMENTS\": [\n",
    "                        \"ARGUMENTOS\",\n",
    "                        \"ANTECEDENTES Y ARGUMENTOS\",\n",
    "                        'CONSIDERO'\n",
    "                    ],\n",
    "                    \"SECTION:DECISION\": [\n",
    "                        \"DECID[EO]\",\n",
    "                        \"RESUELV[EO]\",\n",
    "                    ],\n",
    "                    'KEYWORDS': [\n",
    "                        \"PALABRAS[_\\s]+CLAVES?[\\w\\d\\s_:]*\",\n",
    "                    ]\n",
    "                },\n",
    "            },\n",
    "        ),\n",
    "        (SpacyRulerKeywords, {}),\n",
    "    ],\n",
    "    \"models\": [\n",
    "    ],\n",
    "    \"postprocess\": [],\n",
    "    \"multiprocessing\": {},\n",
    "    \"use_cache\": False,\n",
    "    # 'log_level': 'debug'\n",
    "}\n",
    "\n",
    "pipeline = AymurAIPipeline(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = pipeline.preprocess(train[:40])\n",
    "result = pipeline.predict(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "index = random.choice(range(len(result)))\n",
    "index=24\n",
    "print(index)\n",
    "registry = result[index]\n",
    "\n",
    "\n",
    "print(registry['path'])\n",
    "metadata = {k: v for k, v in registry['metadata'].items() if type(v) not in [dict, list]}\n",
    "print(json.dumps(metadata, indent=4))\n",
    "\n",
    "\n",
    "print('annotations')\n",
    "print('art infingido:', [x['art_infringido'] for x in registry['annotations']])\n",
    "print('codigo:', [x['codigo_o_ley'] for x in registry['annotations']])\n",
    "print('conducta:', [x['conducta'] for x in registry['annotations']])\n",
    "print('conducta detalle:', [x['conducta_descripcion'] for x in registry['annotations']])\n",
    "print('prediction')\n",
    "# print(registry['predictions'])\n",
    "# print('art_infringido:', registry['predictions']['records']['art_infringido'])\n",
    "# print('conducta:', registry['predictions']['records']['conducta'])\n",
    "\n",
    "\n",
    "print('\\n-------\\n')\n",
    "render(registry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = pipeline.preprocess(train)\n",
    "result = pipeline.predict(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import zip_longest\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from jiwer import cer\n",
    "import matplotlib.pyplot as plt\n",
    "from more_itertools import collapse\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from aymurai.meta.types import DataItem\n",
    "\n",
    "EMPTY_ENTITY = {\n",
    "    key: None for key in [\"text\", \"start\", \"end\", \"label\", \"start_char\", \"end_char\"]\n",
    "}\n",
    "\n",
    "\n",
    "def annot_dataframe(item: DataItem) -> pd.DataFrame:\n",
    "    path = item[\"path\"]\n",
    "    annots = item[\"annotations\"]\n",
    "    df = pd.DataFrame(annots)\n",
    "    df.insert(0, \"path\", path)\n",
    "\n",
    "    df = df[[\"path\", \"art_infringido\", \"conducta\", \"codigo_o_ley\"]]\n",
    "    return df\n",
    "\n",
    "def get_text(value):\n",
    "    if not isinstance(value, dict):\n",
    "        return value\n",
    "    return value.get('text', '')\n",
    "\n",
    "def preds_dataframe(item: DataItem) -> pd.DataFrame:\n",
    "    path = item[\"path\"]\n",
    "    records = item[\"predictions\"][\"records\"]\n",
    "    max_ = max(map(lambda x: len(x), records.values()))\n",
    "    if max_ > 1:\n",
    "        print(records)\n",
    "    if not max_:\n",
    "        return pd.DataFrame({\"path\": path}, index=pd.Index([0]))\n",
    "\n",
    "    for key, record in records.items():\n",
    "        records[key] = [val for val, _ in zip_longest(record, range(max_))]\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "\n",
    "    df[\"art_infringido\"] = df[\"art_infringido\"].apply(get_text)\n",
    "    df[\"conducta\"] = df[\"conducta\"].apply(get_text)\n",
    "    df[\"codigo_o_ley\"] = df[\"codigo_o_ley\"].apply(get_text)\n",
    "\n",
    "    df.insert(0, \"path\", path)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "references = pd.concat(map(annot_dataframe, result), ignore_index=True)\n",
    "\n",
    "hypotheses = pd.concat(map(preds_dataframe, result), ignore_index=True)\n",
    "hypotheses.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(references, hypotheses, on='path', suffixes=('_ref', '_hyp'))\n",
    "data.dropna(subset=[col for col in data.columns if col.endswith('_ref')], inplace=True)\n",
    "art_scores = data.apply(lambda row: cer(row['art_infringido_ref'], row['art_infringido_hyp']), axis=1)\n",
    "cond_scores = data.apply(lambda row: cer(row['conducta_ref'], row['conducta_hyp']), axis=1)\n",
    "codi_scores = data.apply(lambda row: cer(row['codigo_o_ley_ref'], row['codigo_o_ley_hyp']), axis=1)\n",
    "\n",
    "print('art_infringido cer:', cer(data['art_infringido_ref'].tolist(), data['art_infringido_hyp'].tolist()))\n",
    "print('conducta cer:', cer(data['conducta_ref'].tolist(), data['conducta_hyp'].tolist()))\n",
    "print('codigo_o_ley cer:', cer(data['codigo_o_ley_ref'].tolist(), data['codigo_o_ley_hyp'].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = filter(lambda x: x['path'] == '/resources/restricted/ar-juz-pcyf-10/RESOLUCIONES DEL JUZGADO-pdf/2021/TOMO 38_JUNIO _21/3587_38 CAUSA 77325_21.pdf', test)\n",
    "example = list(example)\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = pipeline.preprocess(test)\n",
    "result = pipeline.predict(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from jiwer import cer\n",
    "import matplotlib.pyplot as plt\n",
    "from more_itertools import collapse\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from aymurai.meta.types import DataItem\n",
    "\n",
    "\n",
    "def annot_dataframe(item: DataItem) -> pd.DataFrame:\n",
    "    path = item['path']\n",
    "    annots = item['annotations']\n",
    "    df = pd.DataFrame(annots)\n",
    "    df.insert(0, 'path', path)\n",
    "\n",
    "    df = df[['path', 'art_infringido', 'conducta', 'codigo_o_ley']]\n",
    "    return df\n",
    "\n",
    "def preds_dataframe(item: DataItem) -> pd.DataFrame:\n",
    "    path = item['path']\n",
    "    preds = item['predictions']['records']\n",
    "    df = pd.DataFrame(preds)\n",
    "    df['art_infringido'] = df['art_infringido'].apply(lambda x: x['text'])\n",
    "    df['conducta'] = df['conducta'].apply(lambda x: x['text'])\n",
    "    df['codigo_o_ley'] = df['codigo_o_ley'].apply(lambda x: x['text'])\n",
    "    df.insert(0, 'path', path)\n",
    "    return df\n",
    "\n",
    "references = pd.concat(map(annot_dataframe, result), ignore_index=True)\n",
    "\n",
    "hypotheses = pd.concat(map(preds_dataframe, result), ignore_index=True)\n",
    "hypotheses.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(references, hypotheses, on='path', suffixes=('_ref', '_hyp'))\n",
    "art_scores = data.apply(lambda row: cer(row['art_infringido_ref'], row['art_infringido_hyp']), axis=1)\n",
    "cond_scores = data.apply(lambda row: cer(row['conducta_ref'], row['conducta_hyp']), axis=1)\n",
    "\n",
    "print('art_infringido cer:', cer(data['art_infringido_ref'].tolist(), data['art_infringido_hyp'].tolist()))\n",
    "print('conducta cer:', cer(data['conducta_ref'].tolist(), data['conducta_hyp'].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
