{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%load_ext aymurai.devtools.magic\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sudo python -m spacy download es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class DFtoDataset(Dataset):\n",
    "    def __init__(self, texts: list[str], targets: list[int]):\n",
    "\n",
    "        self.x_ = texts.values\n",
    "        self.y_ = targets.values\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y_)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Note y and x are inverted to mimic AR_NEWS dataset format\n",
    "        return self.y_[idx], self.x_[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\n",
    "    \"sentences-decision-manual.csv\",\n",
    "    usecols=[\"path\", \"nro_registro\", \"tomo\", \"sentence\", \"decision\", \"hace_lugar\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)\n",
    "# target_classes = [\"none\", \"decision:no_hace_lugar\", \"decision:hace_lugar\"]\n",
    "\n",
    "\n",
    "def force_bool(value):\n",
    "    return True if value in ['True', True, 1, \"1\"] else False\n",
    "\n",
    "\n",
    "def get_category(pair):\n",
    "    decision, hace_lugar = pair\n",
    "    # print(decision, hace_lugar, type(decision), type(hace_lugar))\n",
    "    if not decision:\n",
    "        cat = 0\n",
    "    elif decision and not hace_lugar:\n",
    "        cat = 1\n",
    "    elif decision and hace_lugar:\n",
    "        cat = 1\n",
    "    else:\n",
    "        raise \"not valid\"\n",
    "    return cat\n",
    "\n",
    "\n",
    "# # data[['decision', 'hace_lugar']] = data[['decision', 'hace_lugar']].apply(lambda x: literal_eval(x), axis=1).astype(bool) \n",
    "data['decision'] = data['decision'].apply(force_bool).astype(bool) \n",
    "data['hace_lugar'] = data['hace_lugar'].apply(force_bool).astype(bool) \n",
    "data[\"category\"] = data[[\"decision\", \"hace_lugar\"]].apply(get_category, axis=1)\n",
    "data.dropna(subset=['category'], inplace=True)\n",
    "\n",
    "data.drop_duplicates(subset=\"sentence\", inplace=True)\n",
    "print(len(data))\n",
    "data[\"sentence\"].apply(lambda x: len(x.split(\" \"))).hist(\n",
    "    bins=[32 * i for i in range(10)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset = DFtoDataset(data['sentence'], data['category'])\n",
    "\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "test, val = train_test_split(test, test_size=0.5, random_state=42)\n",
    "\n",
    "train_df = train.copy()\n",
    "val_df = val.copy()\n",
    "test_df = test.copy()\n",
    "\n",
    "train = DFtoDataset(train['sentence'], train['category'])\n",
    "val = DFtoDataset(val['sentence'], val['category'])\n",
    "test = DFtoDataset(test['sentence'], test['category'])\n",
    "\n",
    "\n",
    "print(len(train))\n",
    "print(len(val))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "DEVICE = 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[59]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aymurai.models.decision.binregex import DecisionConv1dBinRegex\n",
    "\n",
    "model = DecisionConv1dBinRegex(\n",
    "    tokenizer_path=\"https://drive.google.com/uc?id=1eljQOinpObdfBREIKxVnC5Y2g_sbhPHT&confirm=true\",\n",
    "    model_checkpoint=\"https://drive.google.com/uc?id=19_YmBJnO06iS0qW8ak0zl0EIsJYin8kQ&confirm=true\",\n",
    "    device=\"cpu\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"1. DECLARAR EXTINGUIDA LA ACCIÓN PENAL en este caso por cumplimiento de la suspensión del proceso a prueba, y SOBRESEER a EZEQUIEL CAMILO MARCONNI, DNI 11.222.333, en orden a los delitos de lesiones leves agravadas, amenazas simples y agravadas por el uso de armas.\"\n",
    "\n",
    "input_ids = model.tokenizer.encode_batch([text])\n",
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model(input_ids).exp().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from aymurai.models.decision.tokenizer import Tokenizer\n",
    "from unidecode import unidecode\n",
    "\n",
    "tokenizer = model.tokenizer \n",
    "vocab = tokenizer.vocab\n",
    "\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_tokens = 128\n",
    "\n",
    "def vectorize_text(batch):\n",
    "    Y, X = list(zip(*batch))\n",
    "    # X = [vocab(tokenizer(text)) for text in X]\n",
    "    # X = [tokens+([0]* (max_tokens-len(tokens))) if len(tokens)<max_tokens else tokens[:max_tokens] for tokens in X] ## Bringing all samples to max_tokens length.\n",
    "    X = tokenizer.encode_batch(X)\n",
    "\n",
    "    xx, yy = torch.tensor(X, dtype=torch.int32), torch.tensor(Y) ## We have deducted 1 from target names to get them in range [0,1,2,3] from [1,2,3,4]\n",
    "    xx = xx.to(DEVICE)\n",
    "    yy = yy.to(DEVICE)\n",
    "    return xx, yy\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=1024, collate_fn=vectorize_text, shuffle=True)\n",
    "val_loader  = DataLoader(val,  batch_size=1024, collate_fn=vectorize_text)\n",
    "test_loader  = DataLoader(test,  batch_size=1024, collate_fn=vectorize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_text([[1, 'En función de tales motivos, dispondré la']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for X, Y in train_loader:\n",
    "    print(X.shape, Y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = [data[0] for data in train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    x, y = batch\n",
    "    # x = x.to('cuda')\n",
    "    \n",
    "    print(x.shape)\n",
    "    b = model.model.forward(x)\n",
    "    print(b)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print(\"TRAIN\")\n",
    "\n",
    "reference = []\n",
    "hypothesis = []\n",
    "probs = []\n",
    "ltmodel = model.model.to(DEVICE)\n",
    "for batch in train_loader:\n",
    "    x, y = batch\n",
    "    x = x.to(DEVICE)\n",
    "\n",
    "    y_pred = ltmodel(x)  # .exp().argmax(axis=1)\n",
    "\n",
    "    hypothesis.append(y_pred.cpu().detach().numpy())\n",
    "    reference.append(y.cpu().detach().numpy())\n",
    "\n",
    "\n",
    "reference = np.concatenate(reference)\n",
    "hypothesis = np.concatenate(hypothesis)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "\n",
    "confusion = confusion_matrix(reference, hypothesis.argmax(axis=1))\n",
    "print(confusion)\n",
    "sns.heatmap(confusion, annot=True, fmt=\"d\", ax=ax)\n",
    "ax.set_xlabel(\"hypothesis\")\n",
    "ax.set_ylabel(\"reference\")\n",
    "# ax.set_xticklabels([\"None\", \"desicion/no_hace_lugar\", \"descion/hace_lugar\"])\n",
    "# ax.set_yticklabels([\"None\", \"desicion/no_hace_lugar\", \"descion/hace_lugar\"])\n",
    "ax.set_title(\"TRAIN\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "report = classification_report(reference, hypothesis.argmax(axis=1), output_dict=True)\n",
    "pd.DataFrame(report).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(hypothesis)\n",
    "\n",
    "score_class0 = np.exp(hypothesis)[:, 0]\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        'score': 1- score_class0,\n",
    "        'true_class': reference\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, subplot = plt.subplots(1, 2, figsize=(14, 4), sharey=True)\n",
    "\n",
    "\n",
    "sns.histplot(\n",
    "    data=df,\n",
    "    x=\"score\",\n",
    "    hue='true_class'\n",
    "    # color=\"r\",\n",
    "    # alpha=0.8,\n",
    "    # label=\"class 0\",\n",
    "    # ax=subplot[0],\n",
    "    # stat=\"probability\",\n",
    ")\n",
    "# sns.histplot(\n",
    "#     df.query(\"true_class == 1\"),\n",
    "#     x=\"score\",\n",
    "#     color=\"b\",\n",
    "#     alpha=0.5,\n",
    "#     label=\"class 1\",\n",
    "#     ax=subplot[1],\n",
    "#     stat=\"probability\",\n",
    "# )\n",
    "# sns.histplot(class1, color='b', alpha=0.5)\n",
    "subplot[0].set_yscale(\"log\")\n",
    "subplot[1].set_yscale(\"log\")\n",
    "# subplot.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {\n",
    "    'threshold': [],\n",
    "    'precision': [],\n",
    "    'recall': [],\n",
    "    'f1-score': []\n",
    "}\n",
    "\n",
    "y_true = df['true_class']\n",
    "for cutoff in np.linspace(0, 1, 100):\n",
    "    y_pred = df['score'] > cutoff\n",
    "    \n",
    "    scores['threshold'].append(cutoff)\n",
    "\n",
    "    report = classification_report(y_true, y_pred, output_dict=True)\n",
    "    # print(report['1'])\n",
    "    precision = report['1']['precision']\n",
    "    recall = report['1']['recall']\n",
    "    f1score = report['1']['f1-score']\n",
    "    scores['precision'].append(precision)\n",
    "    scores['recall'].append(recall)\n",
    "    scores['f1-score'].append(f1score)\n",
    "\n",
    "scores = pd.DataFrame(scores)\n",
    "scores = scores.melt(['threshold'], value_vars=['precision', 'recall', 'f1-score'])\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=scores, x='threshold', y='value', hue='variable')\n",
    "\n",
    "THRESHOLD = 0.90\n",
    "plt.axvline(THRESHOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print('VAL')\n",
    "\n",
    "reference = []\n",
    "hypothesis = []\n",
    "probs = []\n",
    "ltmodel = ltmodel.to(DEVICE)\n",
    "for batch in val_loader:\n",
    "    x, y = batch\n",
    "    x = x.to(DEVICE)\n",
    "\n",
    "    y_pred = ltmodel(x)#.exp().argmax(axis=1)\n",
    "    \n",
    "\n",
    "    hypothesis.append(y_pred.cpu().detach().numpy())\n",
    "    reference.append(y.cpu().detach().numpy())\n",
    "\n",
    "\n",
    "reference = np.concatenate(reference)\n",
    "hypothesis = np.concatenate(hypothesis)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "\n",
    "confusion =  confusion_matrix(reference, hypothesis.argmax(axis=1))\n",
    "print(confusion)\n",
    "sns.heatmap(confusion, annot=True, fmt='d', ax=ax)\n",
    "ax.set_xlabel(\"hypothesis\")\n",
    "ax.set_ylabel(\"reference\")\n",
    "# ax.set_xticklabels([\"None\", \"desicion/no_hace_lugar\", \"descion/hace_lugar\"])\n",
    "# ax.set_yticklabels([\"None\", \"desicion/no_hace_lugar\", \"descion/hace_lugar\"])\n",
    "ax.set_title('VAL')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "report = classification_report(reference, hypothesis.argmax(axis=1), output_dict=True)\n",
    "pd.DataFrame(report).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(hypothesis)\n",
    "\n",
    "score_class0 = np.exp(hypothesis)[:, 0]\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        'score': 1- score_class0,\n",
    "        'true_class': reference\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, subplot = plt.subplots(1, 2, figsize=(14, 4), sharey=True)\n",
    "\n",
    "\n",
    "sns.histplot(df.query('true_class == 0'), x='score', color='r', alpha=0.8, label='class 0', ax=subplot[0], stat='probability')\n",
    "sns.histplot(df.query('true_class == 1'), x='score', color='b', alpha=0.5, label='class 1',ax=subplot[1], stat='probability')\n",
    "# sns.histplot(class1, color='b', alpha=0.5)\n",
    "subplot[0].set_yscale('log')\n",
    "subplot[1].set_yscale('log')\n",
    "# subplot.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=scores, x='threshold', y='value', hue='variable')\n",
    "\n",
    "THRESHOLD = 0.90\n",
    "plt.axvline(THRESHOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print('TEST')\n",
    "\n",
    "reference = []\n",
    "hypothesis = []\n",
    "probs = []\n",
    "ltmodel = ltmodel.to(DEVICE)\n",
    "for batch in test_loader:\n",
    "    x, y = batch\n",
    "    x = x.to(DEVICE)\n",
    "\n",
    "    y_pred = ltmodel(x)\n",
    "    y_pred = ltmodel(x).exp()[:, 1]  > THRESHOLD #.argmax(axis=1)\n",
    "    \n",
    "\n",
    "    hypothesis.append(y_pred.cpu().detach().numpy())\n",
    "    reference.append(y.cpu().detach().numpy())\n",
    "\n",
    "\n",
    "reference = np.concatenate(reference)\n",
    "hypothesis = np.concatenate(hypothesis)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "\n",
    "confusion =  confusion_matrix(reference, hypothesis)\n",
    "print(confusion)\n",
    "sns.heatmap(confusion, annot=True, fmt='d', ax=ax)\n",
    "ax.set_xlabel(\"hypothesis\")\n",
    "ax.set_ylabel(\"reference\")\n",
    "# ax.set_xticklabels([\"None\", \"desicion/no_hace_lugar\", \"descion/hace_lugar\"])\n",
    "# ax.set_yticklabels([\"None\", \"desicion/no_hace_lugar\", \"descion/hace_lugar\"])\n",
    "ax.set_title('TEST')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "report = classification_report(reference, hypothesis, output_dict=True)\n",
    "pd.DataFrame(report).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sentences = [pair[1] for pair in test]\n",
    "\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"sentence\": sentences,\n",
    "        \"decision\": test_df['decision'],\n",
    "        \"hace_lugar\": test_df['hace_lugar'],\n",
    "        \"cat\": reference,\n",
    "        \"pred_cat\": hypothesis.astype(int),\n",
    "    }\n",
    ")\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\n",
    "    \"display.max_columns\",\n",
    "    1000,\n",
    "    \"display.width\",\n",
    "    1000,\n",
    "    \"display.max_colwidth\",\n",
    "    None,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pred_ok'] = df['cat'] == df['pred_cat']\n",
    "# test_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_.query('decision == 0 and pred_decision and not pred_hace_lugar').sample(1)\n",
    "df.query('pred_ok == 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# regex classificacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "\n",
    "pattern = regex.compile(r\"(?i)(no hacer? lugar|rechaz[ao]r?|no admitir|no convalidar|no autorizar|declarar inadmisible)\")\n",
    "\n",
    "def recategorize(row, cat_col='pred_cat'):\n",
    "    # print(type(row))\n",
    "    # print(len(row))\n",
    "    # # i, row = row\n",
    "    # print(row)\n",
    "    # print()\n",
    "\n",
    "    decision = row['decision']\n",
    "    hace_lugar = row['hace_lugar']\n",
    "    match (decision, hace_lugar):\n",
    "        case (1, 0):\n",
    "            row['cat'] = 1\n",
    "        case (1, 1):\n",
    "            row['cat'] = 2\n",
    "        case _:\n",
    "            row['cat'] = 0\n",
    "\n",
    "    decision_pred = row[cat_col]\n",
    "    if not decision_pred:\n",
    "        return row\n",
    "    \n",
    "    match = pattern.findall(row['sentence'])\n",
    "    if not match:\n",
    "        row[cat_col] = 2\n",
    "    return row\n",
    "    \n",
    "a = train_df.query('decision and not hace_lugar')\n",
    "df_recat = df.apply(recategorize, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recat['pred_cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reference = df_recat['cat']\n",
    "hypothesis = df_recat['pred_cat']\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "\n",
    "confusion =  confusion_matrix(reference, hypothesis)\n",
    "print(confusion)\n",
    "sns.heatmap(confusion, annot=True, fmt='d', ax=ax)\n",
    "ax.set_xlabel(\"hypothesis\")\n",
    "ax.set_ylabel(\"reference\")\n",
    "# ax.set_xticklabels([\"None\", \"desicion/no_hace_lugar\", \"descion/hace_lugar\"])\n",
    "# ax.set_yticklabels([\"None\", \"desicion/no_hace_lugar\", \"descion/hace_lugar\"])\n",
    "ax.set_title('TEST')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "report = classification_report(reference, hypothesis, output_dict=True)\n",
    "pd.DataFrame(report).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(df_recat['cat'], df_recat['pred_cat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recat.query('pred_cat == 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Oct 12 2022, 19:14:26) [GCC 9.4.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "97cc609b13305c559618ec78a438abc56230b9381f827f22d070313b9a1f3777"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
