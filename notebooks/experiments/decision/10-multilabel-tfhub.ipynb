{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo pip install tensorflow_hub tensorflow-gpu tensorflow_text tensorflow-addons scikit-multilearn iterative-stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%load_ext aymurai.devtools.magic\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('sentences-decision.csv')\n",
    "data.drop_duplicates(subset='sentence', inplace=True)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "from sklearn.utils import indexable, _safe_indexing\n",
    "from sklearn.utils.validation import _num_samples\n",
    "from sklearn.model_selection._split import _validate_shuffle_split\n",
    "from itertools import chain\n",
    "\n",
    "def multilabel_train_test_split(*arrays,\n",
    "                                test_size=None,\n",
    "                                train_size=None,\n",
    "                                random_state=None,\n",
    "                                shuffle=True,\n",
    "                                stratify=None):\n",
    "    \"\"\"\n",
    "    Train test split for multilabel classification. Uses the algorithm from: \n",
    "    'Sechidis K., Tsoumakas G., Vlahavas I. (2011) On the Stratification of Multi-Label Data'.\n",
    "    \"\"\"\n",
    "    if stratify is None:\n",
    "        return train_test_split(*arrays, test_size=test_size,train_size=train_size,\n",
    "                                random_state=random_state, stratify=None, shuffle=shuffle)\n",
    "    \n",
    "    assert shuffle, \"Stratified train/test split is not implemented for shuffle=False\"\n",
    "    \n",
    "    n_arrays = len(arrays)\n",
    "    arrays = indexable(*arrays)\n",
    "    n_samples = _num_samples(arrays[0])\n",
    "    n_train, n_test = _validate_shuffle_split(\n",
    "        n_samples, test_size, train_size, default_test_size=0.25\n",
    "    )\n",
    "    cv = MultilabelStratifiedShuffleSplit(test_size=n_test, train_size=n_train, random_state=123)\n",
    "    train, test = next(cv.split(X=arrays[0], y=stratify))\n",
    "\n",
    "    return list(\n",
    "        chain.from_iterable(\n",
    "            (_safe_indexing(a, train), _safe_indexing(a, test)) for a in arrays\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset from private data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "x = data['sentence'].values\n",
    "y = data[['decision', 'hace_lugar']].values\n",
    "\n",
    "train, test = multilabel_train_test_split(data, test_size=0.2, random_state=42, stratify=y)\n",
    "train, val = multilabel_train_test_split(test, test_size=0.2, random_state=42)\n",
    "\n",
    "print('train:', len(train))\n",
    "print('test:', len(test))\n",
    "print('val:', len(val))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"decisiones: {len(train.query('decision'))} from {len(train)} sentences\")\n",
    "print(f\"hace lugar: {len(train.query('decision and hace_lugar'))} from {len(train.query('decision'))} decisiones\")\n",
    "\n",
    "print(f\"decisiones: {len(val.query('decision'))} from {len(val)} sentences\")\n",
    "print(f\"hace lugar: {len(val.query('decision and hace_lugar'))} from {len(val.query('decision'))} decisiones\")\n",
    "\n",
    "print(f\"decisiones: {len(test.query('decision'))} from {len(test)} sentences\")\n",
    "print(f\"hace lugar: {len(test.query('decision and hace_lugar'))} from {len(test.query('decision'))} decisiones\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# manual train balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_0 = train.query(\"decision == 0\")\n",
    "# class_1 = train.query(\"decision == 1\")\n",
    "# train = pd.concat(\n",
    "#     [\n",
    "#         class_0.sample(len(class_1)),\n",
    "#         class_1,\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# print(f\"decisiones: {len(train.query('decision'))} from {len(train)} sentences\")\n",
    "# print(f\"hace lugar: {len(train.query('decision and hace_lugar'))} from {len(train.query('decision'))} decisiones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_train = train['sentence'].values\n",
    "y_train = train[['decision', 'hace_lugar']].values\n",
    "\n",
    "x_val = val['sentence'].values\n",
    "y_val = val[['decision', 'hace_lugar']].values\n",
    "\n",
    "x_test = test['sentence'].values\n",
    "y_test = test[['decision', 'hace_lugar']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from more_itertools import collapse\n",
    "from itertools import chain\n",
    "\n",
    "vocab = set()\n",
    "\n",
    "# text = chain()\n",
    "# text = map(str.lower, text)\n",
    "text = data['sentence'].values\n",
    "text = map(str.split, text)\n",
    "text = collapse(text)\n",
    "vocab.update(text)\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class_weights = {k: w for k, w in enumerate(1/(y_train.sum(axis=0)/len(y_train)))}\n",
    "class_weights\n",
    "\n",
    "class_weights = {1: 1 - y_train.sum()/np.prod(y_train.shape)}\n",
    "class_weights[0] = 1 - class_weights[1]\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tqdm.auto import tqdm\n",
    "import tensorflow_text as text\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_addons as tfa\n",
    "from more_itertools import flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, TextVectorization, LSTM, Bidirectional, Embedding\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "num_classes = y_train.shape[-1]\n",
    "f1_score = tfa.metrics.F1Score(\n",
    "    num_classes=num_classes, average=\"macro\", name=\"f1_score\"\n",
    ")\n",
    "\n",
    "\n",
    "text_vectorizer = TextVectorization(\n",
    "    max_tokens=len(vocab),\n",
    "    ngrams=3,\n",
    "    output_mode=\"tf_idf\",\n",
    "    split='whitespace',\n",
    "    standardize='lower_and_strip_punctuation'\n",
    ")\n",
    "\n",
    "with tf.device(\"/CPU:0\"):\n",
    "    text_vectorizer.adapt(x_train)\n",
    "\n",
    "def get_model_tfidf(encoder_trainable: bool = False) -> Model:\n",
    "    input_ = Input(shape=[], dtype=tf.string)\n",
    "    x = text_vectorizer(input_)\n",
    "    embed_shape = x.shape[1]\n",
    "    # x = Dense(1024, activation=\"relu\")(x)\n",
    "    x = Dense(512, activation=\"relu\")(x)\n",
    "    x = Dense(128, activation=\"relu\")(x)\n",
    "\n",
    "    output = Dense(num_classes, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=[input_], outputs=output)\n",
    "\n",
    "    adamw = tf.keras.optimizers.experimental.AdamW()\n",
    "    model.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=adamw,\n",
    "        metrics=[\"categorical_accuracy\", f1_score],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def get_model(encoder_trainable: bool = False, lr=1e-3) -> Model:\n",
    "\n",
    "    input_ = Input(shape=[], dtype=tf.string)\n",
    "    x = hub.KerasLayer(\n",
    "        # \"https://tfhub.dev/google/sentence-t5/st5-base/1\",\n",
    "        # \"https://tfhub.dev/google/universal-sentence-encoder-large/5\",\n",
    "        # \"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
    "        # \"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\",\n",
    "        \"https://tfhub.dev/google/nnlm-es-dim128/2\",\n",
    "        trainable=encoder_trainable,\n",
    "    )(input_)\n",
    "\n",
    "    embed_shape = x.shape[1]\n",
    "    # x = Bidirectional(LSTM(128))\n",
    "    x = LSTM(128)\n",
    "    # x = Dense(embed_shape, activation=\"relu\")(x)\n",
    "    # x = Dense(embed_shape, activation=\"relu\")(x)\n",
    "\n",
    "    output = Dense(num_classes, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=[input_], outputs=output)\n",
    "    adamw = tf.keras.optimizers.experimental.AdamW(lr=lr)\n",
    "    model.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=adamw,\n",
    "        metrics=[\"categorical_accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "# model = get_model_tfidf(encoder_trainable=True)\n",
    "model = get_model(encoder_trainable=False)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function keeps the initial learning rate for the first ten epochs\n",
    "# and decreases it exponentially after that.\n",
    "def scheduler(epoch, lr):\n",
    "  if epoch < 3:\n",
    "    return lr\n",
    "  else:\n",
    "    return lr * tf.math.exp(-0.1)\n",
    "\n",
    "callbacks = [\n",
    "    # ModelCheckpoint(\"glove_embeddings_sequence_model.keras\", save_best_only=True, save_format='tf'),\n",
    "    EarlyStopping(patience=15, monitor=\"val_loss\", restore_best_weights=True),\n",
    "    # tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "]\n",
    "\n",
    "\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import numpy as np   \n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, x_set, y_set, batch_size):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        return batch_x, batch_y\n",
    "\n",
    "train_gen = DataGenerator(x_train, y_train, 32)\n",
    "val_gen = DataGenerator(x_val, y_val, 32)\n",
    "\n",
    "\n",
    "# model.fit(\n",
    "#     x_train,\n",
    "#     y_train,\n",
    "#     batch_size=64,\n",
    "#     validation_data=(x_val, y_val),\n",
    "#     epochs=50,\n",
    "#     callbacks=callbacks,\n",
    "#     class_weight=class_weights,\n",
    "#     shuffle=True,\n",
    "# )\n",
    "model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=50,\n",
    "    callbacks=callbacks,\n",
    "    class_weight=class_weights,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORIES = ['decision', 'hace lugar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
    "\n",
    "print('TRAIN')\n",
    "\n",
    "hypothesis = model.predict(x_train)\n",
    "reference = y_train\n",
    "\n",
    "fig, subplot = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "confusion = multilabel_confusion_matrix(reference, hypothesis > 0.5)\n",
    "for ax, matrix, cat in zip(subplot.flatten(), confusion, CATEGORIES):\n",
    "    sns.heatmap(matrix, annot=True, fmt='d', ax=ax)\n",
    "    ax.set_xlabel(\"hypothesis\")\n",
    "    ax.set_ylabel(\"reference\")\n",
    "    ax.set_xticklabels([\"false\", \"true\"])\n",
    "    ax.set_yticklabels([\"false\", \"true\"])\n",
    "    ax.set_title(cat)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "report = classification_report(reference, hypothesis > 0.5, output_dict=True, target_names=CATEGORIES)\n",
    "pd.DataFrame(report).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
    "\n",
    "print('VALIDATION')\n",
    "\n",
    "hypothesis = model.predict(x_val)\n",
    "reference = y_val\n",
    "\n",
    "fig, subplot = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "confusion = multilabel_confusion_matrix(reference, hypothesis > 0.5)\n",
    "for ax, matrix, cat in zip(subplot.flatten(), confusion, CATEGORIES):\n",
    "    sns.heatmap(matrix, annot=True, fmt='d', ax=ax)\n",
    "    ax.set_xlabel(\"hypothesis\")\n",
    "    ax.set_ylabel(\"reference\")\n",
    "    ax.set_xticklabels([\"false\", \"true\"])\n",
    "    ax.set_yticklabels([\"false\", \"true\"])\n",
    "    ax.set_title(cat)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "report = classification_report(reference, hypothesis > 0.5, output_dict=True, target_names=CATEGORIES)\n",
    "pd.DataFrame(report).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
    "\n",
    "print('TEST')\n",
    "\n",
    "hypothesis = model.predict(x_test)\n",
    "reference = y_test\n",
    "\n",
    "fig, subplot = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "confusion = multilabel_confusion_matrix(reference, hypothesis > 0.5)\n",
    "for ax, matrix, cat in zip(subplot.flatten(), confusion, CATEGORIES):\n",
    "    sns.heatmap(matrix, annot=True, fmt='d', ax=ax)\n",
    "    ax.set_xlabel(\"hypothesis\")\n",
    "    ax.set_ylabel(\"reference\")\n",
    "    ax.set_xticklabels([\"false\", \"true\"])\n",
    "    ax.set_yticklabels([\"false\", \"true\"])\n",
    "    ax.set_title(cat)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "report = classification_report(reference, hypothesis > 0.5, output_dict=True, target_names=CATEGORIES)\n",
    "pd.DataFrame(report).T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
