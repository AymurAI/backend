{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%load_ext aymurai.devtools.magic\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('sentences-decision.csv')\n",
    "target_classes = [\"none\", \"decision:no_hace_lugar\", \"decision:hace_lugar\"]\n",
    "\n",
    "def get_category(pair):\n",
    "    decision, hace_lugar = pair\n",
    "    if not decision:\n",
    "        cat = 0\n",
    "    elif decision and not hace_lugar:\n",
    "        cat = 1\n",
    "    elif decision and hace_lugar == 1:\n",
    "        cat = 2\n",
    "    else:\n",
    "        raise \"not valid\"\n",
    "    return cat\n",
    "\n",
    "data['category'] = data[['decision', 'hace_lugar']].apply(get_category, axis=1) \n",
    "\n",
    "data.drop_duplicates(subset='sentence', inplace=True)\n",
    "print(len(data))\n",
    "data['sentence'].apply(lambda x: len(x.split(' '))).hist(bins=[32*i for i in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "data = pd.read_csv(\"sentences-decision-manual.csv\", usecols=['path', 'nro_registro', 'tomo', 'sentence', 'decision', 'hace_lugar'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data.dropna(inplace=True)\n",
    "# target_classes = [\"none\", \"decision:no_hace_lugar\", \"decision:hace_lugar\"]\n",
    "\n",
    "\n",
    "def force_bool(value):\n",
    "    return True if value in ['True', True, 1, \"1\"] else False\n",
    "\n",
    "\n",
    "def get_category(pair):\n",
    "    decision, hace_lugar = pair\n",
    "    # print(decision, hace_lugar, type(decision), type(hace_lugar))\n",
    "    if not decision:\n",
    "        cat = 0\n",
    "    elif decision and not hace_lugar:\n",
    "        cat = 1\n",
    "    elif decision and hace_lugar:\n",
    "        cat = 2\n",
    "    else:\n",
    "        raise \"not valid\"\n",
    "    return cat\n",
    "\n",
    "\n",
    "# # data[['decision', 'hace_lugar']] = data[['decision', 'hace_lugar']].apply(lambda x: literal_eval(x), axis=1).astype(bool) \n",
    "data['decision'] = data['decision'].apply(force_bool).astype(bool) \n",
    "data['hace_lugar'] = data['hace_lugar'].apply(force_bool).astype(bool) \n",
    "data[\"category\"] = data[[\"decision\", \"hace_lugar\"]].apply(get_category, axis=1)\n",
    "data.dropna(subset=['category'], inplace=True)\n",
    "\n",
    "data.drop_duplicates(subset=\"sentence\", inplace=True)\n",
    "print(len(data))\n",
    "data[\"sentence\"].apply(lambda x: len(x.split(\" \"))).hist(\n",
    "    bins=[32 * i for i in range(10)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train, test = train_test_split(\n",
    "    data,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=data[\"category\"],\n",
    ")\n",
    "test, val = train_test_split(\n",
    "    test,\n",
    "    test_size=0.5,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"train:\", len(train))\n",
    "print(\"test:\", len(test))\n",
    "print(\"val:\", len(val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"cat 0: {len(train.query('category == 0'))} from {len(train)} sentences\")\n",
    "print(f\"cat 1: {len(train.query('category == 1'))} from {len(train)} sentences\")\n",
    "print(f\"cat 2: {len(train.query('category == 2'))} from {len(train)} sentences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# manual train balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_0 = train.query(\"decision == 0\")\n",
    "# class_1 = train.query(\"decision == 1\")\n",
    "# train = pd.concat(\n",
    "#     [\n",
    "#         class_0.sample(len(class_1), random_state=42),\n",
    "#         class_1,\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# print(f\"decisiones: {len(train.query('decision'))} from {len(train)} sentences\")\n",
    "# print(f\"hace lugar: {len(train.query('decision and hace_lugar'))} from {len(train.query('decision'))} decisiones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    \"balanced\", classes=np.unique(train['category']), y=train['category']\n",
    ")\n",
    "# class_weights = {k: v for k, v in enumerate(class_weights)}\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "\n",
    "counts = np.bincount(train['category'])\n",
    "labels_weights = 1. / counts\n",
    "weights = labels_weights[train['category']]\n",
    "train_sampler = WeightedRandomSampler(weights, len(weights), replacement=True)\n",
    "\n",
    "counts = np.bincount(val['category'])\n",
    "labels_weights = 1. / counts\n",
    "weights = labels_weights[val['category']]\n",
    "val_sampler = WeightedRandomSampler(weights, len(weights), replacement=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "nlp = spacy.blank(\"es\")\n",
    "\n",
    "\n",
    "def encode_text(text):\n",
    "    return [t.text for t in nlp.tokenizer(text.lower())]\n",
    "\n",
    "\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield encode_text(text)\n",
    "\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train[\"sentence\"]), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%export aymurai.models.decision.torch.tokenizer\n",
    "\n",
    "import spacy\n",
    "import torch\n",
    "\n",
    "class Tokenizer(object):\n",
    "    def __init__(self, vocab):\n",
    "        self.max_len = 128\n",
    "        self.nlp = spacy.blank('es')\n",
    "        self.vocab = vocab\n",
    "    \n",
    "    def save(self, path: str):\n",
    "        torch.save(self.vocab, path)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path:str):\n",
    "        vocab = torch.load(path)\n",
    "        return cls(vocab=vocab)\n",
    "    \n",
    "        \n",
    "    def tokenize(self, text: str):\n",
    "        return [t.text for t in self.nlp.tokenizer(text.lower())]\n",
    "\n",
    "    def encode(self, text: int):\n",
    "        tokens = self.tokenize(text)[:self.max_len]\n",
    "        indices =  self.vocab(tokens)\n",
    "        indices = torch.tensor(indices, dtype=torch.int64)\n",
    "        indices = torch.nn.functional.pad(indices, (0, self.max_len-len(indices)))\n",
    "        return indices\n",
    "    \n",
    "    def encode_batch(self, texts):\n",
    "        indices = [self.encode(text) for text in texts]\n",
    "        indices = torch.stack(indices)\n",
    "        return indices\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class DFtoDataset(Dataset):\n",
    "    def __init__(self, texts: list[str], targets: list[int]):\n",
    "\n",
    "        self.x_ = texts\n",
    "        self.y_ = targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y_)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x_[idx], self.y_[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from aymurai.models.decision.torch.tokenizer import Tokenizer\n",
    "\n",
    "train_dataset = DFtoDataset(train[\"sentence\"].values, train[\"category\"].values)\n",
    "val_dataset = DFtoDataset(val[\"sentence\"].values, val[\"category\"].values)\n",
    "test_dataset = DFtoDataset(test[\"sentence\"].values, test[\"category\"].values)\n",
    "\n",
    "train_dataset = to_map_style_dataset(train_dataset)\n",
    "val_dataset = to_map_style_dataset(val_dataset)\n",
    "test_dataset = to_map_style_dataset(test_dataset)\n",
    "\n",
    "tokenizer = Tokenizer(vocab)\n",
    "\n",
    "\n",
    "def vectorize_batch(batch):\n",
    "    x, y = list(zip(*batch))\n",
    "\n",
    "    x = tokenizer.encode_batch(x)\n",
    "    x = x.to(DEVICE)\n",
    "\n",
    "    y = torch.tensor(y, device=DEVICE)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=64,\n",
    "    collate_fn=vectorize_batch,\n",
    "    # shuffle=True,\n",
    "    sampler=train_sampler,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=64,\n",
    "    collate_fn=vectorize_batch,\n",
    "    # sampler=val_sampler,\n",
    "    # shuffle=True,\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=64,\n",
    "    collate_fn=vectorize_batch,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save('tokenizer.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.load('tokenizer.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(train_loader))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%export aymurai.models.decision.torch.conv1d\n",
    "\n",
    "# import pytorch_lightning as pl\n",
    "# import torch.nn.functional as F\n",
    "# import torchmetrics\n",
    "# from torch import nn\n",
    "# import torch\n",
    "\n",
    "\n",
    "# class Conv1dTextClassifier(pl.LightningModule):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         vocab_size: int,\n",
    "#         embed_len: int = 64,\n",
    "#         nfeatures: int = 64,\n",
    "#         num_classes: int = 3,\n",
    "#     ):\n",
    "#         self.vocab_size = vocab_size\n",
    "#         self.embed_len = embed_len\n",
    "#         self.nfeatures = nfeatures\n",
    "#         self.num_classes = num_classes\n",
    "#         self.lr = 1e-3\n",
    "\n",
    "#         super().__init__()\n",
    "#         self.save_hyperparameters()\n",
    "\n",
    "#         # layers\n",
    "#         self.embedding_layer = nn.Embedding(\n",
    "#             num_embeddings=self.vocab_size,\n",
    "#             embedding_dim=self.embed_len,\n",
    "#         )\n",
    "#         self.conv1 = nn.Conv1d(self.embed_len, self.nfeatures, kernel_size=7, padding=\"same\")\n",
    "#         self.linear1 = nn.Linear(self.nfeatures, 32)\n",
    "#         self.linear2 = nn.Linear(32, self.num_classes)\n",
    "#         # self.linear = nn.Linear(self.nfeatures, self.num_classes)\n",
    "\n",
    "#         self.class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
    "#         self.loss = nn.CrossEntropyLoss(weight=self.class_weights)\n",
    "\n",
    "#         # metrics\n",
    "#         self.accuracy = torchmetrics.Accuracy(\n",
    "#             task=\"multiclass\",\n",
    "#             num_classes=self.num_classes,\n",
    "#         )\n",
    "#         self.f1score = torchmetrics.F1Score(\n",
    "#             task=\"multiclass\",\n",
    "#             num_classes=self.num_classes,\n",
    "#         )\n",
    "\n",
    "#     def forward(self, X_batch):\n",
    "#         x = self.embedding_layer(X_batch)\n",
    "#         x = x.reshape(len(x), self.embed_len, 128) ## Embedding Length needs to be treated as channel dimension\n",
    "#         x = F.relu(self.conv1(x))\n",
    "#         x, _ = x.max(dim=-1)\n",
    "\n",
    "#         x = self.linear1(x)\n",
    "#         x = self.linear2(x)\n",
    "#         # x = self.linear(x)\n",
    "#         # x = F.linear(x, torch.tensor([self.nfeatures, 32]))\n",
    "#         # x = F.linear(x, torch.tensor([32, self.num_classes]))\n",
    "#         y_hat = F.log_softmax(x)\n",
    "\n",
    "#         return y_hat\n",
    "\n",
    "#     def training_step(self, batch, batch_idx):\n",
    "#         # training_step defines the train loop.\n",
    "#         x, y = batch\n",
    "\n",
    "#         y_pred = self.forward(x)\n",
    "\n",
    "#         # loss = F.cross_entropy(y_pred, y, weight=self.class_weights)\n",
    "#         loss = self.loss(y_pred, y)\n",
    "#         acc = self.accuracy(y_pred, y)\n",
    "#         f1score = self.f1score(y_pred, y)\n",
    "\n",
    "#         self.log(\"loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "#         self.log(\"acc\", acc, on_epoch=True, prog_bar=True, logger=True)\n",
    "#         self.log(\"f1score\", f1score, on_epoch=True, prog_bar=True, logger=True)\n",
    "#         return loss\n",
    "\n",
    "#     def validation_step(self, batch, batch_idx):\n",
    "#         x, y = batch\n",
    "\n",
    "#         y_pred = self.forward(x)\n",
    "\n",
    "#         # loss = F.cross_entropy(y_pred, y)\n",
    "#         loss = self.loss(y_pred, y)\n",
    "#         acc = self.accuracy(y_pred, y)\n",
    "#         f1score = self.f1score(y_pred, y)\n",
    "\n",
    "#         self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "#         self.log(\"val_acc\", acc, on_epoch=True, prog_bar=True, logger=True)\n",
    "#         self.log(\"val_f1score\", f1score, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "#     def test_step(self, batch, batch_idx):\n",
    "#         x, y = batch\n",
    "\n",
    "#         y_pred = self.forward(x)\n",
    "\n",
    "#         # loss = F.cross_entropy(y_pred, y)\n",
    "#         loss = self.loss(y_pred, y)\n",
    "#         acc = self.accuracy(y_pred, y)\n",
    "#         f1score = self.f1score(y_pred, y)\n",
    "\n",
    "#         self.log(\"test_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "#         self.log(\"test_acc\", acc, on_epoch=True, prog_bar=True, logger=True)\n",
    "#         self.log(\"test_f1score\", f1score, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "#     def configure_optimizers(self):\n",
    "#         # optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "#         optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr)\n",
    "#         return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%export aymurai.models.decision.torch.conv1d\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "class Conv1dTextClassifier(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        embed_len: int = 64,\n",
    "        nfeatures: int = 64,\n",
    "        num_classes: int = 3,\n",
    "        lr_scheduler_patience: int = 2,\n",
    "    ):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_len = embed_len\n",
    "        self.nfeatures = nfeatures\n",
    "        self.num_classes = num_classes\n",
    "        self.lr = 1e-3\n",
    "        self.lr_scheduler_patience = lr_scheduler_patience\n",
    "\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # layers\n",
    "        self.embedding_layer = nn.Embedding(\n",
    "            num_embeddings=self.vocab_size,\n",
    "            embedding_dim=self.embed_len,\n",
    "        )\n",
    "        self.conv1 = nn.Conv1d(self.embed_len, 64, kernel_size=7, padding=\"same\")\n",
    "        self.conv2 = nn.Conv1d(64, 32, kernel_size=7, padding=\"same\")\n",
    "        self.pooling = nn.MaxPool1d(2)\n",
    "\n",
    "        self.linear1 = nn.Linear(32, 32)\n",
    "        self.linear2 = nn.Linear(32, self.num_classes)\n",
    "        # self.linear = nn.Linear(self.nfeatures, self.num_classes)\n",
    "\n",
    "        self.class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
    "        # self.loss = nn.CrossEntropyLoss(weight=self.class_weights)\n",
    "\n",
    "\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "        self.loss = nn.NLLLoss(weight=self.class_weights)\n",
    "\n",
    "        # metrics\n",
    "        self.accuracy = torchmetrics.Accuracy(\n",
    "            task=\"multiclass\",\n",
    "            num_classes=self.num_classes,\n",
    "        )\n",
    "        self.f1score = torchmetrics.F1Score(\n",
    "            task=\"multiclass\",\n",
    "            num_classes=self.num_classes,\n",
    "        )\n",
    "\n",
    "    def forward(self, X_batch):\n",
    "        x = self.embedding_layer(X_batch)\n",
    "        x = x.reshape(\n",
    "            len(x), self.embed_len, 128\n",
    "        )  ## Embedding Length needs to be treated as channel dimension\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pooling(x)\n",
    "        x = F.dropout(x, 0.5)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x, _ = x.max(dim=-1)\n",
    "\n",
    "        # x = self.linear1(x)\n",
    "        x = self.linear2(x)\n",
    "        # x = self.linear(x)\n",
    "        # x = F.linear(x, torch.tensor([self.nfeatures, 32]))\n",
    "        # x = F.linear(x, torch.tensor([32, self.num_classes]))\n",
    "        x = self.logsoftmax(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        x, y = batch\n",
    "\n",
    "        y_pred = self.forward(x)\n",
    "\n",
    "        # loss = F.cross_entropy(y_pred, y, weight=self.class_weights)\n",
    "        loss = self.loss(y_pred, y)\n",
    "        acc = self.accuracy(y_pred, y)\n",
    "        f1score = self.f1score(y_pred, y)\n",
    "\n",
    "        self.log(\"loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"acc\", acc, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"f1score\", f1score, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "\n",
    "        y_pred = self.forward(x)\n",
    "\n",
    "        # loss = F.cross_entropy(y_pred, y)\n",
    "        loss = self.loss(y_pred, y)\n",
    "        acc = self.accuracy(y_pred, y)\n",
    "        f1score = self.f1score(y_pred, y)\n",
    "\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_acc\", acc, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_f1score\", f1score, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "\n",
    "        y_pred = self.forward(x)\n",
    "\n",
    "        # loss = F.cross_entropy(y_pred, y)\n",
    "        loss = self.loss(y_pred, y)\n",
    "        acc = self.accuracy(y_pred, y)\n",
    "        f1score = self.f1score(y_pred, y)\n",
    "\n",
    "        self.log(\"test_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"test_acc\", acc, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"test_f1score\", f1score, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr)\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": ReduceLROnPlateau(\n",
    "                    optimizer,\n",
    "                    patience=self.lr_scheduler_patience,\n",
    "                ),\n",
    "                \"monitor\": \"val_loss\",\n",
    "                \"frequency\": 1\n",
    "                # If \"monitor\" references validation metrics, then \"frequency\" should be set to a\n",
    "                # multiple of \"trainer.check_val_every_n_epoch\".\n",
    "            },\n",
    "        }\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from aymurai.models.decision.torch.conv1d import Conv1dTextClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Conv1dTextClassifier(vocab_size=len(vocab))\n",
    "model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    x, y = batch\n",
    "    \n",
    "    print(x.shape)\n",
    "    b = model.forward(x)\n",
    "    print(b)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltmodel = Conv1dTextClassifier(vocab_size=len(vocab), num_classes=len(np.unique(train['category'])))\n",
    "ltmodel = ltmodel.to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import (\n",
    "    EarlyStopping,\n",
    "    RichProgressBar,\n",
    "    RichModelSummary,\n",
    "    LearningRateFinder,\n",
    "    LearningRateMonitor,\n",
    "    StochasticWeightAveraging,\n",
    "    ModelCheckpoint,\n",
    ")\n",
    "\n",
    "\n",
    "CHECKPOINT_PATH = \"checkpoints/pl-emb-conv/\"\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    save_top_k=1,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    dirpath=CHECKPOINT_PATH,\n",
    "    # filename=\"{epoch}-{val_loss:.2f}-{other_metric:.2f}\",\n",
    ")\n",
    "\n",
    "# train model\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=DEVICE,\n",
    "    devices=1,\n",
    "    callbacks=[\n",
    "        EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            mode=\"min\",\n",
    "            min_delta=0.00,\n",
    "            patience=10,\n",
    "            verbose=False,\n",
    "        ),\n",
    "        checkpoint_callback,\n",
    "        # StochasticWeightAveraging(swa_lrs=1e-2),\n",
    "        LearningRateFinder(),\n",
    "        LearningRateMonitor(),\n",
    "        RichModelSummary(),\n",
    "        # RichProgressBar(),\n",
    "    ],\n",
    "    max_epochs=50,\n",
    "    min_epochs=5,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer.fit(\n",
    "    model=ltmodel,\n",
    "    train_dataloaders=train_loader,\n",
    "    val_dataloaders=val_loader,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(checkpoint_callback.best_model_path)  # prints path to the best model's checkpoint\n",
    "print(checkpoint_callback.best_model_score)  # and prints it score\n",
    "path = checkpoint_callback.best_model_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.seed_everything(42)\n",
    "# path = '/workspace/notebooks/experiments/decision/test/conv/model.ckpt'\n",
    "# path = '/workspace/notebooks/experiments/decision/checkpoints/pl-emb-conv/epoch=38-step=6981.ckpt'\n",
    "# best_model = ltmodel.load_from_checkpoint(path, map_location='cpu')\n",
    "best_model = ltmodel.eval()\n",
    "best_model.eval()\n",
    "\n",
    "trainer.test(ltmodel, dataloaders=test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = x[:1]\n",
    "xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    a = best_model(xx).exp().argmax(axis=1)\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print('TRAIN')\n",
    "\n",
    "x = tokenizer.encode_batch(train['sentence'].iloc[:])\n",
    "hypothesis = best_model(x).argmax(axis=1)\n",
    "reference = train['category']\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "\n",
    "confusion =  confusion_matrix(reference, hypothesis)\n",
    "print(confusion)\n",
    "sns.heatmap(confusion, annot=True, fmt='d', ax=ax)\n",
    "ax.set_xlabel(\"hypothesis\")\n",
    "ax.set_ylabel(\"reference\")\n",
    "# ax.set_xticklabels([\"None\", \"desicion/no_hace_lugar\", \"descion/hace_lugar\"])\n",
    "# ax.set_yticklabels([\"None\", \"desicion/no_hace_lugar\", \"descion/hace_lugar\"])\n",
    "ax.set_title('TRAIN')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "report = classification_report(reference, hypothesis, output_dict=True)\n",
    "pd.DataFrame(report).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print('VAL')\n",
    "\n",
    "x = tokenizer.encode_batch(val['sentence'].iloc[:])\n",
    "hypothesis = best_model(x).argmax(axis=1)\n",
    "reference = val['category']\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "\n",
    "confusion =  confusion_matrix(reference, hypothesis)\n",
    "print(confusion)\n",
    "sns.heatmap(confusion, annot=True, fmt='d', ax=ax)\n",
    "ax.set_xlabel(\"hypothesis\")\n",
    "ax.set_ylabel(\"reference\")\n",
    "# ax.set_xticklabels([\"None\", \"desicion/no_hace_lugar\", \"descion/hace_lugar\"])\n",
    "# ax.set_yticklabels([\"None\", \"desicion/no_hace_lugar\", \"descion/hace_lugar\"])\n",
    "ax.set_title('VAL')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "report = classification_report(reference, hypothesis, output_dict=True)\n",
    "pd.DataFrame(report).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print('TEST')\n",
    "\n",
    "x = tokenizer.encode_batch(test['sentence'].iloc[:])\n",
    "hypothesis = best_model(x).argmax(axis=1)\n",
    "reference = test['category']\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "\n",
    "confusion =  confusion_matrix(reference, hypothesis)\n",
    "print(confusion)\n",
    "sns.heatmap(confusion, annot=True, fmt='d', ax=ax)\n",
    "ax.set_xlabel(\"hypothesis\")\n",
    "ax.set_ylabel(\"reference\")\n",
    "# ax.set_xticklabels([\"None\", \"desicion/no_hace_lugar\", \"descion/hace_lugar\"])\n",
    "# ax.set_yticklabels([\"None\", \"desicion/no_hace_lugar\", \"descion/hace_lugar\"])\n",
    "ax.set_title('TEST')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "report = classification_report(reference, hypothesis, output_dict=True)\n",
    "pd.DataFrame(report).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ = test.copy()\n",
    "test_[\"pred_cat\"] = hypothesis\n",
    "\n",
    "def cat2label(cat):\n",
    "    if cat == 0:\n",
    "        return (False, False)\n",
    "    if cat == 1:\n",
    "        return (True, False)\n",
    "    if cat == 2:\n",
    "        return (True, True)\n",
    "\n",
    "test_[['pred_decision', 'pred_hace_lugar']] = [cat2label(d) for d in test_['pred_cat']]\n",
    "test_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\n",
    "    \"display.max_columns\",\n",
    "    1000,\n",
    "    \"display.width\",\n",
    "    1000,\n",
    "    \"display.max_colwidth\",\n",
    "    None,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_['pred_ok'] = test_['category'] == test_['pred_cat']\n",
    "# test_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_.query('decision == 0 and pred_decision and not pred_hace_lugar').sample(1)\n",
    "test_.query('pred_ok == 1 and decision')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Oct 12 2022, 19:14:26) [GCC 9.4.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
