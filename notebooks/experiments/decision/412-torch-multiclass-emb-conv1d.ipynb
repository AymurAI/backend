{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%load_ext aymurai.devtools.magic\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class DFtoDataset(Dataset):\n",
    "    def __init__(self, texts: list[str], targets: list[int]):\n",
    "\n",
    "        self.x_ = texts.values\n",
    "        self.y_ = targets.values\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y_)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Note y and x are inverted to mimic AR_NEWS dataset format\n",
    "        return self.y_[idx], self.x_[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\n",
    "    \"sentences-decision-manual.csv\",\n",
    "    usecols=[\"path\", \"nro_registro\", \"tomo\", \"sentence\", \"decision\", \"hace_lugar\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)\n",
    "# target_classes = [\"none\", \"decision:no_hace_lugar\", \"decision:hace_lugar\"]\n",
    "\n",
    "\n",
    "def force_bool(value):\n",
    "    return True if value in ['True', True, 1, \"1\"] else False\n",
    "\n",
    "\n",
    "def get_category(pair):\n",
    "    decision, hace_lugar = pair\n",
    "    # print(decision, hace_lugar, type(decision), type(hace_lugar))\n",
    "    if not decision:\n",
    "        cat = 0\n",
    "    elif decision and not hace_lugar:\n",
    "        cat = 1\n",
    "    elif decision and hace_lugar:\n",
    "        cat = 2\n",
    "    else:\n",
    "        raise \"not valid\"\n",
    "    return cat\n",
    "\n",
    "\n",
    "# # data[['decision', 'hace_lugar']] = data[['decision', 'hace_lugar']].apply(lambda x: literal_eval(x), axis=1).astype(bool) \n",
    "data['decision'] = data['decision'].apply(force_bool).astype(bool) \n",
    "data['hace_lugar'] = data['hace_lugar'].apply(force_bool).astype(bool) \n",
    "data[\"category\"] = data[[\"decision\", \"hace_lugar\"]].apply(get_category, axis=1)\n",
    "data.dropna(subset=['category'], inplace=True)\n",
    "\n",
    "data.drop_duplicates(subset=\"sentence\", inplace=True)\n",
    "print(len(data))\n",
    "data[\"sentence\"].apply(lambda x: len(x.split(\" \"))).hist(\n",
    "    bins=[32 * i for i in range(10)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset = DFtoDataset(data['sentence'], data['category'])\n",
    "\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "test, val = train_test_split(test, test_size=0.5, random_state=42)\n",
    "\n",
    "train = DFtoDataset(train['sentence'], train['category'])\n",
    "val = DFtoDataset(val['sentence'], val['category'])\n",
    "test = DFtoDataset(test['sentence'], test['category'])\n",
    "\n",
    "\n",
    "print(len(train))\n",
    "print(len(val))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "DEVICE = 'cuda'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo python -m spacy download es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[59]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%export aymurai.models.decision.torch.tokenizer\n",
    "\n",
    "import torch\n",
    "from torchtext.data import get_tokenizer\n",
    "\n",
    "\n",
    "class Tokenizer(object):\n",
    "    def __init__(self, vocab):\n",
    "        self.max_len = 128\n",
    "        self.tokenizer = get_tokenizer(\"spacy\", \"es_core_news_sm\")\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def save(self, path: str):\n",
    "        torch.save(self.vocab, path)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path: str):\n",
    "        vocab = torch.load(path)\n",
    "        return cls(vocab=vocab)\n",
    "\n",
    "    def __call__(self, text: str):\n",
    "        return self.tokenizer(text)\n",
    "\n",
    "    def encode(self, text: int):\n",
    "        tokens = self(text)[: self.max_len]\n",
    "        indices = self.vocab(tokens)\n",
    "        indices = torch.tensor(indices, dtype=torch.int64)\n",
    "        indices = torch.nn.functional.pad(indices, (0, self.max_len - len(indices)))\n",
    "        return indices\n",
    "\n",
    "    def encode_batch(self, texts):\n",
    "        indices = [self.encode(text) for text in texts]\n",
    "        indices = torch.stack(indices)\n",
    "        return indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "# tokenizer = get_tokenizer('spacy', \"es\")\n",
    "tokenizer = Tokenizer(vocab=None)\n",
    "\n",
    "def build_vocabulary(datasets):\n",
    "    for dataset in datasets:\n",
    "        for _, text in dataset:\n",
    "            # print(text)\n",
    "            yield tokenizer(text)\n",
    "\n",
    "# vocab = build_vocab_from_iterator(build_vocabulary([dataset]), min_freq=1, specials=[\"<UNK>\",])\n",
    "vocab = build_vocab_from_iterator(build_vocabulary([train]), min_freq=1, specials=[\"<UNK>\",])\n",
    "vocab.set_default_index(vocab[\"<UNK>\"])\n",
    "\n",
    "tokenizer.vocab = vocab\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = Tokenizer(vocab)\n",
    "\n",
    "max_tokens = 128\n",
    "\n",
    "def vectorize_text(batch):\n",
    "    Y, X = list(zip(*batch))\n",
    "    # X = [vocab(tokenizer(text)) for text in X]\n",
    "    # X = [tokens+([0]* (max_tokens-len(tokens))) if len(tokens)<max_tokens else tokens[:max_tokens] for tokens in X] ## Bringing all samples to max_tokens length.\n",
    "    X = tokenizer.encode_batch(X)\n",
    "\n",
    "    xx, yy = torch.tensor(X, dtype=torch.int32), torch.tensor(Y) ## We have deducted 1 from target names to get them in range [0,1,2,3] from [1,2,3,4]\n",
    "    xx = xx.to(DEVICE)\n",
    "    yy = yy.to(DEVICE)\n",
    "    return xx, yy\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=1024, collate_fn=vectorize_text, shuffle=True)\n",
    "val_loader  = DataLoader(val,  batch_size=1024, collate_fn=vectorize_text)\n",
    "test_loader  = DataLoader(test,  batch_size=1024, collate_fn=vectorize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_text([[1, 'En función de tales motivos, dispondré la']])[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for X, Y in train_loader:\n",
    "    print(X.shape, Y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = [data[0] for data in train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    \"balanced\", classes=np.unique(cats), y=cats\n",
    ")\n",
    "# class_weights = {k: v for k, v in enumerate(class_weights)}\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "\n",
    "counts = np.bincount(cats)\n",
    "labels_weights = 1. / counts\n",
    "weights = labels_weights[cats]\n",
    "train_sampler = WeightedRandomSampler(weights, len(weights), replacement=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%export aymurai.models.decision.torch.conv1d\n",
    "\n",
    "import torch\n",
    "import torchmetrics\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "class Conv1dTextClassifier(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        max_tokens: int = 50,\n",
    "        embed_len: int = 64,\n",
    "        nfeatures: int = 64,\n",
    "        num_classes: int = 4,\n",
    "        lr_scheduler_patience: int = 2,\n",
    "        class_weights: list = []\n",
    "    ):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_tokens = max_tokens\n",
    "        self.embed_len = embed_len\n",
    "        self.nfeatures = nfeatures\n",
    "        self.num_classes = num_classes\n",
    "        self.lr = 1e-3\n",
    "        self.lr_scheduler_patience = lr_scheduler_patience\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # layers\n",
    "        self.embedding_layer = nn.Embedding(\n",
    "            num_embeddings=self.vocab_size,\n",
    "            embedding_dim=self.embed_len,\n",
    "        )\n",
    "        self.conv1 = nn.Conv1d(self.embed_len, 64, kernel_size=7, padding=\"same\")\n",
    "        self.conv2 = nn.Conv1d(64, 32, kernel_size=7, padding=\"same\")\n",
    "        self.pooling = nn.MaxPool1d(2)\n",
    "\n",
    "        self.linear1 = nn.Linear(32, 32)\n",
    "        self.linear2 = nn.Linear(32, self.num_classes)\n",
    "        # self.linear = nn.Linear(self.nfeatures, self.num_classes)\n",
    "\n",
    "        if len(self.class_weights):\n",
    "            self.class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
    "        # self.loss = nn.CrossEntropyLoss(weight=self.class_weights)\n",
    "\n",
    "\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "        self.loss = nn.NLLLoss(weight=self.class_weights)\n",
    "\n",
    "        # metrics\n",
    "        self.accuracy = torchmetrics.Accuracy(\n",
    "            task=\"multiclass\",\n",
    "            num_classes=self.num_classes,\n",
    "        )\n",
    "        self.f1score = torchmetrics.F1Score(\n",
    "            task=\"multiclass\",\n",
    "            num_classes=self.num_classes,\n",
    "        )\n",
    "\n",
    "    def forward(self, X_batch):\n",
    "        x = self.embedding_layer(X_batch)\n",
    "        x = x.reshape(\n",
    "            len(x), self.embed_len, self.max_tokens\n",
    "        )  ## Embedding Length needs to be treated as channel dimension\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pooling(x)\n",
    "        # x = F.dropout(x, 0.5)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x, _ = x.max(dim=-1)\n",
    "\n",
    "        # x = self.linear1(x)\n",
    "        x = self.linear2(x)\n",
    "        # x = self.linear(x)\n",
    "        # x = F.linear(x, torch.tensor([self.nfeatures, 32]))\n",
    "        # x = F.linear(x, torch.tensor([32, self.num_classes]))\n",
    "        x = self.logsoftmax(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        return self(batch)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        x, y = batch\n",
    "\n",
    "        y_pred = self.forward(x)\n",
    "\n",
    "        # loss = F.cross_entropy(y_pred, y, weight=self.class_weights)\n",
    "        loss = self.loss(y_pred, y)\n",
    "        acc = self.accuracy(y_pred, y)\n",
    "        f1score = self.f1score(y_pred, y)\n",
    "\n",
    "        self.log(\"loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"acc\", acc, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"f1score\", f1score, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "\n",
    "        y_pred = self.forward(x)\n",
    "\n",
    "        # loss = F.cross_entropy(y_pred, y)\n",
    "        loss = self.loss(y_pred, y)\n",
    "        acc = self.accuracy(y_pred, y)\n",
    "        f1score = self.f1score(y_pred, y)\n",
    "\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_acc\", acc, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_f1score\", f1score, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "\n",
    "        y_pred = self.forward(x)\n",
    "\n",
    "        # loss = F.cross_entropy(y_pred, y)\n",
    "        loss = self.loss(y_pred, y)\n",
    "        acc = self.accuracy(y_pred, y)\n",
    "        f1score = self.f1score(y_pred, y)\n",
    "\n",
    "        self.log(\"test_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"test_acc\", acc, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"test_f1score\", f1score, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr)\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": ReduceLROnPlateau(\n",
    "                    optimizer,\n",
    "                    patience=self.lr_scheduler_patience,\n",
    "                ),\n",
    "                \"monitor\": \"val_loss\",\n",
    "                \"frequency\": 1\n",
    "                # If \"monitor\" references validation metrics, then \"frequency\" should be set to a\n",
    "                # multiple of \"trainer.check_val_every_n_epoch\".\n",
    "            },\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aymurai.models.decision.torch.conv1d import Conv1dTextClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Conv1dTextClassifier(vocab_size=len(vocab), embed_len=128, num_classes=3, max_tokens=max_tokens, class_weights=class_weights)\n",
    "model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    x, y = batch\n",
    "    # x = x.to('cuda')\n",
    "    \n",
    "    print(x.shape)\n",
    "    b = model.forward(x)\n",
    "    print(b)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltmodel = model.to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import (\n",
    "    EarlyStopping,\n",
    "    RichProgressBar,\n",
    "    RichModelSummary,\n",
    "    LearningRateFinder,\n",
    "    LearningRateMonitor,\n",
    "    StochasticWeightAveraging,\n",
    "    ModelCheckpoint,\n",
    ")\n",
    "\n",
    "\n",
    "CHECKPOINT_PATH = \"checkpoints/multiclass-emb-conv/\"\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    save_top_k=1,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    dirpath=CHECKPOINT_PATH,\n",
    "    # filename=\"{epoch}-{val_loss:.2f}-{other_metric:.2f}\",\n",
    ")\n",
    "\n",
    "# train model\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=DEVICE,\n",
    "    devices=1,\n",
    "    callbacks=[\n",
    "        EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            mode=\"min\",\n",
    "            min_delta=0.00,\n",
    "            patience=10,\n",
    "            verbose=False,\n",
    "        ),\n",
    "        checkpoint_callback,\n",
    "        # StochasticWeightAveraging(swa_lrs=1e-2),\n",
    "        LearningRateFinder(),\n",
    "        LearningRateMonitor(),\n",
    "        RichModelSummary(),\n",
    "        # RichProgressBar(),\n",
    "    ],\n",
    "    max_epochs=50,\n",
    "    min_epochs=5,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer.fit(\n",
    "    model=ltmodel,\n",
    "    train_dataloaders=train_loader,\n",
    "    val_dataloaders=val_loader,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(checkpoint_callback.best_model_path)  # prints path to the best model's checkpoint\n",
    "print(checkpoint_callback.best_model_score)  # and prints it score\n",
    "path = checkpoint_callback.best_model_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pl.seed_everything(42)\n",
    "# path = '/workspace/notebooks/experiments/decision/test/conv/model.ckpt'\n",
    "# path = '/workspace/notebooks/experiments/decision/checkpoints/pl-emb-conv/epoch=38-step=6981.ckpt'\n",
    "# path = \"/workspace/notebooks/experiments/decision/checkpoints/pl-emb-conv/epoch=23-step=624-v1.ckpt\"\n",
    "# best_model = ltmodel.load_from_checkpoint(path, map_location='cpu')\n",
    "tokenizer.save('tokenizer.pth')\n",
    "best_model = ltmodel.eval()\n",
    "best_model.eval()\n",
    "\n",
    "trainer.test(ltmodel, dataloaders=test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print('TRAIN')\n",
    "\n",
    "reference = []\n",
    "hypothesis = []\n",
    "\n",
    "ltmodel = ltmodel.to('cuda')\n",
    "for batch in train_loader:\n",
    "    x, y = batch\n",
    "    x = x.to('cuda')\n",
    "\n",
    "    y_pred = ltmodel(x).exp().argmax(axis=1)\n",
    "\n",
    "    hypothesis.append(y_pred.cpu())\n",
    "    reference.append(y.cpu())\n",
    "\n",
    "reference = np.concatenate(reference)\n",
    "hypothesis = np.concatenate(hypothesis)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "\n",
    "confusion =  confusion_matrix(reference, hypothesis)\n",
    "print(confusion)\n",
    "sns.heatmap(confusion, annot=True, fmt='d', ax=ax)\n",
    "ax.set_xlabel(\"hypothesis\")\n",
    "ax.set_ylabel(\"reference\")\n",
    "# ax.set_xticklabels([\"None\", \"desicion/no_hace_lugar\", \"descion/hace_lugar\"])\n",
    "# ax.set_yticklabels([\"None\", \"desicion/no_hace_lugar\", \"descion/hace_lugar\"])\n",
    "ax.set_title('TRAIN')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "report = classification_report(reference, hypothesis, output_dict=True)\n",
    "pd.DataFrame(report).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print('VAL')\n",
    "\n",
    "reference = []\n",
    "hypothesis = []\n",
    "\n",
    "ltmodel = ltmodel.to('cuda')\n",
    "for batch in val_loader:\n",
    "    x, y = batch\n",
    "    x = x.to('cuda')\n",
    "\n",
    "    y_pred = ltmodel(x).exp().argmax(axis=1)\n",
    "\n",
    "    hypothesis.append(y_pred.cpu())\n",
    "    reference.append(y.cpu())\n",
    "\n",
    "reference = np.concatenate(reference)\n",
    "hypothesis = np.concatenate(hypothesis)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "\n",
    "confusion =  confusion_matrix(reference, hypothesis)\n",
    "print(confusion)\n",
    "sns.heatmap(confusion, annot=True, fmt='d', ax=ax)\n",
    "ax.set_xlabel(\"hypothesis\")\n",
    "ax.set_ylabel(\"reference\")\n",
    "# ax.set_xticklabels([\"None\", \"desicion/no_hace_lugar\", \"descion/hace_lugar\"])\n",
    "# ax.set_yticklabels([\"None\", \"desicion/no_hace_lugar\", \"descion/hace_lugar\"])\n",
    "ax.set_title('VAL')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "report = classification_report(reference, hypothesis, output_dict=True)\n",
    "pd.DataFrame(report).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print('TEST')\n",
    "\n",
    "reference = []\n",
    "hypothesis = []\n",
    "ltmodel = ltmodel.to('cuda')\n",
    "for batch in test_loader:\n",
    "    x, y = batch\n",
    "    x = x.to('cuda')\n",
    "\n",
    "    y_pred = ltmodel(x).exp().argmax(axis=1)\n",
    "\n",
    "    hypothesis.append(y_pred.cpu())\n",
    "    reference.append(y.cpu())\n",
    "\n",
    "reference = np.concatenate(reference)\n",
    "hypothesis = np.concatenate(hypothesis)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "\n",
    "confusion =  confusion_matrix(reference, hypothesis)\n",
    "print(confusion)\n",
    "sns.heatmap(confusion, annot=True, fmt='d', ax=ax)\n",
    "ax.set_xlabel(\"hypothesis\")\n",
    "ax.set_ylabel(\"reference\")\n",
    "# ax.set_xticklabels([\"None\", \"desicion/no_hace_lugar\", \"descion/hace_lugar\"])\n",
    "# ax.set_yticklabels([\"None\", \"desicion/no_hace_lugar\", \"descion/hace_lugar\"])\n",
    "ax.set_title('TEST')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "report = classification_report(reference, hypothesis, output_dict=True)\n",
    "pd.DataFrame(report).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sentences = [pair[1] for pair in test]\n",
    "\n",
    "\n",
    "def cat2label(cat):\n",
    "    if cat == 0:\n",
    "        return (False, False)\n",
    "    if cat == 1:\n",
    "        return (True, False)\n",
    "    if cat == 2:\n",
    "        return (True, True)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"sentence\": sentences,\n",
    "        \"cat\": reference,\n",
    "        \"pred_cat\": hypothesis,\n",
    "    }\n",
    ")\n",
    "\n",
    "# df[[\"pred_decision\", \"pred_hace_lugar\"]] = [cat2label(d) for d in df[\"pred_cat\"]]\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\n",
    "    \"display.max_columns\",\n",
    "    1000,\n",
    "    \"display.width\",\n",
    "    1000,\n",
    "    \"display.max_colwidth\",\n",
    "    None,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pred_ok'] = df['cat'] == df['pred_cat']\n",
    "# test_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_.query('decision == 0 and pred_decision and not pred_hace_lugar').sample(1)\n",
    "df.query('pred_ok == 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Oct 12 2022, 19:14:26) [GCC 9.4.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
