{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%load_ext aymurai.devtools.magic\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sudo python -m spacy download es"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "This notebook is used to train the decision model. The model is a two classifier that predicts if a paragraph is a decision or not. \n",
    "The model schema follows:\n",
    "1. Embedding layer build from custom vocabulary from training data.\n",
    "2. Convolutional layer with 1D kernel.\n",
    "3. Max pooling layer.\n",
    "4. Fully connected layer.\n",
    "5. Softmax layer.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\n",
    "    \"sentences-decision-manual.csv\",\n",
    "    usecols=[\"path\", \"nro_registro\", \"tomo\", \"sentence\", \"decision\", \"hace_lugar\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class DFtoDataset(Dataset):\n",
    "    def __init__(self, texts: list[str], targets: list[int]):\n",
    "\n",
    "        self.x_ = texts.values\n",
    "        self.y_ = targets.values\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y_)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Note y and x are inverted to mimic AR_NEWS dataset format\n",
    "        return self.y_[idx], self.x_[idx]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format data\n",
    "The data is formatted in the following way:\n",
    "- The first column is the label (0 or 1)\n",
    "- The second column is the text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)\n",
    "# target_classes = [\"none\", \"decision:no_hace_lugar\", \"decision:hace_lugar\"]\n",
    "\n",
    "\n",
    "def force_bool(value):\n",
    "    return True if value in ['True', True, 1, \"1\"] else False\n",
    "\n",
    "\n",
    "def get_category(pair):\n",
    "    decision, hace_lugar = pair\n",
    "    # print(decision, hace_lugar, type(decision), type(hace_lugar))\n",
    "    if not decision:\n",
    "        cat = 0\n",
    "    elif decision and not hace_lugar:\n",
    "        cat = 1\n",
    "    elif decision and hace_lugar:\n",
    "        cat = 1\n",
    "    else:\n",
    "        raise \"not valid\"\n",
    "    return cat\n",
    "\n",
    "\n",
    "# # data[['decision', 'hace_lugar']] = data[['decision', 'hace_lugar']].apply(lambda x: literal_eval(x), axis=1).astype(bool) \n",
    "data['decision'] = data['decision'].apply(force_bool).astype(bool) \n",
    "data['hace_lugar'] = data['hace_lugar'].apply(force_bool).astype(bool) \n",
    "data[\"category\"] = data[[\"decision\", \"hace_lugar\"]].apply(get_category, axis=1)\n",
    "data.dropna(subset=['category'], inplace=True)\n",
    "\n",
    "data.drop_duplicates(subset=\"sentence\", inplace=True)\n",
    "print(len(data))\n",
    "data[\"sentence\"].apply(lambda x: len(x.split(\" \"))).hist(\n",
    "    bins=[32 * i for i in range(10)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset = DFtoDataset(data['sentence'], data['category'])\n",
    "\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "test, val = train_test_split(test, test_size=0.5, random_state=42)\n",
    "\n",
    "train_df = train.copy()\n",
    "val_df = val.copy()\n",
    "test_df = test.copy()\n",
    "\n",
    "train = DFtoDataset(train['sentence'], train['category'])\n",
    "val = DFtoDataset(val['sentence'], val['category'])\n",
    "test = DFtoDataset(test['sentence'], test['category'])\n",
    "\n",
    "\n",
    "print(len(train))\n",
    "print(len(val))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "DEVICE = 'cpu'\n",
    "\n",
    "\n",
    "# train_dataset, test_dataset = torchtext.datasets.AG_NEWS()\n",
    "\n",
    "# train_dataset = to_map_style_dataset(train_dataset)\n",
    "# test_dataset = to_map_style_dataset(test_dataset)\n",
    "# train_dataset, val_dataset = train_test_split( train_dataset, test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[59]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build vocabulary\n",
    "The vocabulary is built from the training data. The vocabulary is a dictionary that maps each word to an index. The index is used to represent the word in the embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from aymurai.models.decision.tokenizer import Tokenizer\n",
    "from unidecode import unidecode\n",
    "\n",
    "tokenizer = Tokenizer(vocab=None)\n",
    "\n",
    "def build_vocabulary(datasets):\n",
    "    for dataset in datasets:\n",
    "        for _, text in dataset:\n",
    "            # print(text)\n",
    "            text = text.lower()\n",
    "            text = unidecode(text)\n",
    "            yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(build_vocabulary([train]), min_freq=1, specials=[\"<UNK>\",])\n",
    "vocab.set_default_index(vocab[\"<UNK>\"])\n",
    "\n",
    "tokenizer.vocab = vocab\n",
    "\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_tokens = 128\n",
    "\n",
    "def vectorize_text(batch):\n",
    "    Y, X = list(zip(*batch))\n",
    "    # X = [vocab(tokenizer(text)) for text in X]\n",
    "    # X = [tokens+([0]* (max_tokens-len(tokens))) if len(tokens)<max_tokens else tokens[:max_tokens] for tokens in X] ## Bringing all samples to max_tokens length.\n",
    "    X = tokenizer.encode_batch(X)\n",
    "\n",
    "    xx, yy = torch.tensor(X, dtype=torch.int32), torch.tensor(Y) ## We have deducted 1 from target names to get them in range [0,1,2,3] from [1,2,3,4]\n",
    "    xx = xx.to(DEVICE)\n",
    "    yy = yy.to(DEVICE)\n",
    "    return xx, yy\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=1024, collate_fn=vectorize_text, shuffle=True)\n",
    "val_loader  = DataLoader(val,  batch_size=1024, collate_fn=vectorize_text)\n",
    "test_loader  = DataLoader(test,  batch_size=1024, collate_fn=vectorize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_text([[1, 'En función de tales motivos, dispondré la']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for X, Y in train_loader:\n",
    "    print(X.shape, Y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = [data[0] for data in train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    \"balanced\", classes=np.unique(cats), y=cats\n",
    ")\n",
    "# class_weights = {k: v for k, v in enumerate(class_weights)}\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "\n",
    "counts = np.bincount(cats)\n",
    "labels_weights = 1. / counts\n",
    "weights = labels_weights[cats]\n",
    "train_sampler = WeightedRandomSampler(weights, len(weights), replacement=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aymurai.models.decision.conv1d import Conv1dTextClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Conv1dTextClassifier(\n",
    "    vocab_size=len(vocab),\n",
    "    embed_len=128,\n",
    "    num_classes=2,\n",
    "    max_tokens=max_tokens,\n",
    "    class_weights=class_weights,\n",
    ")\n",
    "model = model.to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    x, y = batch\n",
    "    # x = x.to('cuda')\n",
    "    \n",
    "    print(x.shape)\n",
    "    b = model.forward(x)\n",
    "    print(b)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltmodel = model.to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import (\n",
    "    EarlyStopping,\n",
    "    RichProgressBar,\n",
    "    RichModelSummary,\n",
    "    LearningRateFinder,\n",
    "    LearningRateMonitor,\n",
    "    StochasticWeightAveraging,\n",
    "    ModelCheckpoint,\n",
    ")\n",
    "\n",
    "\n",
    "CHECKPOINT_PATH = \"checkpoints/413-torch-binary-emb-conv1d/\"\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    save_top_k=1,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    dirpath=CHECKPOINT_PATH,\n",
    "    # filename=\"{epoch}-{val_loss:.2f}-{other_metric:.2f}\",\n",
    ")\n",
    "\n",
    "# train model\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=DEVICE,\n",
    "    devices=1,\n",
    "    callbacks=[\n",
    "        EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            mode=\"min\",\n",
    "            min_delta=0.00,\n",
    "            patience=10,\n",
    "            verbose=False,\n",
    "        ),\n",
    "        checkpoint_callback,\n",
    "        # StochasticWeightAveraging(swa_lrs=1e-2),\n",
    "        LearningRateFinder(),\n",
    "        LearningRateMonitor(),\n",
    "        RichModelSummary(),\n",
    "        # RichProgressBar(),\n",
    "    ],\n",
    "    max_epochs=50,\n",
    "    min_epochs=5,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer.fit(\n",
    "    model=ltmodel,\n",
    "    train_dataloaders=train_loader,\n",
    "    val_dataloaders=val_loader,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(checkpoint_callback.best_model_path)  # prints path to the best model's checkpoint\n",
    "print(checkpoint_callback.best_model_score)  # and prints it score\n",
    "path = checkpoint_callback.best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_model = ltmodel.eval()\n",
    "best_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pl.seed_everything(42)\n",
    "# path = '/workspace/notebooks/experiments/decision/checkpoints/pl-emb-conv/epoch=8-step=234.ckpt'\n",
    "# path = '/resources/pipelines/production/full-pipeline/DecisionConv1dBinRegex/model.ckpt'\n",
    "# best_model = ltmodel.load_from_checkpoint(path, map_location='cpu')\n",
    "best_model = ltmodel.eval()\n",
    "best_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    trainer.test(ltmodel, dataloaders=test_loader)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ltmodel = model.eval()\n",
    "\n",
    "# path = '/resources/pipelines/production/full-pipeline/DecisionConv1dBinRegex/model.ckpt'\n",
    "# path = '/workspace/notebooks/experiments/decision/checkpoints/413-torch-binary-emb-conv1d/epoch=8-step=234.ckpt'\n",
    "# ltmodel = ltmodel.load_from_checkpoint(path, map_location='cpu')\n",
    "ltmodel.eval()\n",
    "\n",
    "tokenizer.save('tokenizer.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print('TRAIN')\n",
    "\n",
    "reference = []\n",
    "hypothesis = []\n",
    "probs = []\n",
    "ltmodel = ltmodel.to(DEVICE)\n",
    "for batch in train_loader:\n",
    "    x, y = batch\n",
    "    x = x.to(DEVICE)\n",
    "\n",
    "    y_pred = ltmodel(x)#.exp().argmax(axis=1)\n",
    "    \n",
    "\n",
    "    hypothesis.append(y_pred.cpu().detach().numpy())\n",
    "    reference.append(y.cpu().detach().numpy())\n",
    "\n",
    "\n",
    "reference = np.concatenate(reference)\n",
    "hypothesis = np.concatenate(hypothesis)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "\n",
    "confusion =  confusion_matrix(reference, hypothesis.argmax(axis=1))\n",
    "print(confusion)\n",
    "sns.heatmap(confusion, annot=True, fmt='d', ax=ax)\n",
    "ax.set_xlabel(\"hypothesis\")\n",
    "ax.set_ylabel(\"reference\")\n",
    "# ax.set_xticklabels([\"None\", \"desicion/no_hace_lugar\", \"descion/hace_lugar\"])\n",
    "# ax.set_yticklabels([\"None\", \"desicion/no_hace_lugar\", \"descion/hace_lugar\"])\n",
    "ax.set_title('TRAIN')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "report = classification_report(reference, hypothesis.argmax(axis=1), output_dict=True)\n",
    "pd.DataFrame(report).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(hypothesis)\n",
    "\n",
    "score_class0 = np.exp(hypothesis)[:, 0]\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        'score': 1- score_class0,\n",
    "        'true_class': reference\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, subplot = plt.subplots(1, 2, figsize=(14, 4), sharey=True)\n",
    "\n",
    "\n",
    "sns.histplot(df.query('true_class == 0'), x='score', color='r', alpha=0.8, label='class 0', ax=subplot[0], stat='probability')\n",
    "sns.histplot(df.query('true_class == 1'), x='score', color='b', alpha=0.5, label='class 1',ax=subplot[1], stat='probability')\n",
    "# sns.histplot(class1, color='b', alpha=0.5)\n",
    "subplot[0].set_yscale('log')\n",
    "subplot[1].set_yscale('log')\n",
    "# subplot.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {\n",
    "    'threshold': [],\n",
    "    'precision': [],\n",
    "    'recall': [],\n",
    "    'f1-score': []\n",
    "}\n",
    "\n",
    "y_true = df['true_class']\n",
    "for cutoff in np.linspace(0, 1, 100):\n",
    "    y_pred = df['score'] > cutoff\n",
    "    \n",
    "    scores['threshold'].append(cutoff)\n",
    "\n",
    "    report = classification_report(y_true, y_pred, output_dict=True)\n",
    "    # print(report['1'])\n",
    "    precision = report['1']['precision']\n",
    "    recall = report['1']['recall']\n",
    "    f1score = report['1']['f1-score']\n",
    "    scores['precision'].append(precision)\n",
    "    scores['recall'].append(recall)\n",
    "    scores['f1-score'].append(f1score)\n",
    "\n",
    "scores = pd.DataFrame(scores)\n",
    "scores = scores.melt(['threshold'], value_vars=['precision', 'recall', 'f1-score'])\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=scores, x='threshold', y='value', hue='variable')\n",
    "\n",
    "THRESHOLD = 0.90\n",
    "plt.axvline(THRESHOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print('VAL')\n",
    "\n",
    "reference = []\n",
    "hypothesis = []\n",
    "probs = []\n",
    "ltmodel = ltmodel.to(DEVICE)\n",
    "for batch in val_loader:\n",
    "    x, y = batch\n",
    "    x = x.to(DEVICE)\n",
    "\n",
    "    y_pred = ltmodel(x)#.exp().argmax(axis=1)\n",
    "    \n",
    "\n",
    "    hypothesis.append(y_pred.cpu().detach().numpy())\n",
    "    reference.append(y.cpu().detach().numpy())\n",
    "\n",
    "\n",
    "reference = np.concatenate(reference)\n",
    "hypothesis = np.concatenate(hypothesis)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "\n",
    "confusion =  confusion_matrix(reference, hypothesis.argmax(axis=1))\n",
    "print(confusion)\n",
    "sns.heatmap(confusion, annot=True, fmt='d', ax=ax)\n",
    "ax.set_xlabel(\"hypothesis\")\n",
    "ax.set_ylabel(\"reference\")\n",
    "# ax.set_xticklabels([\"None\", \"desicion/no_hace_lugar\", \"descion/hace_lugar\"])\n",
    "# ax.set_yticklabels([\"None\", \"desicion/no_hace_lugar\", \"descion/hace_lugar\"])\n",
    "ax.set_title('VAL')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "report = classification_report(reference, hypothesis.argmax(axis=1), output_dict=True)\n",
    "pd.DataFrame(report).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(hypothesis)\n",
    "\n",
    "score_class0 = np.exp(hypothesis)[:, 0]\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        'score': 1- score_class0,\n",
    "        'true_class': reference\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, subplot = plt.subplots(1, 2, figsize=(14, 4), sharey=True)\n",
    "\n",
    "\n",
    "sns.histplot(df.query('true_class == 0'), x='score', color='r', alpha=0.8, label='class 0', ax=subplot[0], stat='probability')\n",
    "sns.histplot(df.query('true_class == 1'), x='score', color='b', alpha=0.5, label='class 1',ax=subplot[1], stat='probability')\n",
    "# sns.histplot(class1, color='b', alpha=0.5)\n",
    "subplot[0].set_yscale('log')\n",
    "subplot[1].set_yscale('log')\n",
    "# subplot.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=scores, x='threshold', y='value', hue='variable')\n",
    "\n",
    "THRESHOLD = 0.90\n",
    "plt.axvline(THRESHOLD)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print('TEST')\n",
    "\n",
    "reference = []\n",
    "hypothesis = []\n",
    "probs = []\n",
    "ltmodel = ltmodel.to(DEVICE)\n",
    "for batch in test_loader:\n",
    "    x, y = batch\n",
    "    x = x.to(DEVICE)\n",
    "\n",
    "    y_pred = ltmodel(x)\n",
    "    y_pred = ltmodel(x).exp()[:, 1]  > THRESHOLD #.argmax(axis=1)\n",
    "    \n",
    "\n",
    "    hypothesis.append(y_pred.cpu().detach().numpy())\n",
    "    reference.append(y.cpu().detach().numpy())\n",
    "\n",
    "\n",
    "reference = np.concatenate(reference)\n",
    "hypothesis = np.concatenate(hypothesis)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "\n",
    "confusion =  confusion_matrix(reference, hypothesis)\n",
    "print(confusion)\n",
    "sns.heatmap(confusion, annot=True, fmt='d', ax=ax)\n",
    "ax.set_xlabel(\"hypothesis\")\n",
    "ax.set_ylabel(\"reference\")\n",
    "# ax.set_xticklabels([\"None\", \"desicion/no_hace_lugar\", \"descion/hace_lugar\"])\n",
    "# ax.set_yticklabels([\"None\", \"desicion/no_hace_lugar\", \"descion/hace_lugar\"])\n",
    "ax.set_title('TEST')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "report = classification_report(reference, hypothesis, output_dict=True)\n",
    "pd.DataFrame(report).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sentences = [pair[1] for pair in test]\n",
    "\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"sentence\": sentences,\n",
    "        \"decision\": test_df['decision'],\n",
    "        \"hace_lugar\": test_df['hace_lugar'],\n",
    "        \"cat\": reference,\n",
    "        \"pred_cat\": hypothesis.astype(int),\n",
    "    }\n",
    ")\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\n",
    "    \"display.max_columns\",\n",
    "    1000,\n",
    "    \"display.width\",\n",
    "    1000,\n",
    "    \"display.max_colwidth\",\n",
    "    None,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pred_ok'] = df['cat'] == df['pred_cat']\n",
    "# test_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_.query('decision == 0 and pred_decision and not pred_hace_lugar').sample(1)\n",
    "df.query('pred_ok == 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# regex classificacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "\n",
    "pattern = regex.compile(r\"(?i)(no hacer? lugar|rechaz[ao]r?|no admitir|no convalidar|no autorizar|declarar inadmisible)\")\n",
    "\n",
    "def recategorize(row, cat_col='pred_cat'):\n",
    "    # print(type(row))\n",
    "    # print(len(row))\n",
    "    # # i, row = row\n",
    "    # print(row)\n",
    "    # print()\n",
    "\n",
    "    decision = row['decision']\n",
    "    hace_lugar = row['hace_lugar']\n",
    "    match (decision, hace_lugar):\n",
    "        case (1, 0):\n",
    "            row['cat'] = 1\n",
    "        case (1, 1):\n",
    "            row['cat'] = 2\n",
    "        case _:\n",
    "            row['cat'] = 0\n",
    "\n",
    "    decision_pred = row[cat_col]\n",
    "    if not decision_pred:\n",
    "        return row\n",
    "    \n",
    "    match = pattern.findall(row['sentence'])\n",
    "    if not match:\n",
    "        row[cat_col] = 2\n",
    "    return row\n",
    "    \n",
    "a = train_df.query('decision and not hace_lugar')\n",
    "df_recat = df.apply(recategorize, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recat['pred_cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reference = df_recat['cat']\n",
    "hypothesis = df_recat['pred_cat']\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "\n",
    "confusion =  confusion_matrix(reference, hypothesis)\n",
    "print(confusion)\n",
    "sns.heatmap(confusion, annot=True, fmt='d', ax=ax)\n",
    "ax.set_xlabel(\"hypothesis\")\n",
    "ax.set_ylabel(\"reference\")\n",
    "# ax.set_xticklabels([\"None\", \"desicion/no_hace_lugar\", \"descion/hace_lugar\"])\n",
    "# ax.set_yticklabels([\"None\", \"desicion/no_hace_lugar\", \"descion/hace_lugar\"])\n",
    "ax.set_title('TEST')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "report = classification_report(reference, hypothesis, output_dict=True)\n",
    "pd.DataFrame(report).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(df_recat['cat'], df_recat['pred_cat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recat.query('pred_cat == 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (main, Dec 19 2022, 17:35:49) [GCC 12.2.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "97cc609b13305c559618ec78a438abc56230b9381f827f22d070313b9a1f3777"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
