{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('sentences-decision.csv')\n",
    "target_classes = [\"none\", \"decision:no_hace_lugar\", \"decision:hace_lugar\"]\n",
    "\n",
    "def get_category(pair):\n",
    "    decision, hace_lugar = pair\n",
    "    if not decision:\n",
    "        cat = 0\n",
    "    elif decision and not hace_lugar:\n",
    "        cat = 1\n",
    "    elif decision and hace_lugar == 1:\n",
    "        cat = 1\n",
    "    else:\n",
    "        raise \"not valid\"\n",
    "    return cat\n",
    "\n",
    "data['category'] = data[['decision', 'hace_lugar']].apply(get_category, axis=1) \n",
    "\n",
    "data.drop_duplicates(subset='sentence', inplace=True)\n",
    "print(len(data))\n",
    "data['sentence'].apply(lambda x: len(x.split(' '))).hist(bins=[32*i for i in range(10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train, test = train_test_split(\n",
    "    data,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=data[\"category\"],\n",
    ")\n",
    "test, val = train_test_split(\n",
    "    train,\n",
    "    test_size=0.5,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"train:\", len(train))\n",
    "print(\"test:\", len(test))\n",
    "print(\"val:\", len(val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"cat 0: {len(train.query('category == 0'))} from {len(train)} sentences\")\n",
    "print(f\"cat 1: {len(train.query('category == 1'))} from {len(train)} sentences\")\n",
    "print(f\"cat 2: {len(train.query('category == 2'))} from {len(train)} sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    \"balanced\", classes=np.unique(train['category']), y=train['category']\n",
    ")\n",
    "# class_weights = {k: v for k, v in enumerate(class_weights)}\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "\n",
    "counts = np.bincount(train['category'])\n",
    "labels_weights = 1. / counts\n",
    "weights = labels_weights[train['category']]\n",
    "sampler = WeightedRandomSampler(weights, len(weights), replacement=True)\n",
    "sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "nlp = spacy.blank(\"es\")\n",
    "\n",
    "\n",
    "def encode_text(text):\n",
    "    return [t.text for t in nlp.tokenizer(text)]\n",
    "\n",
    "\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield encode_text(text)\n",
    "\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train[\"sentence\"]), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Tokenizer(object):\n",
    "    def __init__(self, vocab, max_len: int = 128):\n",
    "        self.nlp = spacy.blank('es')\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def save(self, path: str):\n",
    "        torch.save(path)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path:str):\n",
    "        vocab = torch.load(path)\n",
    "        return cls(vocab=vocab)\n",
    "    \n",
    "        \n",
    "    def tokenize(self, text: str):\n",
    "        return [t.text for t in self.nlp.tokenizer(text)]\n",
    "\n",
    "    def encode(self, text: int):\n",
    "        tokens = self.tokenize(text)[:self.max_len]\n",
    "        indices =  self.vocab(tokens)\n",
    "        indices = torch.tensor(indices, dtype=torch.int64)\n",
    "        indices = torch.nn.functional.pad(indices, (0, self.max_len-len(indices)))\n",
    "        return indices\n",
    "    \n",
    "    def encode_batch(self, texts):\n",
    "        indices = [self.encode(text) for text in texts]\n",
    "        indices = torch.stack(indices)\n",
    "        return indices\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode('hola como estas').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class DFtoDataset(Dataset):\n",
    "    def __init__(self, texts: list[str], targets: list[int], max_tokens=128, device = 'cuda'):\n",
    "\n",
    "        self.max_tokens = max_tokens\n",
    "\n",
    "        self.x_ = texts\n",
    "        self.y_ = targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y_)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x_[idx], self.y_[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "train_dataset = DFtoDataset(train[\"sentence\"].values, train[\"category\"].values)\n",
    "val_dataset = DFtoDataset(val[\"sentence\"].values, val[\"category\"].values)\n",
    "test_dataset = DFtoDataset(test[\"sentence\"].values, test[\"category\"].values)\n",
    "\n",
    "train_dataset = to_map_style_dataset(train_dataset)\n",
    "val_dataset = to_map_style_dataset(val_dataset)\n",
    "test_dataset = to_map_style_dataset(test_dataset)\n",
    "\n",
    "tokenizer = Tokenizer(vocab, max_len=128)\n",
    "\n",
    "\n",
    "def vectorize_batch(batch):\n",
    "    x, y = list(zip(*batch))\n",
    "\n",
    "    x = tokenizer.encode_batch(x)\n",
    "    x = x.to(DEVICE)\n",
    "\n",
    "    y = torch.tensor(y, device=DEVICE)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=24,\n",
    "    collate_fn=vectorize_batch,\n",
    "    sampler=sampler,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=24,\n",
    "    collate_fn=vectorize_batch,\n",
    "    shuffle=True,\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=24,\n",
    "    collate_fn=vectorize_batch,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(train_loader))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class TextClassifier(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_len: int = 50,\n",
    "        hidden_dim: int = 50,\n",
    "        n_layers: int = 1,\n",
    "        num_classes: int = 3,\n",
    "    ):\n",
    "        self.embed_len = embed_len\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.num_classes = num_classes\n",
    "        self.lr = 1e-3\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # layers\n",
    "        self.embedding_layer = nn.Embedding(\n",
    "            num_embeddings=len(vocab),\n",
    "            embedding_dim=self.embed_len,\n",
    "        )\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.embed_len,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers=self.n_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.linear = nn.Linear(self.hidden_dim, 1)\n",
    "\n",
    "        self.class_weights = torch.tensor(class_weights[1], dtype=torch.float32)\n",
    "        # self.loss = nn.CrossEntropyLoss(weight=self.class_weights)\n",
    "        # self.loss = nn.BCELoss(weight=self.class_weights)\n",
    "\n",
    "        # self.loss = nn.BCELoss()\n",
    "        self.loss = nn.BCEWithLogitsLoss(pos_weight=self.class_weights)\n",
    "\n",
    "        # metrics\n",
    "        self.accuracy = torchmetrics.Accuracy(task=\"binary\")\n",
    "        self.f1score = torchmetrics.F1Score(task=\"binary\", average='macro')\n",
    "\n",
    "    def forward(self, X_batch):\n",
    "        embeddings = self.embedding_layer(X_batch)\n",
    "        hidden = torch.randn(self.n_layers, len(X_batch), self.hidden_dim).to(DEVICE)\n",
    "        carry = torch.randn(self.n_layers, len(X_batch), self.hidden_dim).to(DEVICE)\n",
    "\n",
    "        output, (hidden, carry) = self.lstm(embeddings, (hidden, carry))\n",
    "        pred = self.linear(output[:, -1])\n",
    "\n",
    "        return pred\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        x, y = batch\n",
    "        y = y.type(torch.float32)\n",
    "\n",
    "        y_pred = self.forward(x)\n",
    "        y_pred = y_pred.reshape((len(y_pred)))\n",
    "\n",
    "        loss = self.loss(y_pred, y)\n",
    "        acc = self.accuracy(y_pred, y)\n",
    "        f1score = self.f1score(y_pred, y)\n",
    "\n",
    "        self.log(\"loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"acc\", acc, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"f1score\", f1score, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y = y.type(torch.float32)\n",
    "\n",
    "        y_pred = self.forward(x)\n",
    "        y_pred = y_pred.reshape((len(y_pred)))\n",
    "\n",
    "        loss = self.loss(y_pred, y)\n",
    "        acc = self.accuracy(y_pred, y)\n",
    "        f1score = self.f1score(y_pred, y)\n",
    "\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_acc\", acc, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_f1score\", f1score, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y = y.type(torch.float32)\n",
    "\n",
    "        y_pred = self.forward(x)\n",
    "        y_pred = y_pred.reshape((len(y_pred)))\n",
    "\n",
    "        loss = self.loss(y_pred, y)\n",
    "        acc = self.accuracy(y_pred, y)\n",
    "        f1score = self.f1score(y_pred, y)\n",
    "\n",
    "        self.log(\"test_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"test_acc\", acc, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"test_f1score\", f1score, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltmodel = TextClassifier()\n",
    "ltmodel = ltmodel.to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    x, y = batch\n",
    "\n",
    "    print(x.shape)\n",
    "    b = ltmodel.forward(x)\n",
    "\n",
    "    bb = b.reshape((len(b)))\n",
    "    yy = y.type(torch.float32)\n",
    "    ltmodel.loss(bb, yy)\n",
    "    print(b.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pytorch_lightning.callbacks import (\n",
    "    EarlyStopping,\n",
    "    RichProgressBar,\n",
    "    RichModelSummary,\n",
    "    LearningRateFinder,\n",
    "    LearningRateMonitor,\n",
    "    StochasticWeightAveraging,\n",
    "    ModelCheckpoint,\n",
    ")\n",
    "\n",
    "\n",
    "CHECKPOINT_PATH = \"checkpoints/pl-emb-lstm/\"\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    save_top_k=1,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    dirpath=CHECKPOINT_PATH,\n",
    "    # filename=\"{epoch}-{val_loss:.2f}-{other_metric:.2f}\",\n",
    ")\n",
    "\n",
    "# train model\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=DEVICE,\n",
    "    devices=1,\n",
    "    callbacks=[\n",
    "        EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            mode=\"min\",\n",
    "            min_delta=0.00,\n",
    "            patience=5,\n",
    "            verbose=False,\n",
    "        ),\n",
    "        checkpoint_callback,\n",
    "        StochasticWeightAveraging(swa_lrs=1e-2),\n",
    "        LearningRateFinder(),\n",
    "        LearningRateMonitor(),\n",
    "        RichModelSummary(),\n",
    "        # RichProgressBar(),\n",
    "    ],\n",
    "    max_epochs=50,\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    model=ltmodel,\n",
    "    train_dataloaders=train_loader,\n",
    "    val_dataloaders=val_loader,\n",
    ")\n",
    "\n",
    "\n",
    "print(checkpoint_callback.best_model_path)  # prints path to the best model's checkpoint\n",
    "print(checkpoint_callback.best_model_score)  # and prints it score\n",
    "best_model = ltmodel.load_from_checkpoint(checkpoint_callback.best_model_path)\n",
    "\n",
    "trainer.test(ltmodel, dataloaders=test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltmodel.eval()\n",
    "model = ltmodel.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print('TRAIN')\n",
    "\n",
    "x = tokenizer.encode_batch(train['sentence'].iloc[:]).to(DEVICE)\n",
    "hypothesis = model(x).argmax(axis=1)\n",
    "reference = train['category']\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "\n",
    "confusion =  confusion_matrix(reference, hypothesis)\n",
    "print(confusion)\n",
    "sns.heatmap(confusion, annot=True, fmt='d', ax=ax)\n",
    "ax.set_xlabel(\"hypothesis\")\n",
    "ax.set_ylabel(\"reference\")\n",
    "ax.set_xticklabels([\"None\", \"desicion/no_hace_lugar\", \"descion/hace_lugar\"])\n",
    "ax.set_yticklabels([\"None\", \"desicion/no_hace_lugar\", \"descion/hace_lugar\"])\n",
    "ax.set_title('TRAIN')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "report = classification_report(reference, hypothesis, output_dict=True)\n",
    "pd.DataFrame(report).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print('VAL')\n",
    "\n",
    "x = tokenizer.encode_batch(val['sentence'].iloc[:]).to(DEVICE)\n",
    "hypothesis = [model(x[i:i+1]).argmax(axis=1).cpu()[0] for i in range(len(x))]\n",
    "reference = val['category']\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "\n",
    "confusion =  confusion_matrix(reference, hypothesis)\n",
    "print(confusion)\n",
    "sns.heatmap(confusion, annot=True, fmt='d', ax=ax)\n",
    "ax.set_xlabel(\"hypothesis\")\n",
    "ax.set_ylabel(\"reference\")\n",
    "ax.set_xticklabels([\"None\", \"desicion/no_hace_lugar\", \"descion/hace_lugar\"])\n",
    "ax.set_yticklabels([\"None\", \"desicion/no_hace_lugar\", \"descion/hace_lugar\"])\n",
    "ax.set_title('VAL')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "report = classification_report(reference, hypothesis, output_dict=True)\n",
    "pd.DataFrame(report).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print('TEST')\n",
    "\n",
    "x = tokenizer.encode_batch(test['sentence'].iloc[:])\n",
    "hypothesis = model(x).argmax(axis=1)\n",
    "reference = test['category']\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "\n",
    "confusion =  confusion_matrix(reference, hypothesis)\n",
    "print(confusion)\n",
    "sns.heatmap(confusion, annot=True, fmt='d', ax=ax)\n",
    "ax.set_xlabel(\"hypothesis\")\n",
    "ax.set_ylabel(\"reference\")\n",
    "ax.set_xticklabels([\"None\", \"desicion/no_hace_lugar\", \"descion/hace_lugar\"])\n",
    "ax.set_yticklabels([\"None\", \"desicion/no_hace_lugar\", \"descion/hace_lugar\"])\n",
    "ax.set_title('TRAIN')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "report = classification_report(reference, hypothesis, output_dict=True)\n",
    "pd.DataFrame(report).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ = test.copy()\n",
    "test_[\"pred_cat\"] = hypothesis\n",
    "\n",
    "def cat2label(cat):\n",
    "    if cat == 0:\n",
    "        return (False, False)\n",
    "    if cat == 1:\n",
    "        return (True, False)\n",
    "    if cat == 2:\n",
    "        return (True, True)\n",
    "\n",
    "test_[['pred_decision', 'pred_hace_lugar']] = [cat2label(d) for d in test_['pred_cat']]\n",
    "test_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
