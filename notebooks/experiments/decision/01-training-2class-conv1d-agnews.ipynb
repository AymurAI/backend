{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%load_ext aymurai.devtools.magic\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "\n",
    "train_dataset, test_dataset = torchtext.datasets.AG_NEWS()\n",
    "\n",
    "train_dataset = to_map_style_dataset(train_dataset)\n",
    "test_dataset = to_map_style_dataset(test_dataset)\n",
    "train_dataset, val_dataset = train_test_split( train_dataset, test_size=0.1)\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(len(val_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def build_vocabulary(datasets):\n",
    "    for dataset in datasets:\n",
    "        for _, text in dataset:\n",
    "            yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(build_vocabulary([train_dataset, val_dataset, test_dataset]), min_freq=1, specials=[\"<UNK>\",])\n",
    "vocab.set_default_index(vocab[\"<UNK>\"])\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target_classes = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
    "\n",
    "max_tokens = 50\n",
    "\n",
    "def vectorize_text(batch):\n",
    "    Y, X = list(zip(*batch))\n",
    "    X = [vocab(tokenizer(text)) for text in X]\n",
    "    X = [tokens+([0]* (max_tokens-len(tokens))) if len(tokens)<max_tokens else tokens[:max_tokens] for tokens in X] ## Bringing all samples to max_tokens length.\n",
    "\n",
    "    xx, yy = torch.tensor(X, dtype=torch.int32), torch.tensor(Y) - 1 ## We have deducted 1 from target names to get them in range [0,1,2,3] from [1,2,3,4]\n",
    "    xx = xx.to(DEVICE)\n",
    "    yy = yy.to(DEVICE)\n",
    "    return xx, yy\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1024, collate_fn=vectorize_text, shuffle=True)\n",
    "val_loader  = DataLoader(val_dataset,  batch_size=1024, collate_fn=vectorize_text)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=1024, collate_fn=vectorize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for X, Y in train_loader:\n",
    "    print(X.shape, Y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = [data[0] for data in train_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    \"balanced\", classes=np.unique(cats), y=cats\n",
    ")\n",
    "# class_weights = {k: v for k, v in enumerate(class_weights)}\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "\n",
    "counts = np.bincount(cats)\n",
    "labels_weights = 1. / counts\n",
    "weights = labels_weights[cats]\n",
    "train_sampler = WeightedRandomSampler(weights, len(weights), replacement=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%export aymurai.models.decision.conv1d\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "class Conv1dTextClassifier(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        embed_len: int = 64,\n",
    "        nfeatures: int = 64,\n",
    "        num_classes: int = 4,\n",
    "        lr_scheduler_patience: int = 2,\n",
    "    ):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_len = embed_len\n",
    "        self.nfeatures = nfeatures\n",
    "        self.num_classes = num_classes\n",
    "        self.lr = 1e-3\n",
    "        self.lr_scheduler_patience = lr_scheduler_patience\n",
    "\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # layers\n",
    "        self.embedding_layer = nn.Embedding(\n",
    "            num_embeddings=self.vocab_size,\n",
    "            embedding_dim=self.embed_len,\n",
    "        )\n",
    "        self.conv1 = nn.Conv1d(self.embed_len, 64, kernel_size=7, padding=\"same\")\n",
    "        self.conv2 = nn.Conv1d(64, 32, kernel_size=7, padding=\"same\")\n",
    "        self.pooling = nn.MaxPool1d(2)\n",
    "\n",
    "        self.linear1 = nn.Linear(32, 32)\n",
    "        self.linear2 = nn.Linear(32, self.num_classes)\n",
    "        # self.linear = nn.Linear(self.nfeatures, self.num_classes)\n",
    "\n",
    "        self.class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
    "        # self.loss = nn.CrossEntropyLoss(weight=self.class_weights)\n",
    "\n",
    "\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "        self.loss = nn.NLLLoss(weight=self.class_weights)\n",
    "\n",
    "        # metrics\n",
    "        self.accuracy = torchmetrics.Accuracy(\n",
    "            task=\"multiclass\",\n",
    "            num_classes=self.num_classes,\n",
    "        )\n",
    "        self.f1score = torchmetrics.F1Score(\n",
    "            task=\"multiclass\",\n",
    "            num_classes=self.num_classes,\n",
    "        )\n",
    "\n",
    "    def forward(self, X_batch):\n",
    "        x = self.embedding_layer(X_batch)\n",
    "        x = x.reshape(\n",
    "            len(x), self.embed_len, max_tokens\n",
    "        )  ## Embedding Length needs to be treated as channel dimension\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pooling(x)\n",
    "        x = F.dropout(x, 0.5)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x, _ = x.max(dim=-1)\n",
    "\n",
    "        # x = self.linear1(x)\n",
    "        x = self.linear2(x)\n",
    "        # x = self.linear(x)\n",
    "        # x = F.linear(x, torch.tensor([self.nfeatures, 32]))\n",
    "        # x = F.linear(x, torch.tensor([32, self.num_classes]))\n",
    "        x = self.logsoftmax(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        x, y = batch\n",
    "\n",
    "        y_pred = self.forward(x)\n",
    "\n",
    "        # loss = F.cross_entropy(y_pred, y, weight=self.class_weights)\n",
    "        loss = self.loss(y_pred, y)\n",
    "        acc = self.accuracy(y_pred, y)\n",
    "        f1score = self.f1score(y_pred, y)\n",
    "\n",
    "        self.log(\"loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"acc\", acc, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"f1score\", f1score, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "\n",
    "        y_pred = self.forward(x)\n",
    "\n",
    "        # loss = F.cross_entropy(y_pred, y)\n",
    "        loss = self.loss(y_pred, y)\n",
    "        acc = self.accuracy(y_pred, y)\n",
    "        f1score = self.f1score(y_pred, y)\n",
    "\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_acc\", acc, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_f1score\", f1score, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "\n",
    "        y_pred = self.forward(x)\n",
    "\n",
    "        # loss = F.cross_entropy(y_pred, y)\n",
    "        loss = self.loss(y_pred, y)\n",
    "        acc = self.accuracy(y_pred, y)\n",
    "        f1score = self.f1score(y_pred, y)\n",
    "\n",
    "        self.log(\"test_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"test_acc\", acc, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"test_f1score\", f1score, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr)\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": ReduceLROnPlateau(\n",
    "                    optimizer,\n",
    "                    patience=self.lr_scheduler_patience,\n",
    "                ),\n",
    "                \"monitor\": \"val_loss\",\n",
    "                \"frequency\": 1\n",
    "                # If \"monitor\" references validation metrics, then \"frequency\" should be set to a\n",
    "                # multiple of \"trainer.check_val_every_n_epoch\".\n",
    "            },\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from aymurai.models.decision.conv1d import Conv1dTextClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Conv1dTextClassifier(vocab_size=len(vocab), embed_len=128, num_classes=4)\n",
    "model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    x, y = batch\n",
    "    # x = x.to('cuda')\n",
    "    \n",
    "    print(x.shape)\n",
    "    b = model.forward(x)\n",
    "    print(b)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltmodel = Conv1dTextClassifier(vocab_size=len(vocab), embed_len=128, num_classes=4)\n",
    "ltmodel = ltmodel.to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import (\n",
    "    EarlyStopping,\n",
    "    RichProgressBar,\n",
    "    RichModelSummary,\n",
    "    LearningRateFinder,\n",
    "    LearningRateMonitor,\n",
    "    StochasticWeightAveraging,\n",
    "    ModelCheckpoint,\n",
    ")\n",
    "\n",
    "\n",
    "CHECKPOINT_PATH = \"checkpoints/pl-emb-conv/\"\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    save_top_k=1,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    dirpath=CHECKPOINT_PATH,\n",
    "    # filename=\"{epoch}-{val_loss:.2f}-{other_metric:.2f}\",\n",
    ")\n",
    "\n",
    "# train model\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=DEVICE,\n",
    "    devices=1,\n",
    "    callbacks=[\n",
    "        EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            mode=\"min\",\n",
    "            min_delta=0.00,\n",
    "            patience=10,\n",
    "            verbose=False,\n",
    "        ),\n",
    "        checkpoint_callback,\n",
    "        # StochasticWeightAveraging(swa_lrs=1e-2),\n",
    "        LearningRateFinder(),\n",
    "        LearningRateMonitor(),\n",
    "        RichModelSummary(),\n",
    "        # RichProgressBar(),\n",
    "    ],\n",
    "    max_epochs=50,\n",
    "    min_epochs=5,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer.fit(\n",
    "    model=ltmodel,\n",
    "    train_dataloaders=train_loader,\n",
    "    val_dataloaders=val_loader,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(checkpoint_callback.best_model_path)  # prints path to the best model's checkpoint\n",
    "print(checkpoint_callback.best_model_score)  # and prints it score\n",
    "path = checkpoint_callback.best_model_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pl.seed_everything(42)\n",
    "# path = '/workspace/notebooks/experiments/decision/test/conv/model.ckpt'\n",
    "# path = '/workspace/notebooks/experiments/decision/checkpoints/pl-emb-conv/epoch=38-step=6981.ckpt'\n",
    "# best_model = ltmodel.load_from_checkpoint(path, map_location='cpu')\n",
    "best_model = ltmodel.eval()\n",
    "best_model.eval()\n",
    "\n",
    "trainer.test(ltmodel, dataloaders=test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = x[:1]\n",
    "xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print('TRAIN')\n",
    "\n",
    "reference = []\n",
    "hypothesis = []\n",
    "\n",
    "ltmodel = ltmodel.to('cuda')\n",
    "for batch in train_loader:\n",
    "    x, y = batch\n",
    "    x = x.to('cuda')\n",
    "\n",
    "    y_pred = ltmodel(x).exp().argmax(axis=1)\n",
    "\n",
    "    hypothesis.append(y_pred.cpu())\n",
    "    reference.append(y.cpu())\n",
    "\n",
    "reference = np.concatenate(reference)\n",
    "hypothesis = np.concatenate(hypothesis)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "\n",
    "confusion =  confusion_matrix(reference, hypothesis)\n",
    "print(confusion)\n",
    "sns.heatmap(confusion, annot=True, fmt='d', ax=ax)\n",
    "ax.set_xlabel(\"hypothesis\")\n",
    "ax.set_ylabel(\"reference\")\n",
    "# ax.set_xticklabels([\"None\", \"desicion/no_hace_lugar\", \"descion/hace_lugar\"])\n",
    "# ax.set_yticklabels([\"None\", \"desicion/no_hace_lugar\", \"descion/hace_lugar\"])\n",
    "ax.set_title('TRAIN')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "report = classification_report(reference, hypothesis, output_dict=True)\n",
    "pd.DataFrame(report).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print('VAL')\n",
    "\n",
    "reference = []\n",
    "hypothesis = []\n",
    "\n",
    "ltmodel = ltmodel.to('cuda')\n",
    "for batch in val_loader:\n",
    "    x, y = batch\n",
    "    x = x.to('cuda')\n",
    "\n",
    "    y_pred = ltmodel(x).exp().argmax(axis=1)\n",
    "\n",
    "    hypothesis.append(y_pred.cpu())\n",
    "    reference.append(y.cpu())\n",
    "\n",
    "reference = np.concatenate(reference)\n",
    "hypothesis = np.concatenate(hypothesis)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "\n",
    "confusion =  confusion_matrix(reference, hypothesis)\n",
    "print(confusion)\n",
    "sns.heatmap(confusion, annot=True, fmt='d', ax=ax)\n",
    "ax.set_xlabel(\"hypothesis\")\n",
    "ax.set_ylabel(\"reference\")\n",
    "# ax.set_xticklabels([\"None\", \"desicion/no_hace_lugar\", \"descion/hace_lugar\"])\n",
    "# ax.set_yticklabels([\"None\", \"desicion/no_hace_lugar\", \"descion/hace_lugar\"])\n",
    "ax.set_title('VAL')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "report = classification_report(reference, hypothesis, output_dict=True)\n",
    "pd.DataFrame(report).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print('TEST')\n",
    "\n",
    "reference = []\n",
    "hypothesis = []\n",
    "ltmodel = ltmodel.to('cuda')\n",
    "for batch in test_loader:\n",
    "    x, y = batch\n",
    "    x = x.to('cuda')\n",
    "\n",
    "    y_pred = ltmodel(x).exp().argmax(axis=1)\n",
    "\n",
    "    hypothesis.append(y_pred.cpu())\n",
    "    reference.append(y.cpu())\n",
    "\n",
    "reference = np.concatenate(reference)\n",
    "hypothesis = np.concatenate(hypothesis)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "\n",
    "confusion =  confusion_matrix(reference, hypothesis)\n",
    "print(confusion)\n",
    "sns.heatmap(confusion, annot=True, fmt='d', ax=ax)\n",
    "ax.set_xlabel(\"hypothesis\")\n",
    "ax.set_ylabel(\"reference\")\n",
    "# ax.set_xticklabels([\"None\", \"desicion/no_hace_lugar\", \"descion/hace_lugar\"])\n",
    "# ax.set_yticklabels([\"None\", \"desicion/no_hace_lugar\", \"descion/hace_lugar\"])\n",
    "ax.set_title('TEST')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "report = classification_report(reference, hypothesis, output_dict=True)\n",
    "pd.DataFrame(report).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sentences = [pair[1] for pair in test_dataset]\n",
    "\n",
    "\n",
    "def cat2label(cat):\n",
    "    if cat == 0:\n",
    "        return (False, False)\n",
    "    if cat == 1:\n",
    "        return (True, False)\n",
    "    if cat == 2:\n",
    "        return (True, True)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"sentence\": sentences,\n",
    "        \"cat\": reference,\n",
    "        \"pred_cat\": hypothesis,\n",
    "    }\n",
    ")\n",
    "\n",
    "# df[[\"pred_decision\", \"pred_hace_lugar\"]] = [cat2label(d) for d in df[\"pred_cat\"]]\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\n",
    "    \"display.max_columns\",\n",
    "    1000,\n",
    "    \"display.width\",\n",
    "    1000,\n",
    "    \"display.max_colwidth\",\n",
    "    None,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pred_ok'] = df['cat'] == df['pred_cat']\n",
    "# test_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_.query('decision == 0 and pred_decision and not pred_hace_lugar').sample(1)\n",
    "df.query('pred_ok == 0')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
