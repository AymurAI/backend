{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo pip install tensorflow_hub tensorflow-gpu tensorflow_text tensorflow-addons scikit-multilearn iterative-stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%load_ext aymurai.devtools.magic\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('sentences-decision.csv')\n",
    "\n",
    "def get_category(pair):\n",
    "    if pair[0] == 0:\n",
    "        cat = 0\n",
    "    elif pair[0] == 1 and pair[1] == 0:\n",
    "        cat = 1\n",
    "    elif pair[0] == 1 and pair[1] == 1:\n",
    "        cat = 2\n",
    "    else:\n",
    "        raise \"not valid\"\n",
    "    return cat\n",
    "\n",
    "data['category'] = data[['decision', 'hace_lugar']].apply(get_category, axis=1) \n",
    "\n",
    "data.drop_duplicates(subset='sentence', inplace=True)\n",
    "print(len(data))\n",
    "data['sentence'].apply(lambda x: len(x.split(' '))).hist(bins=[32*i for i in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "x = data['sentence'].values\n",
    "y = data['category'].values\n",
    "\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=42, stratify=y)\n",
    "test, val = train_test_split(test, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "print('train:', len(train))\n",
    "print('test:', len(test))\n",
    "print('val:', len(val))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"cat 0: {len(train.query('category == 0'))} from {len(train)} sentences\")\n",
    "print(f\"cat 1: {len(train.query('category == 1'))} from {len(train)} sentences\")\n",
    "print(f\"cat 2: {len(train.query('category == 2'))} from {len(train)} sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_train = train['sentence'].values\n",
    "y_train = train['category'].values\n",
    "\n",
    "x_val = val['sentence'].values\n",
    "y_val = val['category'].values\n",
    "\n",
    "x_test = test['sentence'].values\n",
    "y_test = test['category'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    \"balanced\", classes=np.unique(y_train), y=y_train\n",
    ")\n",
    "class_weights = {k: v for k, v in enumerate(class_weights)}\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(lower=True)\n",
    "tokenizer.fit_on_texts(train['sentence'].values)\n",
    "\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_val = tokenizer.texts_to_sequences(x_val)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_tokens = 128\n",
    "\n",
    "x_train = pad_sequences(\n",
    "    x_train, maxlen=max_tokens, padding=\"post\", truncating=\"post\", value=0\n",
    ")\n",
    "x_val = pad_sequences(\n",
    "    x_val, maxlen=max_tokens, padding=\"post\", truncating=\"post\", value=0\n",
    ")\n",
    "x_test = pad_sequences(\n",
    "    x_test, maxlen=max_tokens, padding=\"post\", truncating=\"post\", value=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tqdm.auto import tqdm\n",
    "import tensorflow_text as text\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_addons as tfa\n",
    "from more_itertools import flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense,\n",
    "    Input,\n",
    "    TextVectorization,\n",
    "    LSTM,\n",
    "    Bidirectional,\n",
    "    Embedding,\n",
    "    Dropout, Flatten\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "num_classes = y_train.shape[-1]\n",
    "f1_score = tfa.metrics.F1Score(num_classes=3, average='micro', name=\"f1_score\")\n",
    "embed_len = 25\n",
    "lstm_out = 20\n",
    "\n",
    "\n",
    "def get_model() -> Model:\n",
    "    model = Sequential(\n",
    "        [\n",
    "            Embedding(\n",
    "                input_dim=len(tokenizer.word_index) + 1,\n",
    "                output_dim=embed_len,\n",
    "                input_length=max_tokens,\n",
    "            ),\n",
    "            # Dropout(0.5),\n",
    "            # LSTM(lstm_out),\n",
    "            Bidirectional(LSTM(lstm_out)),\n",
    "            # Flatten(),\n",
    "            # Dense(50),\n",
    "            Dense(3, activation=\"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    adamw = tf.keras.optimizers.experimental.AdamW()\n",
    "    model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        optimizer=adamw,\n",
    "        # optimizer='rmsprop',\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "model = get_model()\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# This function keeps the initial learning rate for the first ten epochs\n",
    "# and decreases it exponentially after that.\n",
    "def scheduler(epoch, lr):\n",
    "  if epoch < 3:\n",
    "    return lr\n",
    "  else:\n",
    "    return lr * tf.math.exp(-0.1)\n",
    "\n",
    "callbacks = [\n",
    "    # ModelCheckpoint(\"glove_embeddings_sequence_model.keras\", save_best_only=True, save_format='tf'),\n",
    "    EarlyStopping(patience=15, monitor=\"val_loss\", restore_best_weights=True),\n",
    "    # tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "]\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=64,\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=50,\n",
    "    callbacks=callbacks,\n",
    "    class_weight=class_weights,\n",
    "    # shuffle=True,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, subplot = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "subplot[0].plot(history.history['loss'], label='train')\n",
    "subplot[0].plot(history.history['val_loss'], label='val')\n",
    "\n",
    "subplot[1].plot(history.history['accuracy'], label='train')\n",
    "subplot[1].plot(history.history['val_accuracy'], label='val')\n",
    "\n",
    "subplot[0].legend()\n",
    "subplot[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix, confusion_matrix\n",
    "\n",
    "print('VAL')\n",
    "\n",
    "hypothesis = model.predict(x_val).argmax(axis=1)\n",
    "reference = y_val\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "confusion =  confusion_matrix(reference, hypothesis)\n",
    "print(confusion)\n",
    "sns.heatmap(confusion, annot=True, fmt='d', ax=ax)\n",
    "ax.set_xlabel(\"hypothesis\")\n",
    "ax.set_ylabel(\"reference\")\n",
    "ax.set_xticklabels([\"None\", \"desicion/no_hace_lugar\", \"descion/hace_lugar\"])\n",
    "ax.set_yticklabels([\"None\", \"desicion/no_hace_lugar\", \"descion/hace_lugar\"])\n",
    "ax.set_title('VAL')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "report = classification_report(reference, hypothesis, output_dict=True)\n",
    "pd.DataFrame(report).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix, confusion_matrix\n",
    "\n",
    "print('TEST')\n",
    "\n",
    "hypothesis = model.predict(x_test).argmax(axis=1)\n",
    "reference = y_test\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "confusion =  confusion_matrix(reference, hypothesis)\n",
    "print(confusion)\n",
    "sns.heatmap(confusion, annot=True, fmt='d', ax=ax)\n",
    "ax.set_xlabel(\"hypothesis\")\n",
    "ax.set_ylabel(\"reference\")\n",
    "ax.set_xticklabels([\"None\", \"desicion/no_hace_lugar\", \"descion/hace_lugar\"])\n",
    "ax.set_yticklabels([\"None\", \"desicion/no_hace_lugar\", \"descion/hace_lugar\"])\n",
    "ax.set_title('VAL')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "report = classification_report(reference, hypothesis, output_dict=True)\n",
    "pd.DataFrame(report).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix, confusion_matrix\n",
    "\n",
    "print('TRAIN')\n",
    "\n",
    "hypothesis = model.predict(x_train).argmax(axis=1)\n",
    "reference = y_train\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "confusion =  confusion_matrix(reference, hypothesis)\n",
    "print(confusion)\n",
    "sns.heatmap(confusion, annot=True, fmt='d', ax=ax)\n",
    "ax.set_xlabel(\"hypothesis\")\n",
    "ax.set_ylabel(\"reference\")\n",
    "ax.set_xticklabels([\"None\", \"desicion/no_hace_lugar\", \"descion/hace_lugar\"])\n",
    "ax.set_yticklabels([\"None\", \"desicion/no_hace_lugar\", \"descion/hace_lugar\"])\n",
    "ax.set_title('TRAIN')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "report = classification_report(reference, hypothesis, output_dict=True)\n",
    "pd.DataFrame(report).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = test.copy()\n",
    "a[[\"pred_decision\", \"pred_hace_lugar\"]] = hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\n",
    "    \"display.max_columns\",\n",
    "    1000,\n",
    "    \"display.width\",\n",
    "    1000,\n",
    "    \"display.max_colwidth\",\n",
    "    None,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.query('decision == 1 and pred_decision < 0.5').sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo pip install lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def make_predictions(X_batch_text):\n",
    "    X = tokenizer.texts_to_sequences(X_batch_text)\n",
    "    X = pad_sequences(X, maxlen=max_tokens, padding=\"post\", truncating=\"post\", value=0) ## Bringing all samples to max_tokens length.\n",
    "    preds = model.predict(X)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_predictions([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime import lime_text\n",
    "import numpy as np\n",
    "\n",
    "explainer = lime_text.LimeTextExplainer(class_names=CATEGORIES, verbose=True)\n",
    "\n",
    "\n",
    "rng = np.random.RandomState(1)\n",
    "idx = rng.randint(1, len(test))\n",
    "X = tokenizer.texts_to_sequences(test['sentence'].iloc[idx:idx+1])\n",
    "X = pad_sequences(X, maxlen=max_tokens, padding=\"post\", truncating=\"post\", value=0) ## Bringing all samples to max_tokens length.\n",
    "preds = model.predict(X)\n",
    "\n",
    "text = test['sentence'].iloc[idx]\n",
    "\n",
    "print(text)\n",
    "print(\"Prediction : \", preds > 0.5)\n",
    "print(\"Actual :     \", y_test[idx])\n",
    "\n",
    "explanation = explainer.explain_instance([text], classifier_fn=make_predictions, labels=y_test[idx:idx+1])\n",
    "explanation.show_in_notebook()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
