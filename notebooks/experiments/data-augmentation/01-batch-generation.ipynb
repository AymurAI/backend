{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import srsly\n",
    "import random\n",
    "\n",
    "from rich.pretty import pprint\n",
    "from datasets import load_from_disk\n",
    "\n",
    "from aymurai.data_augmentation import DataAugmenter\n",
    "\n",
    "from aymurai.data_augmentation.anonymizer_entities import (\n",
    "    augmentation_functions,\n",
    "    faker,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# faker.seed_instance(42)\n",
    "faker.seed_instance(None)\n",
    "for i in range(10):\n",
    "    print(faker.name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "import srsly\n",
    "\n",
    "DATASET_NAME = \"/resources/data/restricted/anonymization/datasets/anonymization-dataset-pruned-2023-09-06\"\n",
    "dataset = load_from_disk(DATASET_NAME)\n",
    "\n",
    "with open(f\"{DATASET_NAME}/label_mapping.json\") as file:\n",
    "    label2code = srsly.json_loads(file.read())\n",
    "    code2label = {v: k for k, v in label2code.items()}\n",
    "\n",
    "print(dataset)\n",
    "print(\"nlabels:\", len(code2label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataset[\"train\"]\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labeled = train.filter(lambda x: x[\"n_labels\"][0] > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmenter = DataAugmenter(code2label, augmentation_functions, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = random.choice(train)\n",
    "sample = train[2747]\n",
    "print(sample[\"tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to reset the seed uncomment following\n",
    "# faker.seed_instance(42)\n",
    "\n",
    "augmented_sample = data_augmenter.augment_sample(sample)\n",
    "print(augmented_sample[\"tokens\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmenter = DataAugmenter(\n",
    "    code2label=code2label,\n",
    "    augmentation_functions=augmentation_functions,\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict, concatenate_datasets\n",
    "\n",
    "data_augmenter.random_state = 42\n",
    "\n",
    "datadict = DatasetDict()\n",
    "\n",
    "datadict[\"rebalanced-7k\"] = data_augmenter.augment_dataset(\n",
    "    train_labeled, frac=1, weighted=True\n",
    ")\n",
    "datadict[\"rebalanced-3k\"] = data_augmenter.augment_dataset(\n",
    "    train_labeled, frac=0.5, weighted=True\n",
    ")\n",
    "\n",
    "for copies in range(1, 5):\n",
    "    tag = len(train_labeled) * copies // 1000\n",
    "    name = f\"inbalanced-{tag}k\"\n",
    "    print(f\"generating {name}\")\n",
    "    datadict[name] = concatenate_datasets(\n",
    "        [\n",
    "            train_labeled.map(\n",
    "                lambda x: data_augmenter.augment_sample(x), load_from_cache_file=True\n",
    "            )\n",
    "            for _ in range(copies)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "datadict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from aymurai.utils.display.pandas import pandas_context\n",
    "\n",
    "options = {}\n",
    "options[\"display.max_columns\"] = 0\n",
    "options[\"display.max_colwidth\"] = 0\n",
    "\n",
    "revision = datadict[\"rebalanced-7k\"]\n",
    "\n",
    "item = random.choice(revision)\n",
    "\n",
    "original = train_labeled.filter(lambda x: x[\"hash\"] == item[\"original_hash\"])[0]\n",
    "with pandas_context(**options):\n",
    "    aux = pd.DataFrame(\n",
    "        {\n",
    "            \"labels\": [code2label[code] for code in original[\"tags\"]],\n",
    "            \"tokens\": original[\"tokens\"],\n",
    "        }\n",
    "    )\n",
    "    display(aux.T)\n",
    "original\n",
    "\n",
    "# augmented\n",
    "samples = revision.filter(lambda x: x[\"original_hash\"] == original[\"hash\"])\n",
    "\n",
    "print(\"total augmented:\", len(samples))\n",
    "for i, sample in enumerate(samples):\n",
    "    aux = pd.DataFrame(\n",
    "        {\n",
    "            \"labels\": [code2label[code] for code in sample[\"tags\"]],\n",
    "            \"tokens\": sample[\"tokens\"],\n",
    "        }\n",
    "    )\n",
    "    with pandas_context(**options):\n",
    "        display(aux.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aymurai.data_augmentation.utils import compute_label_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_counts = pd.DataFrame()\n",
    "\n",
    "# original\n",
    "counts = compute_label_counts(dataset=train, code2label=code2label)\n",
    "counts = pd.DataFrame(counts, index=pd.Index([\"full original\"]))\n",
    "counts = counts.sort_values(axis=1, by=\"full original\", ascending=False)\n",
    "total_counts = pd.concat([total_counts, counts])\n",
    "\n",
    "# separator\n",
    "sep = counts.map(lambda x: \"-\")\n",
    "sep.index = pd.Index([\"-\"])\n",
    "total_counts = pd.concat([total_counts, sep])\n",
    "\n",
    "# augmented\n",
    "for name, dataset in datadict.items():\n",
    "    counts = compute_label_counts(dataset=dataset, code2label=code2label)\n",
    "    counts = pd.DataFrame(counts, index=pd.Index([name]))\n",
    "    total_counts = pd.concat([total_counts, counts])\n",
    "\n",
    "with pandas_context(**options):\n",
    "    display(total_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator, AutoMinorLocator\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 2))\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for name, dataset in datadict.items():\n",
    "    hash_count = dataset.to_pandas()[\"original_hash\"].value_counts()\n",
    "    hash_count = pd.DataFrame(hash_count).reset_index()\n",
    "    hash_count[\"dataset\"] = name\n",
    "    df = pd.concat([df, hash_count], ignore_index=True)\n",
    "\n",
    "sns.boxplot(data=df, x=\"count\", y=\"dataset\", ax=ax)\n",
    "\n",
    "ax.tick_params()\n",
    "ax.grid(visible=True, which=\"both\")\n",
    "ax.set_xscale(\"symlog\")\n",
    "ax.xaxis.set_minor_locator(MultipleLocator(1))\n",
    "ax.set_xlim(xmin=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUGMENTED__DATASET_NAME = \"/resources/data/restricted/anonymization/datasets/anonymization-dataset-augmented-2023-09-06\"\n",
    "\n",
    "datadict.save_to_disk(AUGMENTED__DATASET_NAME)\n",
    "with open(f\"{AUGMENTED__DATASET_NAME}/label_mapping.json\", \"w\") as file:\n",
    "    json = srsly.json_dumps(label2code)\n",
    "    file.write(json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
