{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import srsly\n",
    "import random\n",
    "\n",
    "from rich.pretty import pprint\n",
    "from datasets import load_from_disk\n",
    "\n",
    "from aymurai.data_augmentation import DataAugmenter\n",
    "\n",
    "from aymurai.data_augmentation.anonymizer_entities import (\n",
    "    augmentation_functions,\n",
    "    faker,\n",
    "    Faker,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# faker.seed_instance(42)\n",
    "faker.seed_instance(None)\n",
    "for i in range(10):\n",
    "    print(faker.name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "import srsly\n",
    "\n",
    "DATASET_NAME = \"/resources/data/restricted/anonymization/datasets/anonymization-dataset-pruned-2023-09-06\"\n",
    "dataset = load_from_disk(DATASET_NAME)\n",
    "\n",
    "with open(f\"{DATASET_NAME}/label_mapping.json\") as file:\n",
    "    label2code = srsly.json_loads(file.read())\n",
    "    code2label = {v: k for k, v in label2code.items()}\n",
    "\n",
    "print(dataset)\n",
    "print(\"nlabels:\", len(code2label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataset[\"train\"]\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labeled = train.filter(lambda x: x[\"n_labels\"][0] > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmenter = DataAugmenter(code2label, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = random.choice(train)\n",
    "sample = train[2747]\n",
    "print(sample[\"tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmenter = DataAugmenter(code2label, random_state=42)\n",
    "# faker.seed_instance(42)\n",
    "augmented_sample = data_augmenter.augment(sample)\n",
    "print(augmented_sample[\"tokens\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmenter = DataAugmenter(augmentation_functions, code2label, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anonymize(example):\n",
    "    return data_augmenter.augment(example)\n",
    "\n",
    "\n",
    "anonymized_train = train.map(anonymize)\n",
    "anonymized_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def compute_label_weights(\n",
    "    dataset,\n",
    "    ignore_labels: list[str] = [\"O\", \"PER\", \"FECHA\"],\n",
    ") -> dict[str, float]:\n",
    "    counts = []\n",
    "    for example in tqdm(dataset, total=len(dataset)):\n",
    "        labels = [code2label[code] for code in example[\"tags\"]]\n",
    "        labels = [re.sub(r\"[BI]-\", \"\", label) for label in labels]\n",
    "        labels, count = np.unique(labels, return_counts=True)\n",
    "\n",
    "        counts.append({l: c for l, c in zip(labels, count)})\n",
    "    counts = pd.DataFrame(counts)\n",
    "    counts = counts.drop(columns=[\"O\", \"PER\", \"FECHA\"])\n",
    "    counts = counts.sum()\n",
    "\n",
    "    label_weights = counts.sum() / counts\n",
    "    label_weights /= label_weights.min()\n",
    "    label_weights = label_weights.to_dict()\n",
    "    return label_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_weights = compute_label_weights(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weight(example):\n",
    "    labels = [code2label[code] for code in example[\"tags\"]]\n",
    "    labels = [re.sub(r\"[BI]-\", \"\", label) for label in labels]\n",
    "    weights = [label_weights.get(label, 0) for label in labels]\n",
    "\n",
    "    example[\"weight\"] = max(weights)\n",
    "    return example\n",
    "\n",
    "\n",
    "wtrain = train.map(get_weight)\n",
    "wtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from enum import Enum, auto\n",
    "\n",
    "\n",
    "def augment_dataset(\n",
    "    dataset,\n",
    "    frac: float = 1,\n",
    "    weights: str = \"labels_max\",\n",
    "    random_state: int | None = None,\n",
    "):\n",
    "    if weights == \"labels_max\":\n",
    "        w = dataset.map(get_weight)\n",
    "        w = w[\"weight\"]\n",
    "    else:\n",
    "        w = None\n",
    "\n",
    "    resampled = dataset.to_pandas().sample(\n",
    "        frac=frac,\n",
    "        weights=w,\n",
    "        replace=True,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "    resampled = Dataset.from_pandas(resampled)\n",
    "    resampled = resampled.remove_columns([\"__index_level_0__\"])\n",
    "\n",
    "    data_augmenter = DataAugmenter(\n",
    "        # augmentation_functions,\n",
    "        code2label,\n",
    "        random_state=None,\n",
    "    )\n",
    "\n",
    "    def anonymize(example):\n",
    "        return data_augmenter.augment(example)\n",
    "\n",
    "    resampled = resampled.map(anonymize)\n",
    "\n",
    "    return resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train = augment_dataset(\n",
    "    train_labeled, frac=1, random_state=None, weights=\"labels_max\"\n",
    ")\n",
    "aug_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from aymurai.utils.display.pandas import pandas_context\n",
    "\n",
    "options = {}\n",
    "options[\"display.max_columns\"] = 0\n",
    "# options[\"display.max_cols\"] = 0\n",
    "\n",
    "train_df = train_labeled.to_pandas()\n",
    "aug_train_df = aug_train.to_pandas()\n",
    "\n",
    "item = random.choice(aug_train)\n",
    "\n",
    "index = train_df[train_df[\"hash\"] == item[\"original_hash\"]].index[0]\n",
    "aux = train_labeled[int(index)]\n",
    "original_hash = aux.pop(\"hash\")\n",
    "print(\"nlabels:\", aux.pop(\"n_labels\"))\n",
    "print(\"original:\", original_hash)\n",
    "with pandas_context(**options):\n",
    "    display(pd.DataFrame(aux).T)\n",
    "\n",
    "\n",
    "# augmenteds\n",
    "samples = aug_train.filter(lambda x: x[\"original_hash\"] == original_hash)\n",
    "\n",
    "print(\"total augmented:\", len(samples))\n",
    "for i, sample in enumerate(samples):\n",
    "    sample.pop(\"original_hash\")\n",
    "    sample.pop(\"n_labels\")\n",
    "    print(f\"augmented {i:03.0f}:\", sample.pop(\"hash\"))\n",
    "    with pandas_context(**options):\n",
    "        display(pd.DataFrame(sample).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for i, example in tqdm(aug_train.to_pandas().iterrows()):\n",
    "    labels = [code2label[code] for code in example[\"tags\"]]\n",
    "    labels = [re.sub(r\"[BI]-\", \"\", label) for label in labels]\n",
    "    labels, count = np.unique(labels, return_counts=True)\n",
    "\n",
    "    counts_df = pd.DataFrame({l: c for l, c in zip(labels, count)}, index=pd.Index([0]))\n",
    "    df = pd.concat([df, counts_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = df.copy()\n",
    "_df = _df.drop(columns=[\"O\"])\n",
    "\n",
    "counts = df.sum()\n",
    "counts = counts.drop(\"O\")\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "datadict = DatasetDict(\n",
    "    {\n",
    "        \"anonymized_train\": anonymized_train,\n",
    "        \"augmented_train\": aug_train,\n",
    "    }\n",
    ")\n",
    "datadict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadict.save_to_disk(\n",
    "    \"/resources/data/restricted/anonymization/anonymization-dataset-augmented-2023-09-06/\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
