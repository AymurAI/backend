{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import locale\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from faker import Faker\n",
    "from faker.providers import DynamicProvider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciamos Faker con `locale=\"es_AR\"`\n",
    "faker = Faker(locale=\"es_AR\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nombres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombres\n",
    "for i in range(10):\n",
    "    print(faker.name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombres masculinos\n",
    "for i in range(10):\n",
    "    print(faker.first_name_male())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombres femeninos\n",
    "for i in range(10):\n",
    "    print(faker.first_name_female())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombres no-binaries (¿funciona?)\n",
    "for i in range(10):\n",
    "    print(faker.first_name_nonbinary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apellidos\n",
    "for i in range(10):\n",
    "    print(faker.last_name())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direcciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direcciones\n",
    "for i in range(10):\n",
    "    print(faker.street_address())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dirección secundaria\n",
    "for i in range(10):\n",
    "    print(faker.secondary_address())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calles\n",
    "for i in range(10):\n",
    "    print(faker.address())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provincias\n",
    "for i in range(10):\n",
    "    print(faker.province())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ciudades\n",
    "for i in range(10):\n",
    "    print(faker.city())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Municipalidades\n",
    "for i in range(10):\n",
    "    print(faker.municipality())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Licencias\n",
    "for i in range(10):\n",
    "    print(faker.license_plate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Licencias Mercosur\n",
    "for i in range(10):\n",
    "    print(faker.license_plate_mercosur())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Licencias antiguas\n",
    "for i in range(10):\n",
    "    print(faker.license_plate_old())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bancos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bancos\n",
    "for i in range(10):\n",
    "    print(faker.bank())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nacionalidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listado de nacionalidades en español\n",
    "countries = pd.read_html(\"https://www.spanish.cl/Vocabulary/Notes/Nacionalidades.htm\")[\n",
    "    -1\n",
    "]\n",
    "countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# countries.to_csv(\"nacionalidades.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proveedor de nacionalidades de faker\n",
    "nationality_provider = DynamicProvider(\n",
    "    provider_name=\"nationality\",\n",
    "    elements=countries[\"Nacionalidad\"].apply(str.split).sum(),\n",
    ")\n",
    "\n",
    "faker = Faker(\"es_AR\")\n",
    "faker.add_provider(nationality_provider)\n",
    "\n",
    "for i in range(10):\n",
    "    print(faker.nationality())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fechas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Días\n",
    "for i in range(10):\n",
    "    print(faker.day_of_week())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Días - número\n",
    "for i in range(10):\n",
    "    print(faker.day_of_month())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meses\n",
    "for i in range(10):\n",
    "    print(faker.month())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meses - nombres\n",
    "for i in range(10):\n",
    "    print(faker.month_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Años\n",
    "for i in range(10):\n",
    "    print(faker.year())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE https://stackoverflow.com/questions/2090840/python-datetime-localization\n",
    "locale.setlocale(locale.LC_TIME, \"es_AR.UTF-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenación\n",
    "for i in range(10):\n",
    "    dt = datetime.strptime(faker.date(), \"%Y-%m-%d\")\n",
    "    print(\n",
    "        dt.strftime(\n",
    "            random.choice(\n",
    "                [\n",
    "                    \"%A %d de %B del %Y\",\n",
    "                    \"%d de %B del %Y\",\n",
    "                    \"%d de %B de %Y\",\n",
    "                    \"%d de %B del '%y\",\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fake_date():\n",
    "    dt = datetime.strptime(faker.date(), \"%Y-%m-%d\")\n",
    "    return dt.strftime(\n",
    "        random.choice(\n",
    "            [\n",
    "                \"%A %d de %B del %Y\",\n",
    "                \"%d de %B del %Y\",\n",
    "                \"%d de %B de %Y\",\n",
    "                \"%d de %B del '%y\",\n",
    "            ]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E-mail\n",
    "for i in range(10):\n",
    "    print(faker.ascii_email())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E-mail gratuitos\n",
    "for i in range(10):\n",
    "    print(faker.ascii_free_email())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E-mail corporativos\n",
    "for i in range(10):\n",
    "    print(faker.ascii_company_email())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dominios\n",
    "for i in range(10):\n",
    "    print(faker.domain_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URLs\n",
    "for i in range(10):\n",
    "    print(faker.url())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombres de usuarie\n",
    "for i in range(10):\n",
    "    print(faker.user_name())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teléfonos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Números de teléfono\n",
    "for i in range(10):\n",
    "    print(faker.phone_number())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "import srsly\n",
    "\n",
    "ANNOT_DIR = \"/resources/data/restricted/anonymization\"\n",
    "\n",
    "dataset = load_from_disk(f\"{ANNOT_DIR}/hg_dataset\")\n",
    "\n",
    "with open(f\"{ANNOT_DIR}/hg_dataset/label_mapping.json\") as file:\n",
    "    label2code = srsly.json_loads(file.read())\n",
    "    code2label = {v: k for k, v in label2code.items()}\n",
    "\n",
    "print(dataset)\n",
    "print(\"nlabels:\", len(code2label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataset[\"train\"]\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labeled = [doc for doc in train if doc[\"n_labels\"][0] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = train_labeled[10]  # train_labeled[16]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_labeled[16] =>\n",
    "\n",
    "# sample[\"tags\"][28] = 1\n",
    "# sample[\"tags\"][29] = 2\n",
    "# sample[\"tags\"][30] = 2\n",
    "# sample[\"tags\"][31] = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token, tag in zip(sample[\"tokens\"], sample[\"tags\"]):\n",
    "    print(token, tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entity_counts(tags: list, code2label: dict = code2label) -> dict:\n",
    "    entities = [\n",
    "        code2label.get(tag)\n",
    "        for tag in tags\n",
    "        if tag != 0 and code2label.get(tag).startswith(\"B-\")\n",
    "    ]\n",
    "    entity_counts = {\n",
    "        re.sub(r\"^B-\", \"\", k): v\n",
    "        for (k, v) in zip(*np.unique(entities, return_counts=True))\n",
    "    }\n",
    "    return entity_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_counts = get_entity_counts(sample[\"tags\"])\n",
    "entity_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_augmentation_functions = {\n",
    "    \"PER\": faker.name,\n",
    "    \"DIRECCION\": faker.street_address,\n",
    "    \"DNI\": faker.license_plate,\n",
    "    \"FECHA\": fake_date,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens_and_tags(text: str, entity: str) -> tuple[list]:\n",
    "    tokens = text.split()\n",
    "    tags = [f\"I-{entity}\"] * len(tokens)\n",
    "    tags[0] = f\"B-{entity}\"\n",
    "    return tokens, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_tags = [\n",
    "    get_tokens_and_tags(entity_augmentation_functions[entity](), entity)\n",
    "    for (entity, count) in entity_counts.items()\n",
    "    for i in range(count)\n",
    "]\n",
    "\n",
    "tokens_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain, islice\n",
    "from more_itertools import split_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_labels_and_indices_to_replace(tags: list[int], code2label: dict = code2label):\n",
    "    labeled_tags = [code2label.get(tag) for tag in tags]\n",
    "\n",
    "    indices_to_replace = np.where(np.array(sample[\"tags\"]) != 0)[0]\n",
    "    split_indices = np.where(list(map(lambda x: x.startswith(\"B\"), labeled_tags)))[0]\n",
    "    indices_to_replace = list(\n",
    "        split_before(indices_to_replace, lambda x: x in split_indices)\n",
    "    )\n",
    "    indices_to_replace = np.array(\n",
    "        [np.array(idx) for idx in indices_to_replace], dtype=\"object\"\n",
    "    )\n",
    "\n",
    "    labels_to_replace = np.array(labeled_tags)[\n",
    "        [min(idx) for idx in indices_to_replace]\n",
    "    ].tolist()\n",
    "    labels_to_replace = [re.sub(r\"^B-\", \"\", label) for label in labels_to_replace]\n",
    "\n",
    "    return labels_to_replace, indices_to_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_to_replace, indices_to_replace = find_labels_and_indices_to_replace(\n",
    "    sample[\"tags\"]\n",
    ")\n",
    "labels_to_replace, indices_to_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tokens = sample[\"tokens\"]\n",
    "sample_tags = sample[\"tags\"]\n",
    "print(\"Original tokens:\", sample_tokens)\n",
    "print(\"Original tags:\", sample_tags)\n",
    "print()\n",
    "\n",
    "labels_to_replace, indices_to_replace = find_labels_and_indices_to_replace(\n",
    "    sample[\"tags\"]\n",
    ")\n",
    "labels_to_replace, indices_to_replace\n",
    "print(labels_to_replace, indices_to_replace)\n",
    "\n",
    "# Replace list\n",
    "for label_to_replace, idx_to_replace in zip(labels_to_replace, indices_to_replace):\n",
    "    start_idx = min(idx_to_replace)\n",
    "    end_idx = max(idx_to_replace) + 1\n",
    "\n",
    "    entity = re.sub(r\"^B-\", \"\", label_to_replace)\n",
    "\n",
    "    original_tokens = np.array(sample_tokens)[start_idx:end_idx].tolist()\n",
    "    replacement_tokens, replacement_tags = get_tokens_and_tags(\n",
    "        entity_augmentation_functions.get(entity)(), entity\n",
    "    )\n",
    "    replacement_tags = [label2code.get(tag) for tag in replacement_tags]\n",
    "\n",
    "    print(original_tokens, replacement_tokens)\n",
    "    print()\n",
    "    len_diff = len(replacement_tokens) - len(original_tokens)\n",
    "    print(len_diff)\n",
    "    print(indices_to_replace)\n",
    "    indices_to_replace += len_diff\n",
    "    print(indices_to_replace)\n",
    "    print()\n",
    "\n",
    "    # Replace sublist with other in list\n",
    "    # Using itertools.islice()\n",
    "    replaced_tokens = chain(\n",
    "        islice(sample_tokens, 0, start_idx),\n",
    "        replacement_tokens,\n",
    "        islice(sample_tokens, end_idx, len(sample_tokens)),\n",
    "    )\n",
    "\n",
    "    # convert the chain object back to a list\n",
    "    replaced_tokens = list(replaced_tokens)\n",
    "\n",
    "    # Replace sublist with other in list\n",
    "    # Using itertools.islice()\n",
    "    replaced_tags = chain(\n",
    "        islice(sample_tags, 0, start_idx),\n",
    "        replacement_tags,\n",
    "        islice(sample_tags, end_idx, len(sample_tags)),\n",
    "    )\n",
    "\n",
    "    # convert the chain object back to a list\n",
    "    replaced_tags = list(replaced_tags)\n",
    "\n",
    "    # printing result\n",
    "    print(\"Replaced tokens:\", replaced_tokens)\n",
    "    print(\"Replaced tags:\", replaced_tags)\n",
    "    print()\n",
    "    sample_tokens = replaced_tokens\n",
    "    sample_tags = replaced_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token, tag in zip(sample_tokens, sample_tags):\n",
    "    print(token, tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(sample: dict, label2code: dict = label2code) -> dict:\n",
    "    sample_tokens = sample[\"tokens\"].copy()\n",
    "    sample_tags = sample[\"tags\"].copy()\n",
    "\n",
    "    labels_to_replace, indices_to_replace = find_labels_and_indices_to_replace(\n",
    "        sample_tags\n",
    "    )\n",
    "\n",
    "    for label_to_replace, idx_to_replace in zip(labels_to_replace, indices_to_replace):\n",
    "        start_idx = min(idx_to_replace)\n",
    "        end_idx = max(idx_to_replace) + 1\n",
    "\n",
    "        entity = re.sub(r\"^B-\", \"\", label_to_replace)\n",
    "\n",
    "        original_tokens = np.array(sample_tokens)[start_idx:end_idx].tolist()\n",
    "        original_tags = np.array(sample_tags)[start_idx:end_idx].tolist()\n",
    "        replacement_tokens, replacement_tags = get_tokens_and_tags(\n",
    "            entity_augmentation_functions.get(entity)(), entity\n",
    "        )\n",
    "        replacement_tags = [label2code.get(tag) for tag in replacement_tags]\n",
    "\n",
    "        len_diff = len(replacement_tokens) - len(original_tokens)\n",
    "        indices_to_replace += len_diff\n",
    "\n",
    "        replaced_tokens = chain(\n",
    "            islice(original_tokens, 0, start_idx),\n",
    "            replacement_tokens,\n",
    "            islice(original_tokens, end_idx, len(sample_tokens)),\n",
    "        )\n",
    "        replaced_tokens = list(replaced_tokens)\n",
    "\n",
    "        replaced_tags = chain(\n",
    "            islice(sample_tags, 0, start_idx),\n",
    "            replacement_tags,\n",
    "            islice(sample_tags, end_idx, len(sample_tags)),\n",
    "        )\n",
    "        replaced_tags = list(replaced_tags)\n",
    "\n",
    "        sample_tokens = replaced_tokens\n",
    "        sample_tags = replaced_tags"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
