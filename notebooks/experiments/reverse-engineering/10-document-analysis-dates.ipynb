{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "import locale \n",
    "\n",
    "locale.setlocale(locale.LC_ALL, 'es_AR.UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/resources/data/preprocessed.csv')\n",
    "data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extract full text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import textract\n",
    "from zipfile import BadZipFile\n",
    "\n",
    "def get_fulltext(path: str) -> str:\n",
    "    if not isinstance(path, str) or not os.path.exists(path):\n",
    "        return \"missing\"\n",
    "    try:\n",
    "        return textract.process(path, extension='odt').decode('utf-8')\n",
    "    except (BadZipFile, KeyError):\n",
    "        return \"corrupted\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "parallel = Parallel(n_jobs=10)\n",
    "get_fulltext_ = delayed(get_fulltext)\n",
    "data['fulltext'] = parallel(get_fulltext_(path) for path in tqdm(data['path']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mark corrupt or missing files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['valid_file'] = ~np.logical_or(data['fulltext'] == 'corrupted', data['fulltext'] == 'missing')\n",
    "data['valid_file'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filterout invalid data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.query('valid_file', inplace=True)\n",
    "predict = data.copy()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.isna(pd.to_datetime('1/2/2009', format=\"%d/%m/%Y\", errors=\"coerce\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def text_normalize(text: str) -> str:\n",
    "    text = unicodedata.normalize('NFC', text)\n",
    "    chars = list(text)\n",
    "    text = ''.join(char for char in chars if unicodedata.category(char) not in ['Lo', 'So', 'Po', 'C'])\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "locale.setlocale(locale.LC_ALL, 'es_AR.UTF-8')\n",
    "\n",
    "\n",
    "def compile_regex_list(regex: list[str]):\n",
    "    return re.compile('|'.join(regex))\n",
    "\n",
    "REGEX_DATE = compile_regex_list(\n",
    "    [\n",
    "        r\"(?P<day>\\d\\d?)\\s+de\\s+(?P<month>\\w+)\\s+de\\s+(?P<year>\\d\\d\\d?\\d?)\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "def to_datetime(str_date: str, formats: list[str]):\n",
    "    for format_ in formats:\n",
    "        date = pd.to_datetime(str_date, format=format_, errors='coerce')\n",
    "        if not pd.isna(date):\n",
    "            return date\n",
    "    return pd.NaT\n",
    "        \n",
    "\n",
    "\n",
    "def date_getter(text: str):\n",
    "    text = text_normalize(text)\n",
    "    matches = REGEX_DATE.findall(text)\n",
    "    if not matches:\n",
    "        return\n",
    "    str_dates = ['/'.join(match) for match in matches]\n",
    "    dates = [to_datetime(str_date, formats=['%d/%B/%Y', '%d/%B/%y']) for str_date in str_dates]\n",
    "    return dates[0]\n",
    "    \n",
    "\n",
    "predict['date'] = data['fulltext'].apply(date_getter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = data['date'] == predict['date']\n",
    "not_matched = data.loc[~mask]\n",
    "print(f'not matched: {len(not_matched)} from {len(data)} ({100*len(not_matched)/len(data):.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Buenos Aires,   1º de noviembre de 2017.\n",
    "Para resolver en la causa Nº 20221/15, en trámite por ante este Juzgado en lo Penal, Contravencional y de Faltas N° 10, a mi cargo, caratulada “Legajo de juicio en autos R. E. J. Cesar s/ inf. art. 149 bis - CP”.\n",
    "Antecedentes del caso\n",
    "ANTECEDENTES:\n",
    "\"\"\"\n",
    "print(unicodedata.normalize('NFKD', text))\n",
    "text.replace('º', '')\n",
    "matches = REGEX_DATE.findall(text)\n",
    "print(matches)\n",
    "str_dates = ['/'.join(match) for match in matches]\n",
    "print(str_dates)\n",
    "dates = [to_datetime(str_date, formats=['%d/%B/%Y', '%d/%B/%y']) for str_date in str_dates]\n",
    "print(dates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index with human errors: \n",
    "# fecha mal tipeada (prediccion correcta): \n",
    "# >> la mayoria tienen un offset de pocos dias, puede ser la fecha en que se subio la resolucion?\n",
    "# 372, 689, 690, 691, 692, 699, 757, 774, 790, 5241\n",
    "#\n",
    "# no tipeado el dia: 674, 765, 767, 772, 780, 799, 800, 802, 808, 809, 818, 989\n",
    "# no tiene fecha: 984\n",
    "# fecha anonimizada: 5199\n",
    "idx = -80\n",
    "row = not_matched.iloc[idx]\n",
    "row = pd.DataFrame(row).T\n",
    "print(row.index.values)\n",
    "print('date', row['date'].values)\n",
    "print('fecha resolucion:', row['fecha_resolucion'].values)\n",
    "print('prediction', date_getter(row['fulltext'].iloc[0]))\n",
    "print()\n",
    "print(row['fulltext'].iloc[0])\n",
    "# print(text_normalize(row['fulltext'].iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = REGEX_DATE.findall(data['fulltext'].iloc[3])\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['fulltext'].iloc[3]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
