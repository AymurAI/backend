{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"/workspace/resources/data/restricted/anonymization/data-splits-2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\n",
    "    f\"{DATASET_PATH}/train-ready.txt\",\n",
    "    sep=\" \",\n",
    "    names=[\"token\", \"label\"],\n",
    "    skip_blank_lines=False,\n",
    ")\n",
    "\n",
    "val = pd.read_csv(\n",
    "    f\"{DATASET_PATH}/dev-ready.txt\",\n",
    "    sep=\" \",\n",
    "    names=[\"token\", \"label\"],\n",
    "    skip_blank_lines=False,\n",
    ")\n",
    "\n",
    "# test = pd.read_csv(\n",
    "#     f\"{OUTPUT_DIR}/test.txt\",\n",
    "#     sep=\" \",\n",
    "#     names=[\"token\", \"label\"],\n",
    "#     skip_blank_lines=False,\n",
    "# )\n",
    "\n",
    "cat_count = dict(train[\"label\"].value_counts())\n",
    "categories = {v: k for k, v in enumerate(train[\"label\"].unique()) if not pd.isna(v)}\n",
    "label2code = {k: i for i, (k, v) in enumerate(categories.items())}\n",
    "code2label = {v: k for k, v in label2code.items()}\n",
    "\n",
    "print(\"train:\", len(train))\n",
    "print(\"val:\", len(val))\n",
    "# print(\"test:\", len(test))\n",
    "print(\"nlabels:\", len(code2label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(label2code, key=lambda x: x[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from more_itertools import unzip\n",
    "from tqdm.auto import tqdm\n",
    "from more_itertools import split_at\n",
    "from functools import cache\n",
    "\n",
    "\n",
    "def get_hg_format(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    df[\"tags\"] = df[\"label\"].map(lambda x: label2code.get(x, label2code[\"O\"]))\n",
    "    items = []\n",
    "    indices, rows = unzip(df.iterrows())\n",
    "    tuples = map(lambda row: (row[\"token\"], row[\"tags\"]), rows)\n",
    "    tuples = split_at(tuples, lambda x: pd.isna(x[0]))\n",
    "\n",
    "    for i, paragraph in enumerate(tuples):\n",
    "        df = pd.DataFrame(paragraph)\n",
    "        if not len(df):\n",
    "            continue\n",
    "        tokens = df[0].values\n",
    "        labels = df[1].values.astype(int)\n",
    "        nlabels = (len(labels[labels > 0]),)\n",
    "\n",
    "        if any(np.isnan(labels)):\n",
    "            continue\n",
    "\n",
    "        if len(tokens) != len(labels):\n",
    "            print(\"mismatch size\")\n",
    "            continue\n",
    "\n",
    "        items.append(\n",
    "            {\n",
    "                \"n_labels\": nlabels,\n",
    "                \"tokens\": list(tokens),\n",
    "                \"tags\": labels,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from datasets import DatasetDict, Dataset\n",
    "\n",
    "dataset = DatasetDict(\n",
    "    {\n",
    "        \"train\": Dataset.from_list(get_hg_format(train)),\n",
    "        \"validation\": Dataset.from_list(get_hg_format(val)),\n",
    "        # \"test\": Dataset.from_list(get_hg_format(test)),\n",
    "    }\n",
    ")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = dataset[\"train\"].to_pandas()\n",
    "df_dev = dataset[\"validation\"].to_pandas()\n",
    "# df_test = dataset[\"test\"].to_pandas()\n",
    "\n",
    "# apply hash to fast compare dupplicated\n",
    "df_train[\"hash\"] = df_train[\"tokens\"].str.join(\" \").apply(hash)\n",
    "df_dev[\"hash\"] = df_dev[\"tokens\"].str.join(\" \").apply(hash)\n",
    "# df_test[\"hash\"] = df_test[\"tokens\"].str.join(\" \").apply(hash)\n",
    "\n",
    "# drop duplicates\n",
    "df_train.drop_duplicates(subset=\"hash\", inplace=True)\n",
    "df_dev.drop_duplicates(subset=\"hash\", inplace=True)\n",
    "# df_test.drop_duplicates(subset=\"hash\", inplace=True)\n",
    "\n",
    "# get train hashes\n",
    "train_hash = set(df_train[\"hash\"])\n",
    "dev_hash = set(df_dev[\"hash\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aymurai.utils.display.pandas import pandas_context\n",
    "\n",
    "options = {}\n",
    "options[\"display.max_colwidth\"] = 0\n",
    "\n",
    "with pandas_context(**options):\n",
    "    aux = df_train.query(\"hash in @train_hash and hash in @dev_hash\")\n",
    "    aux[\"ntags\"] = aux[\"tags\"].apply(lambda x: np.sum(x))\n",
    "    display(aux.query(\"ntags > 0\"))\n",
    "    # display(aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop paragraphs shared between datasets\n",
    "df_dev.query(\"hash not in @train_hash\", inplace=True)\n",
    "# df_test.query(\"hash not in @train_hash and hash not in @dev_hash\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"] = Dataset.from_pandas(df_train)\n",
    "dataset[\"validation\"] = Dataset.from_pandas(df_dev)\n",
    "# dataset[\"test\"] = Dataset.from_pandas(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(dataset[\"train\"][\"hash\"]).intersection(set(dataset[\"validation\"][\"hash\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import srsly\n",
    "\n",
    "DATASET_NAME = (\n",
    "    \"/resources/data/restricted/anonymization/annonimization-dataset-pruned-2023-09-04\"\n",
    ")\n",
    "\n",
    "dataset.save_to_disk(DATASET_NAME)\n",
    "with open(f\"{DATASET_NAME}/label_mapping.json\", \"w\") as file:\n",
    "    json = srsly.json_dumps(label2code)\n",
    "    file.write(json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
