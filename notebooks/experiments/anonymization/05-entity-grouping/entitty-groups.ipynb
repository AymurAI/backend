{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "API_URL = \"http://localhost:8999\"  # Url for debugger. change it to your own"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_path = \"/resources/data/sample/document-01.docx\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## /document-extract endpoint output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract document using the API\n",
    "def extract_document(file_path: str) -> dict:\n",
    "    # Open the file in binary mode and send the POST request\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        files = {\"file\": file}\n",
    "        response = requests.post(url=f\"{API_URL}/document/extract\", files=files)\n",
    "    response.raise_for_status()\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /document-extract endpoint output\n",
    "extracted_document = extract_document(doc_path)\n",
    "extracted_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(extracted_document[\"paragraphs\"])\n",
    "\n",
    "document_id = extracted_document[\"id\"]\n",
    "print(\"document_id:\", document_id)\n",
    "print(\"number of paragraphs:\", len(extracted_document[\"paragraphs\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "\n",
    "# Function to make inference using the API\n",
    "def get_predictions(paragraph_id: uuid.UUID) -> dict:\n",
    "    response = requests.get(\n",
    "        url=f\"{API_URL}/pipeline/anonymization/paragraph/{paragraph_id}/predict\",\n",
    "        params={\"use_cache\": True},\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [\n",
    "    get_predictions(paragraph[\"id\"])\n",
    "    for paragraph in tqdm(extracted_document[\"paragraphs\"])\n",
    "]\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aymurai.database.crud.prediction import read_document_prediction_paragraphs\n",
    "from aymurai.database.meta.extra import ParagraphPredictionPublic\n",
    "from aymurai.database.schema import ModelType\n",
    "from aymurai.database.session import get_session\n",
    "\n",
    "session = next(get_session())\n",
    "\n",
    "annotations = read_document_prediction_paragraphs(\n",
    "    session=session,\n",
    "    document_id=uuid.UUID(document_id),\n",
    "    model_type=ModelType.ANONYMIZATION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "\n",
    "from more_itertools import flatten\n",
    "\n",
    "\n",
    "def get_entities(prediction):\n",
    "    return prediction[\"labels\"]\n",
    "\n",
    "\n",
    "labels = [para.prediction.labels for para in annotations if para.prediction]\n",
    "entities = list(flatten(labels))\n",
    "\n",
    "indexed_entities = [(i, entity) for i, entity in enumerate(entities)]\n",
    "indexed_entities = sorted(indexed_entities, key=lambda x: x[1].attrs.aymurai_label)\n",
    "\n",
    "\n",
    "groups = {\n",
    "    label: list(group)\n",
    "    for label, group in groupby(\n",
    "        indexed_entities, key=lambda x: x[1].attrs.aymurai_label\n",
    "    )\n",
    "}\n",
    "groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from aymurai.meta.entities import DocLabel\n",
    "\n",
    "\n",
    "# Normalize helper\n",
    "def normalize_text(text):\n",
    "    text = unicodedata.normalize(\"NFKD\", text)\n",
    "    text = \"\".join(ch for ch in text if unicodedata.category(ch) != \"Mn\")\n",
    "    text = regex.sub(r\"\\s+\", \" \", text)\n",
    "    text = regex.sub(r\"\\p{P}\", \"\", text)\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "def cluster_entities(entities: list[DocLabel], eps: float = 0.01, min_samples: int = 2):\n",
    "    \"\"\"\n",
    "    Cluster entity texts in a group and return a DataFrame with columns:\n",
    "    text, norm_text, index, cluster. Uses binary vectorization and Jaccard-DBSCAN.\n",
    "    \"\"\"\n",
    "\n",
    "    indexed_entities = [(i, entity) for i, entity in enumerate(entities)]\n",
    "    indexed_entities = sorted(indexed_entities, key=lambda x: x[1].attrs.aymurai_label)\n",
    "\n",
    "    # Prepare raw and normalized texts\n",
    "    texts = [ent[1].text for ent in indexed_entities]\n",
    "    norm_texts = [normalize_text(t) for t in texts]\n",
    "\n",
    "    # --------- Binary vectorization of words ----------------\n",
    "    vectorizer = CountVectorizer(binary=True, token_pattern=r\"\\b\\w+\\b\")\n",
    "    matrix = vectorizer.fit_transform(\n",
    "        norm_texts\n",
    "    )  # sparse matrix (n_samples x n_features)\n",
    "\n",
    "    # --------- Jaccard distance matrix ----------------\n",
    "    ints = (matrix @ matrix.T).toarray()\n",
    "    row_sums = matrix.sum(axis=1).A1\n",
    "    union = row_sums[:, None] + row_sums[None, :] - ints\n",
    "    # Avoid division by zero and compute distances\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        X = 1 - (ints / union)\n",
    "    X[union == 0] = 1.0\n",
    "\n",
    "    # -------- DBSCAN ----------------------------------------------------\n",
    "    db = DBSCAN(eps=eps, min_samples=min_samples, metric=\"precomputed\")\n",
    "    labels = db.fit_predict(X)\n",
    "\n",
    "    # Merge clusters by overlapping centroids\n",
    "    unique_lbls = sorted(set(labels) - {-1})\n",
    "    if unique_lbls:\n",
    "        # build centroid binary vectors\n",
    "        centroid_vectors = []\n",
    "        for lbl in unique_lbls:\n",
    "            mat = matrix[labels == lbl]\n",
    "            centroid = (mat.sum(axis=0) > 0).A1  # boolean mask\n",
    "            centroid_vectors.append(centroid)\n",
    "\n",
    "        C = np.vstack(centroid_vectors).astype(bool)\n",
    "        sim = (C.astype(int) @ C.T.astype(int)) > 0\n",
    "        mapping = {\n",
    "            lbl: unique_lbls[int(np.argmax(sim[idx]))]\n",
    "            for idx, lbl in enumerate(unique_lbls)\n",
    "        }\n",
    "        labels = [mapping.get(lbl, -1) for lbl in labels]\n",
    "\n",
    "    # Build results DataFrame\n",
    "    df = (\n",
    "        pd.DataFrame({\n",
    "            \"id\": [ent[1].id for ent in indexed_entities],\n",
    "            \"index\": [ent[0] for ent in indexed_entities],\n",
    "            \"paragraph_id\": [ent[1].fk_paragraph for ent in indexed_entities],\n",
    "            \"label\": [ent[1].attrs.aymurai_label for ent in indexed_entities],\n",
    "            \"text\": texts,\n",
    "            \"norm_text\": norm_texts,\n",
    "            \"cluster\": labels,\n",
    "        })\n",
    "        .sort_values(\"cluster\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    df[\"id\"] = df[\"id\"].apply(uuid.UUID)\n",
    "    df[\"paragraph_id\"] = df[\"paragraph_id\"].apply(uuid.UUID)\n",
    "    df.set_index(\"id\", inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Example: cluster the PER group\n",
    "results = cluster_entities(entities)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.groupby(\"cluster\").apply(lambda x: x[\"norm_text\"].tolist()).to_dict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
