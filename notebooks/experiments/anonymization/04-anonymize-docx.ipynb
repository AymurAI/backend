{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "import requests\n",
    "import xml.sax.saxutils\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from joblib import hash\n",
    "from copy import deepcopy\n",
    "from rich.pretty import pprint\n",
    "\n",
    "from aymurai.utils.json_data import load_json\n",
    "from aymurai.text.docx2xml import DocxXMLExtract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docx_xml_extract = DocxXMLExtract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample docx\n",
    "# doc_path = \"/resources/data/restricted/ar-juz-pcyf-10/RESOLUCIONES DEL JUZGADO/Aclaratoria/15473 AMARILLA 89 82 y 80 aclaratoria duracion mediddas de protecciÃ³n - hace lugar.docx\"\n",
    "# doc_path = \"/resources/data/restricted/ar-juz-pcyf-10/RESOLUCIONES DEL JUZGADO/Allanamiento/92939 VADERROSA rechaza allanamiento.docx\"\n",
    "doc_path = \"/resources/data/restricted/ar-juz-pcyf-10/RESOLUCIONES DEL JUZGADO/Allanamiento/Hace lugar/54765 ARDITO 149 allanamiento armas hace lugar.docx\"\n",
    "output_dir = os.path.basename(doc_path).split(\".\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docx_xml_extract.unzip_document(doc_path, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## /document-extract endpoint output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # /document-extract endpoint output\n",
    "# extracted_document = load_json(\"response-document-extract.json\")\n",
    "# extracted_document = load_json(\"response_1716927762752.json\")\n",
    "extracted_document = load_json(\"response_1716928496834.json\")\n",
    "extracted_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(extracted_document[\"document\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## /docx2xml endpoint output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /docx2xml endpoint output\n",
    "xml_document = docx_xml_extract({\"path\": doc_path})\n",
    "xml_document = {\"paragraphs\": xml_document}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(xml_document[\"paragraphs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_document[\"paragraphs\"][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unicodedata import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashes = [hash(paragraph) for paragraph in extracted_document[\"document\"]]\n",
    "hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash2idx = {i: _hash for i, _hash in enumerate(hashes)}\n",
    "hash2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_document[\"document\"][29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash2idx = {\n",
    "    hash(normalize(\"NFKC\", paragraph[\"plain_text\"].strip())): np.where(\n",
    "        np.array(hashes) == hash(normalize(\"NFKC\", paragraph[\"plain_text\"].strip()))\n",
    "    )[0].tolist()\n",
    "    for paragraph in xml_document[\"paragraphs\"]\n",
    "}\n",
    "\n",
    "hash2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_document[\"paragraphs\"] = [\n",
    "    paragraph | {\"hash\": hash(normalize(\"NFKC\", paragraph[\"plain_text\"].strip()))}\n",
    "    for paragraph in xml_document[\"paragraphs\"]\n",
    "]\n",
    "\n",
    "xml_document[\"paragraphs\"] = [\n",
    "    paragraph | {\"extracted_document_indices\": hash2idx[paragraph[\"hash\"]]}\n",
    "    for paragraph in xml_document[\"paragraphs\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(xml_document[\"paragraphs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_document[\"paragraphs\"] = [\n",
    "    paragraph\n",
    "    for paragraph in xml_document[\"paragraphs\"]\n",
    "    if paragraph[\"extracted_document_indices\"]\n",
    "]\n",
    "\n",
    "len(xml_document[\"paragraphs\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make inference\n",
    "def get_predictions(sample: str) -> dict:\n",
    "    response = requests.post(\n",
    "        url=\"http://localhost:8899/anonymizer/predict\",\n",
    "        json={\"text\": sample},\n",
    "    )\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [get_predictions(paragraph) for paragraph in extracted_document[\"document\"]]\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matching\n",
    "xml_document[\"paragraphs\"] = [\n",
    "    paragraph | {\"labels\": preds[hash2idx[paragraph[\"hash\"]][0]][\"labels\"]}\n",
    "    for paragraph in xml_document[\"paragraphs\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_paragraphs = iter(xml_document[\"paragraphs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = next(iter_paragraphs)\n",
    "pprint(paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace plain texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unify_consecutive_labels(sample: dict, text_key: str = \"document\"):\n",
    "    labels = sample[\"labels\"]\n",
    "    document = sample[text_key]\n",
    "\n",
    "    unified_labels = []\n",
    "    current_group = None\n",
    "\n",
    "    for label in labels:\n",
    "        # TODO: make this a post-processing in prediction pipeline\n",
    "        is_punctuation = label[\"text\"] in punctuation\n",
    "        if is_punctuation:\n",
    "            continue\n",
    "\n",
    "        if current_group is None:\n",
    "            # Start a new group with the current label\n",
    "            current_group = {\n",
    "                \"text\": label[\"text\"],\n",
    "                \"start_char\": label[\"start_char\"],\n",
    "                \"end_char\": label[\"end_char\"],\n",
    "                \"aymurai_label\": label[\"attrs\"][\"aymurai_label\"],\n",
    "            }\n",
    "        elif (\n",
    "            current_group[\"aymurai_label\"] == label[\"attrs\"][\"aymurai_label\"]\n",
    "            and (label[\"start_char\"] - current_group[\"end_char\"]) <= 1\n",
    "        ):\n",
    "            # Extend the current group with the current label\n",
    "            current_group[\"end_char\"] = label[\"end_char\"]\n",
    "        else:\n",
    "            # Finish the current group and start a new one\n",
    "            current_group[\"text\"] = document[\n",
    "                current_group[\"start_char\"] : current_group[\"end_char\"] + 1\n",
    "            ]\n",
    "            unified_labels.append(current_group)\n",
    "            current_group = {\n",
    "                \"text\": label[\"text\"],\n",
    "                \"start_char\": label[\"start_char\"],\n",
    "                \"end_char\": label[\"end_char\"],\n",
    "                \"aymurai_label\": label[\"attrs\"][\"aymurai_label\"],\n",
    "            }\n",
    "\n",
    "    # Finish the last group\n",
    "    if current_group is not None:\n",
    "        current_group[\"text\"] = document[\n",
    "            current_group[\"start_char\"] : current_group[\"end_char\"] + 1\n",
    "        ]\n",
    "        unified_labels.append(current_group)\n",
    "\n",
    "    return unified_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unify_consecutive_labels(preds[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[unify_consecutive_labels(pred) for pred in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_labels_in_text(pred: dict, text_key: str = \"document\"):\n",
    "    pred = deepcopy(pred)\n",
    "    doc = pred[text_key]\n",
    "\n",
    "    unified_labels = unify_consecutive_labels(pred, text_key)\n",
    "    offset = 0\n",
    "\n",
    "    for unified_label in unified_labels:\n",
    "        start_char = unified_label[\"start_char\"] + offset\n",
    "        end_char = unified_label[\"end_char\"] + offset\n",
    "        len_text_to_replace = end_char - start_char\n",
    "\n",
    "        aymurai_label = xml.sax.saxutils.escape(f\" <{unified_label['aymurai_label']}>\")\n",
    "        len_aymurai_label = len(aymurai_label)\n",
    "\n",
    "        doc = doc[:start_char] + aymurai_label + doc[end_char:]\n",
    "\n",
    "        offset += len_aymurai_label - len_text_to_replace\n",
    "\n",
    "    return re.sub(r\" +\", \" \", doc).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replaced = [replace_labels_in_text(pred) for pred in preds]\n",
    "replaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replaced = [\n",
    "    replace_labels_in_text(pred, text_key=\"plain_text\")\n",
    "    for pred in xml_document[\"paragraphs\"]\n",
    "]\n",
    "replaced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace source XMLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aymurai.utils.alignment.core import align_text, tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def erase_duplicates_justseen(series: pd.Series) -> pd.Series:\n",
    "    return pd.Series(\n",
    "        [\n",
    "            (\"\" if (i > 0 and series.iloc[i] == series.iloc[i - 1]) else series.iloc[i])\n",
    "            for i in range(len(series))\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def parse_token_indices(sample: dict) -> pd.DataFrame:\n",
    "    original_text = \" \".join(\n",
    "        [fragment[\"text\"] for fragment in sample[\"metadata\"][\"fragments\"]]\n",
    "    )\n",
    "    anonymized_text = replace_labels_in_text(sample, text_key=\"plain_text\")\n",
    "\n",
    "    xml_file = sample[\"metadata\"][\"xml_file\"]\n",
    "\n",
    "    aligned = align_text(\n",
    "        \"<START> \" + original_text + \" <END>\",\n",
    "        \"<START> \" + anonymized_text + \" <END>\",\n",
    "        # preserve_whitespaces=True,\n",
    "    )\n",
    "    aligned[\"target\"] = erase_duplicates_justseen(aligned[\"target\"])\n",
    "\n",
    "    tokens = []\n",
    "    for i, fragment in enumerate(sample[\"metadata\"][\"fragments\"]):\n",
    "        text = fragment[\"text\"]\n",
    "        tokenized_text = tokenize(text)\n",
    "        paragraph_index = fragment[\"paragraph_index\"]\n",
    "\n",
    "        counter = Counter()\n",
    "        for j, token in enumerate(tokenized_text):\n",
    "            counter.update([token])\n",
    "\n",
    "            splits = text.split(token)\n",
    "            left, right = splits[: counter[token]], splits[counter[token] :]\n",
    "            left = \"\".join(left)\n",
    "            right = \"\".join(right)\n",
    "\n",
    "            start = sample[\"metadata\"][\"start\"] + fragment[\"start\"] + len(left)\n",
    "            end = start + len(token)\n",
    "\n",
    "            fragment_start = sample[\"metadata\"][\"start\"] + fragment[\"start\"]\n",
    "            fragment_end = sample[\"metadata\"][\"start\"] + fragment[\"end\"]\n",
    "\n",
    "            tokens.append(\n",
    "                (\n",
    "                    xml_file,\n",
    "                    paragraph_index,\n",
    "                    i,\n",
    "                    j,\n",
    "                    token,\n",
    "                    start,\n",
    "                    end,\n",
    "                    fragment_start,\n",
    "                    fragment_end,\n",
    "                    text,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    tokens = pd.DataFrame(\n",
    "        tokens,\n",
    "        columns=[\n",
    "            \"xml_file\",\n",
    "            \"paragraph_index\",\n",
    "            \"fragment_index\",\n",
    "            \"token_index\",\n",
    "            \"token\",\n",
    "            \"start_char\",\n",
    "            \"end_char\",\n",
    "            \"original_start_char\",\n",
    "            \"original_end_char\",\n",
    "            \"original_text\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    tokens = pd.concat(\n",
    "        [tokens, aligned[\"target\"].iloc[1:-1].reset_index(drop=True)], axis=1\n",
    "    )\n",
    "\n",
    "    # tokens[\"target\"] = tokens[\"target\"].map(xml.sax.saxutils.escape)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_document[\"paragraphs\"][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_token_indices(xml_document[\"paragraphs\"][6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import xml.etree.ElementTree as ET\n",
    "\n",
    "\n",
    "# # Look for every w:t tag in the document and attach the whitespace_preserve attribute\n",
    "# def preserve_whitespace(xml_content: str) -> str:\n",
    "#     namespaces = {\"w\": \"http://schemas.openxmlformats.org/wordprocessingml/2006/main\"}\n",
    "\n",
    "#     # Parse the XML content\n",
    "#     tree = ET.ElementTree(ET.fromstring(xml_content))\n",
    "#     root = tree.getroot()\n",
    "\n",
    "#     # Register namespaces\n",
    "#     for prefix, uri in namespaces.items():\n",
    "#         ET.register_namespace(prefix, uri)\n",
    "\n",
    "#     # Find all w:t elements and set the xml:space attribute\n",
    "#     for wt in root.findall(\".//w:t\", namespaces):\n",
    "#         wt.set(\"{http://www.w3.org/XML/1998/namespace}space\", \"preserve\")\n",
    "\n",
    "#     # Write back the XML content to a string\n",
    "#     xml_str = ET.tostring(root, encoding=\"unicode\", method=\"xml\")\n",
    "\n",
    "#     return xml_str\n",
    "\n",
    "\n",
    "# Look for every w:t tag in the document, attach the whitespace_preserve attribute, and replace multiple spaces with a single space\n",
    "# def preserve_whitespace_and_normalize_spaces(xml_content: str) -> str:\n",
    "#     namespaces = {\"w\": \"http://schemas.openxmlformats.org/wordprocessingml/2006/main\"}\n",
    "\n",
    "#     # Parse the XML content\n",
    "#     tree = ET.ElementTree(ET.fromstring(xml_content))\n",
    "#     root = tree.getroot()\n",
    "\n",
    "#     # Register namespaces\n",
    "#     for prefix, uri in namespaces.items():\n",
    "#         ET.register_namespace(prefix, uri)\n",
    "\n",
    "#     # Find all w:t elements\n",
    "#     for wt in root.findall(\".//w:t\", namespaces):\n",
    "#         # Set the xml:space attribute to preserve\n",
    "#         wt.set(\"{http://www.w3.org/XML/1998/namespace}space\", \"preserve\")\n",
    "\n",
    "#         # Replace multiple spaces with a single space in the text content\n",
    "#         if wt.text:\n",
    "#             wt.text = re.sub(r\"\\s+\", \" \", wt.text)\n",
    "\n",
    "#     # Write back the XML content to a string\n",
    "#     xml_str = ET.tostring(root, encoding=\"unicode\", method=\"xml\")\n",
    "\n",
    "#     return xml_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from lxml import etree\n",
    "\n",
    "\n",
    "# Look for every w:t tag in the document, attach the whitespace_preserve attribute, replace multiple spaces with a single space, and remove empty blocks\n",
    "def normalize_document(\n",
    "    xml_content: str,\n",
    ") -> str:\n",
    "    # Parse the XML content with lxml\n",
    "    parser = etree.XMLParser(ns_clean=True)\n",
    "    root = etree.fromstring(xml_content.encode(\"utf-8\"), parser)\n",
    "\n",
    "    # Extract namespaces\n",
    "    namespaces = {k: v for k, v in root.nsmap.items() if k}\n",
    "\n",
    "    # Find all w:r elements containing w:t elements\n",
    "    for wr in root.xpath(\"//w:r\", namespaces=namespaces):\n",
    "        wt = wr.find(\".//w:t\", namespaces)\n",
    "        if wt is not None:\n",
    "            # Set the xml:space attribute to preserve\n",
    "            wt.set(\"{http://www.w3.org/XML/1998/namespace}space\", \"preserve\")\n",
    "\n",
    "            # Replace multiple spaces with a single space in the text content\n",
    "            if wt.text:\n",
    "                wt.text = re.sub(r\"\\s+\", \" \", wt.text)\n",
    "\n",
    "            # Check if the text is empty after normalization\n",
    "            if not wt.text or wt.text.strip() == \"\":\n",
    "                # Remove the w:r element from its parent\n",
    "                wr.getparent().remove(wr)\n",
    "\n",
    "    # Write back the XML content to a string\n",
    "    xml_str = etree.tostring(root, encoding=\"unicode\", pretty_print=True)\n",
    "\n",
    "    return xml_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_text_in_xml(paragraphs: list[dict], base_dir: str):\n",
    "    tokens = pd.concat(\n",
    "        [parse_token_indices(sample) for sample in paragraphs], ignore_index=True\n",
    "    )\n",
    "    fragments = (\n",
    "        tokens.groupby([\"xml_file\", \"paragraph_index\", \"fragment_index\"])\n",
    "        .agg(\n",
    "            {\n",
    "                \"target\": \" \".join,\n",
    "                \"start_char\": \"min\",\n",
    "                \"end_char\": \"max\",\n",
    "                \"original_start_char\": \"min\",\n",
    "                \"original_end_char\": \"max\",\n",
    "                \"original_text\": \"first\",\n",
    "            }\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    for xml_file, group in fragments.groupby(\"xml_file\"):\n",
    "        group = group.sort_values(\"end_char\", ascending=False)\n",
    "\n",
    "        with open(f\"{base_dir}/word/{xml_file}\", \"r+\") as file:\n",
    "            content = file.read()\n",
    "\n",
    "            for i, r in group.iterrows():\n",
    "                start_char = r[\"original_start_char\"]\n",
    "                end_char = r[\"original_end_char\"]\n",
    "\n",
    "                target = r[\"target\"]\n",
    "\n",
    "                text = r[\"original_text\"]\n",
    "                if text.startswith(\" \") and not target.startswith(\" \"):\n",
    "                    target = \" \" + target\n",
    "                if text.endswith(\" \") and not target.endswith(\" \"):\n",
    "                    target = target + \" \"\n",
    "\n",
    "                target = re.sub(r\"\\s+\", \" \", target)\n",
    "\n",
    "                content = content[:start_char] + target + content[end_char:]\n",
    "\n",
    "            # content = re.sub(r\"\\s+\", \" \", content)\n",
    "\n",
    "            # MUST be at the end to dont screw up the indexes\n",
    "            content = normalize_document(content)\n",
    "            # content = preserve_whitespace(content)\n",
    "            # content = preserve_whitespace_and_normalize_spaces(content)\n",
    "\n",
    "            file.seek(0)  # Move the file pointer to the beginning\n",
    "            file.write(content)\n",
    "            file.truncate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_text_in_xml(xml_document[\"paragraphs\"], output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recreate anonymized docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add files to a zip archive\n",
    "def add_files_to_zip(zip_file, directory):\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            zip_file.write(file_path, os.path.relpath(file_path, directory))\n",
    "\n",
    "\n",
    "# Function to create the DOCX file\n",
    "def create_docx(xml_directory, output_file):\n",
    "    # Create a new zip file\n",
    "    with zipfile.ZipFile(output_file, \"w\") as docx:\n",
    "        # Add XML components\n",
    "        add_files_to_zip(docx, xml_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "os.makedirs(\"anonymized-documents\", exist_ok=True)\n",
    "\n",
    "output_file = \"edited.docx\"\n",
    "create_docx(output_dir, f\"anonymized-documents/{output_dir}-edited.docx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
